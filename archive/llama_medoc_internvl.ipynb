{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = env_config['HF_HOME']\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:42<00:00,  3.88s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=6):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "path = \"OpenGVLab/InternVL-Chat-V1-5\"\n",
    "# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n",
    "model_ocr = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True\n",
    "    ).eval()#.cuda()\n",
    "# Otherwise, you need to set device_map='auto' to use multiple GPUs for inference.\n",
    "# import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map='auto').eval()\n",
    "\n",
    "ocr_tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "# set the max number of tiles in `max_num`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic ViT batch size: 7\n",
      "Extract and format the text from the document in detail The text from the document is as follows:\n",
      "\n",
      "HOSPITAL INFANTIL DE MEXICO\n",
      "FEDERICO GOMEZ\n",
      "\n",
      "CONSULTA EXTERNA\n",
      "SERV. ONCOLOGIA\n",
      "REG. NUM. 84938\n",
      "\n",
      "NOMBRE:\n",
      "SEXO:\n",
      "EDAD:\n",
      "FC:\n",
      "TIA:\n",
      "SC:\n",
      "\n",
      "SELLOS, ORDENES Y TRATAMIENTO\n",
      "NOTAS DE EVOLUCION\n",
      "\n",
      "$5 AGO 2011\n",
      "\n",
      "Edad 18a 5/12\n",
      "Peso 110.5\n",
      "Talla 134\n",
      "FC 66\n",
      "TIA 119/84\n",
      "SC.\n",
      "\n",
      "Temp. 35.6\n",
      "\n",
      "En mantenimiento sem S6\n",
      "a la ef sin datos de actividad\n",
      "neoplasica ni infectiosa\n",
      "cupciente.\n",
      "\n",
      "Se le propone cateter pureto y rehusar\n",
      "Plau 0 con hacer mantenimiento -\n",
      "sem 57-60\n",
      "-mtx 90mg 1M semundarx2\n",
      "- Puntetho 150mg x2sm\n",
      "- mtx 90mg 1M x4\n",
      "- arac 1000mg IV dosssylica\n",
      "- VCR 2mg IV\n",
      "- lasp 1730000 u1m du Baja & Baja\n",
      "- dexa 1.8mg/96hr\n",
      "x isemana\n",
      "- ondawseton\n",
      "- 8mg Wprenio alarac\n",
      "-TMP/SMX propiacto\n",
      "\n",
      "Utilicen una nueva hora de evolucion hasta haberse cerrado que la anterior esta llena.\n",
      "Numerense las horas de evolucion para facilitar la revision del expediente\n"
     ]
    }
   ],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('handwritten_prescription.jpg', max_num=6).to(torch.bfloat16).cuda()\n",
    "\n",
    "generation_config = dict(\n",
    "    num_beams=1,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# single-round single-image conversation\n",
    "question = \"Extract and format the text from the document in detail\" # Please describe the picture in detail\n",
    "response = model_ocr.chat(ocr_tokenizer, pixel_values, question, generation_config)\n",
    "print(question, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic ViT batch size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain this document to me in detail. This document is a medical record from the Hospital Infantil de Mexico Federico Gomez, specifically from the Oncology department. The record is for a patient who is undergoing treatment for cancer. The document is written in Spanish and contains various sections with handwritten notes.\n",
      "\n",
      "At the top, the hospital's name and department are mentioned, along with a reference number for the patient's file. The date of the consultation is listed as \"5 Ago 2011.\"\n",
      "\n",
      "The patient's personal information is provided, including their name, age, and weight. The document also includes the patient's medical history, noting that the child has been diagnosed with cancer and is currently undergoing treatment. The treatment plan is detailed in the \"SELLOS, ORDENES Y TRATAMIENTO\" section, which includes a list of medications and dosages, as well as instructions for the patient's care.\n",
      "\n",
      "The \"NOTAS DE EVOLUCIÓN\" section contains notes on the patient's progress, including updates on their condition and any changes to their treatment plan. The document also includes a section for the patient's medical history, which is blank in this particular record.\n",
      "\n",
      "Overall, this document provides a detailed overview of the patient's medical treatment and progress, as well as important information about their personal and medical history.\n",
      "dynamic ViT batch size: 7\n",
      "What is the phase of the current treatment? The current treatment phase is \"sem 57-60,\" which likely refers to a specific phase or stage in the patient's treatment plan.\n",
      "dynamic ViT batch size: 7\n",
      "List the medications should be taken daily? The medications that should be taken daily include \"MTX 90mg 1M semundx2,\" \"Punetho 150mg x2,\" \"MTX 90mg 1M x4,\" \"Ara-C 1000mg IV,\" \"VCR 2mg IV,\" \"Lasp 17.3000 u/m du Bactrim,\" and \"Dexa 1.8mg/96hr x 1 semana.\"\n"
     ]
    }
   ],
   "source": [
    "# multi-round single-image conversation\n",
    "question = \"Explain this document to me in detail.\" # Please describe the picture in detail\n",
    "response, history = model_ocr.chat(ocr_tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(question, response)\n",
    "\n",
    "question = \"What is the phase of the current treatment?\" # Please write a poem according to the picture\n",
    "response, history = model_ocr.chat(ocr_tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(question, response)\n",
    "\n",
    "question = \"List the medications should be taken daily?\" # Please write a poem according to the picture\n",
    "response, history = model_ocr.chat(ocr_tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(question, response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to prompt Llama 3\n",
    "The base models have no prompt format. Like other base models, they can be used to continue an input sequence with a plausible continuation or for zero-shot/few-shot inference. They are also a great foundation for fine-tuning your own use cases. The Instruct versions use the following conversation structure:\n",
    "```bash\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ model_answer_1 }}<|eot_id|>\n",
    "\n",
    "```\n",
    "\n",
    "This format has to be exactly reproduced for effective use. We’ll later show how easy it is to reproduce the instruct prompt with the chat template available in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.69s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = r\"\"\"\n",
    "This document appears to be a medical record from a hospital visit at \"Hospital Infantil de México Federico Gómez.\" It includes patient details and notes from an oncology consultation. Here’s a detailed explanation:\n",
    "Header Information\n",
    "Hospital Name: Hospital Infantil de México Federico Gómez\n",
    "Department: Oncología (Oncology)\n",
    "Consultation Type: Consulta Externa (Outpatient Consultation)\n",
    "Date: 15 Aug 2021\n",
    "Record Number: 847388\n",
    "Patient Information\n",
    "Age: 18 years\n",
    "Weight: 110.5 kg\n",
    "Height: 174 cm\n",
    "Heart Rate (FC): 66 bpm\n",
    "Respiratory Rate (FR): (not clearly mentioned)\n",
    "Blood Pressure (TA): 119/84 mmHg\n",
    "Temperature: 35.6°C\n",
    "Clinical Notes\n",
    "The patient is known for their age.\n",
    "Currently in maintenance (sem S6, likely week 6 of a treatment regimen).\n",
    "Attended an asymptomatic review.\n",
    "Physical examination showed no signs of active neoplastic (cancerous) or infectious diseases.\n",
    "The proposed treatment includes maintaining the current plan and additional measures (likely related to a catheter, port, and chemotherapy drugs).\n",
    "Treatment Plan\n",
    "Current Medications and Dosages:\n",
    "Methotrexate (MTX): 90 mg IM weekly\n",
    "Purineethol (Mercaptopurine): 150 mg/m² daily\n",
    "Methotrexate (MTX): 90 mg IM weekly\n",
    "Ara-C (Cytarabine): 1000 mg intrathecally\n",
    "VCR (Vincristine): 2 mg IV\n",
    "L-asparaginase (L-ASP): 173,000 UI IM daily\n",
    "Dexamethasone (DEXA): 1.8 mg/m²/day for 6 days weekly\n",
    "Ondansetron: 8 mg for nausea prophylaxis\n",
    "TMP/SMX (Trimethoprim/Sulfamethoxazole): Prophylactic dose\n",
    "Additional Notes\n",
    "The next important date for the patient (likely a significant treatment or appointment) is scheduled for August 9.\n",
    "Other unspecified medical or supportive measures.\n",
    "Prescribing Doctors\n",
    "Signatures or names of doctors involved in the treatment (Dr. Rafael Ballon).\n",
    "Instructions\n",
    "Use a new sheet for further progress notes.\n",
    "Number the evolution sheets for easier review.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": response},\n",
    "    {\"role\": \"user\", \"content\": \"Explain this document to me in detail.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"List the medications should be taken daily \"},\n",
    "    # {\"role\": \"user\", \"content\": \"What is the phase of the current treatment?\"},\n",
    "]\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "input_dict = tokenizer.encode_plus(chat_template)\n",
    "\n",
    "input_ids = torch.tensor(input_dict[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(input_dict[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "The text from the document is as follows:\n",
      "\n",
      "HOSPITAL INFANTIL DE MEXICO\n",
      "FEDERICO GOMEZ\n",
      "\n",
      "CONSULTA EXTERNA\n",
      "SERV. ONCOLOGIA\n",
      "REG. NUM. 84938\n",
      "\n",
      "NOMBRE:\n",
      "SEXO:\n",
      "EDAD:\n",
      "FC:\n",
      "TIA:\n",
      "SC:\n",
      "\n",
      "SELLOS, ORDENES Y TRATAMIENTO\n",
      "NOTAS DE EVOLUCION\n",
      "\n",
      "$5 AGO 2011\n",
      "\n",
      "Edad 18a 5/12\n",
      "Peso 110.5\n",
      "Talla 134\n",
      "FC 66\n",
      "TIA 119/84\n",
      "SC.\n",
      "\n",
      "Temp. 35.6\n",
      "\n",
      "En mantenimiento sem S6\n",
      "a la ef sin datos de actividad\n",
      "neoplasica ni infectiosa\n",
      "cupciente.\n",
      "\n",
      "Se le propone cateter pureto y rehusar\n",
      "Plau 0 con hacer mantenimiento -\n",
      "sem 57-60\n",
      "-mtx 90mg 1M semundarx2\n",
      "- Puntetho 150mg x2sm\n",
      "- mtx 90mg 1M x4\n",
      "- arac 1000mg IV dosssylica\n",
      "- VCR 2mg IV\n",
      "- lasp 1730000 u1m du Baja & Baja\n",
      "- dexa 1.8mg/96hr\n",
      "x isemana\n",
      "- ondawseton\n",
      "- 8mg Wprenio alarac\n",
      "-TMP/SMX propiacto\n",
      "\n",
      "Utilicen una nueva hora de evolucion hasta haberse cerrado que la anterior esta llena.\n",
      "Numerense las horas de evolucion para facilitar la revision del expediente<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Explain this document to me in detail.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "This document appears to be a medical record for a pediatric patient, specifically a child with cancer, being treated at the Hospital Infantil de México Federico Gómez. Here's a breakdown of the different sections and information:\n",
      "\n",
      "**Patient Information**\n",
      "\n",
      "* NOMBRE: The patient's name is not specified, but it is a pediatric patient (INFANTIL).\n",
      "* SEXO: The patient's sex is not specified.\n",
      "* EDAD: The patient's age is 18 months and 5/12 (approximately 1 year and 5 months old).\n",
      "* FC: The patient's heart rate is 66 beats per minute.\n",
      "* TIA: The patient's blood pressure is 119/84 mmHg.\n",
      "* SC: The patient's body temperature is not specified, but it is mentioned later in the document as being 35.6°C (96°F).\n",
      "\n",
      "**Clinical Notes**\n",
      "\n",
      "The document begins with a clinical note dated August 5th, 2011. The note describes the patient's current status, which includes:\n",
      "\n",
      "* Peso (weight): 110.5 kg\n",
      "* Talla (height): 134 cm\n",
      "* FC (heart rate): 66 beats per minute\n",
      "* TIA (blood pressure): 119/84 mmHg\n",
      "* SC (body temperature): 35.6°C (96°F)\n",
      "\n",
      "The note also mentions that the patient is in maintenance status, meaning they are receiving ongoing treatment, and that there are no signs of neoplastic (cancer) or infectious diseases.\n",
      "\n",
      "**Treatment Plan**\n",
      "\n",
      "The document outlines the patient's treatment plan, which includes:\n",
      "\n",
      "* Cateter pureto: The patient has a central venous catheter (CVC) for administration of medications and fluids.\n",
      "* Medications:\n",
      "\t+ Mtx (methotrexate): 90mg every 4 weeks\n",
      "\t+ Puntetho (prednisone): 150mg every 2 weeks\n",
      "\t+ Arac (doxorubicin): 1000mg IV every 4 weeks\n",
      "\t+ VCR (vincristine): 2mg IV every 4 weeks\n",
      "\t+ Lasp (lasix): 1730000 units every 4 weeks\n",
      "\t+ Dexa (dexamethasone): 1.8mg every 96 hours\n",
      "\t+ Ondawseton (ondansetron): 8mg every 4 weeks\n",
      "\t+ TMP/SMX (trimethoprim-sulfamethoxazole): prophylactic\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "assistant_response = tokenizer.decode(outputs[0])\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling explanation for Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from models import MLP\n",
    "\n",
    "class SimilarityMeasure(nn.Module):\n",
    "    def __init__(self, input_ids, attention_mask, embed_size=512):\n",
    "        super(SimilarityMeasure, self).__init__()\n",
    "\n",
    "        self.pred_map = MLP(pred_hidden_size, 128, embed_size, num_blocks=2, bottleneck_dim=64)\n",
    "        self.explain_map = MLP(explain_hidden_size, 128, embed_size, num_blocks=2, bottleneck_dim=64)\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(1.0))\n",
    "    \n",
    "    def forward(self, pred_feature, explain_features):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor of shape [N, pred_hidden_size].\n",
    "            k (torch.Tensor): Key tensor of shape [N, L, explain_hidden_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Similarity tensor of shape [N, L].\n",
    "        \"\"\"\n",
    "        pred_feature = F.normalize(self.pred_map(pred_feature), p=2, dim=-1).unsqueeze(1)  # [N, 1, embed_size]\n",
    "        explain_features = F.normalize(self.explain_map(explain_features), p=2, dim=-1)  # [N, L, embed_size]\n",
    "\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        similarity = torch.matmul(explain_features, pred_feature.transpose(-1, -2)).squeeze(-1) * logit_scale  # [N, L]\n",
    "\n",
    "        return similarity  # [N, L]\n",
    "\n",
    "\n",
    "class MaskGeneratingModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\" \n",
    "        hidden_size: int\n",
    "            The hidden size of the output of the generative model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.similarity_measure = SimilarityMeasure()\n",
    "        self.explain_map = MLP(input_dim=hidden_size, \n",
    "                               hidden_dim=128, \n",
    "                               output_dim=1, \n",
    "                               num_blocks=2, \n",
    "                               bottleneck_dim=64) # takes [N, L, hidden_size] outputs [N, L, 1]\n",
    "        \n",
    "        self.similarity = MLP(input_dim=hidden_size, \n",
    "                               hidden_dim=128, \n",
    "                               output_dim=1, \n",
    "                               num_blocks=2, \n",
    "                               bottleneck_dim=64) # takes [N, hidden_size] outputs [N, 1]\n",
    "\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')\n",
    "    \n",
    "    def forward(self, pred_features):\n",
    "        \"\"\" \n",
    "        pred_features: torch.Tensor of shape [N, L, hidden_size]\n",
    "        \"\"\"\n",
    "        mask_logits = self.explain_map(pred_features) # [N, L, 1]\n",
    "        return mask_logits \n",
    "    \n",
    "    def compute_similarity(self, masked_concat_features):\n",
    "        \"\"\" \n",
    "        masked_concat_features: torch.Tensor of shape [N, hidden_size]\n",
    "        \"\"\"\n",
    "        similarity_logit = self.similarity(masked_concat_features) # [N, 1]\n",
    "        return similarity_logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 27, 11508, 29], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"<mask>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "reader = easyocr.Reader(['es','en']) # this needs to run only once to load the model into memory\n",
    "result = reader.readtext('handwritten_prescription.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = ' '.join([str(result[i]) for i in range(len(result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document is in Spanish and contains medical information. Here is the text extracted from the document:\\n\\nHOSPITAL INFANTIL DE MEXICO\\nFEDERICO GOMEZ\\n\\nCONSULTA EXTERNA\\nSERV. ONCOLOGIA\\nREG. NUM. 84938\\n\\nNOMBRE:\\nSEXO:\\nEDAD:\\nFC:\\nTIA:\\nSC.:\\nTEMP.:\\nNOTAS DE EVOLUCION\\n\\nSELLOS, ORDENES Y TRATAMIENTO\\n$5 AGO. 2021\\n\\nEdad 18a 5/12\\nPeso 110.5\\nTalla 134\\nFC 66\\nTIA 119/84\\nSC. -\\nTemp. 35.6\\n\\nEn mantenimiento sem S6\\na la ef sin datos de actividad\\nneoplasica ni infectiosa\\ncupciente.\\n\\nSe le propone cateter pureto y rehuscar\\nPlau 0 con hacer mantenimiento -\\nsem 57-60\\n-mtx 90mg 1M semundarx2\\n- Puntetho 150mg x 2.5sm\\n- mtx 90mg 1M x 4\\n- arac 1000mg ldoossylica\\n- VCR 2mg lV\\n- lasp 17.3000 lU lM du Baja & Baja\\n- dexa 1.8mg/96hr x Isemana\\n- ondawseton\\n- 8mg Wprenio alarac\\n- TMP/SMX propiacto\\n\\nsem 57-60\\n-mtx 90mg 1M semundarx2\\n- Puntetho 150mg x 2.5sm\\n- mtx 90mg 1M x 4\\n- arac 1000mg ldoossylica\\n- VCR 2mg lV\\n- lasp 17.3000 lU lM du Baja & Baja\\n- dexa 1.8mg/96hr x Isemana\\n- ondawseton\\n- 8mg Wprenio alarac\\n- TMP/SMX propiacto\\n\\nUtilicen una nueva hora de evolucion hasta haberse cerrado que la anterior esta llena.\\nNumerense las horas'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
