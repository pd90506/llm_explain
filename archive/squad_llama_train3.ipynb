{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from transformers import LlamaTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.helper import LlmExpHelper\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['validation']\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/sst2\")\n",
    "# train_ds = ds['train']\n",
    "llm_exp_helper = LlmExpHelper(tokenizer, 'squad')\n",
    "collate_fn = llm_exp_helper.get_collate_fun()\n",
    "\n",
    "# Define batch size here!\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_dataloader))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    def num_words(x):\n",
    "        return len(x.split())\n",
    "    def get_first_k_words(x, k):\n",
    "        return ' '.join(x.split()[:k])\n",
    "    def get_cliped_text(texts, max_len):\n",
    "        return [text if num_words(text) <= max_len else get_first_k_words(text, max_len) for text in texts]\n",
    "    tokenizer = self.tokenizer\n",
    "    max_len = 1024 # characters limit other than token limit\n",
    "    if self.dataset == 'imdb':\n",
    "        texts = [example['text'] for example in examples]\n",
    "        texts = get_cliped_text(texts, max_len)\n",
    "        sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "    elif self.dataset == 'sst2':\n",
    "        texts = [example['sentence'] for example in examples]\n",
    "        texts = get_cliped_text(texts, max_len)\n",
    "        sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "    elif self.dataset == 'squad':\n",
    "        context = [example['context'] for example in examples]\n",
    "        context = get_cliped_text(context, max_len)\n",
    "        question = [example['question'] for example in examples]\n",
    "        texts = [f\"Context: {context[i]}\\nQuestion: {question[i]}\" for i in range(len(context))]\n",
    "        sys_context = \"You are a chatbot for answering questions. You can help users with their questions via concise responses.\"\n",
    "\n",
    "    messages_lambda = lambda texts: [\n",
    "        {\"role\": \"system\", \"content\": sys_context},\n",
    "        {\"role\": \"user\", \"content\": texts},\n",
    "    ]\n",
    "\n",
    "    messages = list(map(messages_lambda, texts))\n",
    "\n",
    "    messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    batch = tokenizer(\n",
    "                messages_with_template_applied,\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "                )\n",
    "    \n",
    "    # find the template boundaries\n",
    "    text_lens = [len(tokenizer.encode(text)) for text in texts]\n",
    "    text_lens_tensor = torch.tensor(text_lens, dtype=torch.long)\n",
    "    \n",
    "    def apply_mask(mask_tensor, text_lens_tensor):\n",
    "        batch_size, seq_len = mask_tensor.shape\n",
    "        for i in range(batch_size):\n",
    "            text_len = text_lens_tensor[i].item()\n",
    "            mask_tensor[i, -text_len-5:-5] = 0\n",
    "        return 1- mask_tensor\n",
    "\n",
    "    mask_tensor = apply_mask(torch.ones_like(batch['input_ids']), text_lens_tensor)\n",
    "\n",
    "    batch['context_mask'] = mask_tensor\n",
    "\n",
    "    if self.dataset == 'squad':\n",
    "        answers_start = [example['answers']['answer_start'][0] for example in examples]\n",
    "        answers_end = [example['answers']['answer_start'][0] + len(example['answers']['text'][0]) for example in examples]\n",
    "        batch['answers_start'] = torch.tensor(answers_start).long()\n",
    "        batch['answers_end'] = torch.tensor(answers_end).long()\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.squad_model3 import MaskGeneratingModel\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(hidden_size=4096, mlp_hidden_dim=4096, mlp_bottleneck_dim=768, mlp_num_blocks=5)\n",
    "mask_gen_model.to(device)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = 0.2536, Reward Loss = 0.2048, Mean Reward = 0.7592,Mask_loss = 0.4885 mask_mean = 0.8487:   0%|          | 1/10950 [00:03<10:58:27,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.0085, Reward Loss = 0.0097, Mean Reward = 0.0521,Mask_loss = -0.0122 mask_mean = 0.0827:   0%|          | 11/10950 [00:31<8:15:01,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 21: Loss = 0.0332, Reward Loss = 0.0350, Mean Reward = 0.1062,Mask_loss = -0.0177 mask_mean = 0.1897:   0%|          | 21/10950 [01:01<8:30:04,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 31: Loss = 0.0514, Reward Loss = 0.0520, Mean Reward = 0.1411,Mask_loss = -0.0060 mask_mean = 0.2098:   0%|          | 31/10950 [01:28<9:21:41,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 41: Loss = 0.1183, Reward Loss = 0.1148, Mean Reward = 0.2955,Mask_loss = 0.0351 mask_mean = 0.3433:   0%|          | 41/10950 [01:56<8:55:55,  2.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 51: Loss = 0.0071, Reward Loss = 0.0073, Mean Reward = 0.0990,Mask_loss = -0.0023 mask_mean = 0.0231:   0%|          | 51/10950 [02:43<14:03:17,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 61: Loss = 0.0378, Reward Loss = 0.0390, Mean Reward = 0.1233,Mask_loss = -0.0126 mask_mean = 0.1624:   1%|          | 61/10950 [03:30<13:26:41,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 71: Loss = 0.0481, Reward Loss = 0.0498, Mean Reward = 0.1305,Mask_loss = -0.0170 mask_mean = 0.2496:   1%|          | 71/10950 [04:09<12:55:58,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 81: Loss = 0.1877, Reward Loss = 0.1753, Mean Reward = 0.4263,Mask_loss = 0.1238 mask_mean = 0.5116:   1%|          | 81/10950 [04:53<11:30:06,  3.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 91: Loss = 0.0167, Reward Loss = 0.0172, Mean Reward = 0.1110,Mask_loss = -0.0051 mask_mean = 0.0564:   1%|          | 91/10950 [05:49<14:57:41,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 101: Loss = 0.1567, Reward Loss = 0.1504, Mean Reward = 0.3541,Mask_loss = 0.0635 mask_mean = 0.4083:   1%|          | 101/10950 [06:33<12:30:24,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 111: Loss = 0.0360, Reward Loss = 0.0372, Mean Reward = 0.1335,Mask_loss = -0.0120 mask_mean = 0.1720:   1%|          | 111/10950 [07:17<15:38:28,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 121: Loss = 0.0175, Reward Loss = 0.0186, Mean Reward = 0.0853,Mask_loss = -0.0112 mask_mean = 0.1033:   1%|          | 121/10950 [08:01<12:28:58,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 131: Loss = 0.3110, Reward Loss = 0.2883, Mean Reward = 0.6352,Mask_loss = 0.2267 mask_mean = 0.5214:   1%|          | 131/10950 [08:51<15:39:54,  5.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 141: Loss = 0.2280, Reward Loss = 0.2037, Mean Reward = 0.5370,Mask_loss = 0.2433 mask_mean = 0.6851:   1%|▏         | 141/10950 [09:41<14:46:34,  4.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 151: Loss = 0.0063, Reward Loss = 0.0083, Mean Reward = 0.0335,Mask_loss = -0.0199 mask_mean = 0.1188:   1%|▏         | 151/10950 [10:33<15:36:30,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 161: Loss = 0.0024, Reward Loss = 0.0026, Mean Reward = 0.0516,Mask_loss = -0.0022 mask_mean = 0.0144:   1%|▏         | 161/10950 [11:26<16:15:22,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 171: Loss = 0.0928, Reward Loss = 0.0911, Mean Reward = 0.2829,Mask_loss = 0.0169 mask_mean = 0.3075:   2%|▏         | 171/10950 [12:30<20:06:35,  6.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 181: Loss = 0.0885, Reward Loss = 0.0873, Mean Reward = 0.2064,Mask_loss = 0.0118 mask_mean = 0.2946:   2%|▏         | 181/10950 [13:43<19:08:42,  6.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 191: Loss = 0.1214, Reward Loss = 0.1188, Mean Reward = 0.2737,Mask_loss = 0.0264 mask_mean = 0.3290:   2%|▏         | 191/10950 [14:30<13:29:01,  4.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 201: Loss = 0.1369, Reward Loss = 0.1256, Mean Reward = 0.3422,Mask_loss = 0.1127 mask_mean = 0.6744:   2%|▏         | 200/10950 [15:12<12:04:10,  4.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 201: Loss = 0.1369, Reward Loss = 0.1256, Mean Reward = 0.3422,Mask_loss = 0.1127 mask_mean = 0.6744:   2%|▏         | 201/10950 [15:26<24:46:59,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 211: Loss = 0.1091, Reward Loss = 0.1074, Mean Reward = 0.2355,Mask_loss = 0.0163 mask_mean = 0.3063:   2%|▏         | 211/10950 [16:15<15:41:03,  5.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 221: Loss = 0.0142, Reward Loss = 0.0157, Mean Reward = 0.0623,Mask_loss = -0.0152 mask_mean = 0.1106:   2%|▏         | 221/10950 [17:09<18:04:19,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 231: Loss = 0.0296, Reward Loss = 0.0312, Mean Reward = 0.0854,Mask_loss = -0.0160 mask_mean = 0.2048:   2%|▏         | 231/10950 [18:04<14:21:15,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 241: Loss = 0.1621, Reward Loss = 0.1366, Mean Reward = 0.5018,Mask_loss = 0.2546 mask_mean = 0.7848:   2%|▏         | 241/10950 [18:45<11:31:38,  3.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 251: Loss = 0.2121, Reward Loss = 0.1784, Mean Reward = 0.5930,Mask_loss = 0.3371 mask_mean = 0.8372:   2%|▏         | 251/10950 [19:24<11:19:39,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 261: Loss = 0.0481, Reward Loss = 0.0505, Mean Reward = 0.1238,Mask_loss = -0.0240 mask_mean = 0.3647:   2%|▏         | 261/10950 [20:02<11:32:58,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 271: Loss = 0.1650, Reward Loss = 0.1480, Mean Reward = 0.4200,Mask_loss = 0.1699 mask_mean = 0.7116:   2%|▏         | 271/10950 [20:53<15:48:16,  5.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 281: Loss = 0.1367, Reward Loss = 0.1282, Mean Reward = 0.3114,Mask_loss = 0.0847 mask_mean = 0.6025:   3%|▎         | 281/10950 [21:42<17:09:16,  5.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 291: Loss = 0.1490, Reward Loss = 0.1426, Mean Reward = 0.3066,Mask_loss = 0.0641 mask_mean = 0.5133:   3%|▎         | 291/10950 [22:31<13:26:27,  4.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 301: Loss = 0.0698, Reward Loss = 0.0699, Mean Reward = 0.1642,Mask_loss = -0.0009 mask_mean = 0.3511:   3%|▎         | 301/10950 [23:27<15:36:51,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 311: Loss = 0.0121, Reward Loss = 0.0141, Mean Reward = 0.0544,Mask_loss = -0.0193 mask_mean = 0.1333:   3%|▎         | 311/10950 [24:18<13:30:54,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 321: Loss = 0.1392, Reward Loss = 0.1310, Mean Reward = 0.3262,Mask_loss = 0.0825 mask_mean = 0.5150:   3%|▎         | 321/10950 [25:02<13:10:44,  4.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 331: Loss = 0.1103, Reward Loss = 0.1017, Mean Reward = 0.2647,Mask_loss = 0.0866 mask_mean = 0.4266:   3%|▎         | 331/10950 [25:46<13:31:02,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 341: Loss = 0.0656, Reward Loss = 0.0658, Mean Reward = 0.1722,Mask_loss = -0.0022 mask_mean = 0.2484:   3%|▎         | 341/10950 [26:27<11:46:54,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 351: Loss = 0.2506, Reward Loss = 0.2234, Mean Reward = 0.5768,Mask_loss = 0.2723 mask_mean = 0.6977:   3%|▎         | 351/10950 [27:07<12:12:42,  4.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 361: Loss = 0.0208, Reward Loss = 0.0243, Mean Reward = 0.0643,Mask_loss = -0.0354 mask_mean = 0.2629:   3%|▎         | 361/10950 [27:53<15:03:55,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 371: Loss = 0.0257, Reward Loss = 0.0275, Mean Reward = 0.0977,Mask_loss = -0.0174 mask_mean = 0.1723:   3%|▎         | 371/10950 [28:41<12:18:41,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 381: Loss = 0.0591, Reward Loss = 0.0595, Mean Reward = 0.1466,Mask_loss = -0.0039 mask_mean = 0.4286:   3%|▎         | 381/10950 [29:26<12:42:42,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 391: Loss = 0.1587, Reward Loss = 0.1518, Mean Reward = 0.3280,Mask_loss = 0.0691 mask_mean = 0.5128:   4%|▎         | 391/10950 [30:11<13:51:34,  4.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 401: Loss = 0.0720, Reward Loss = 0.0728, Mean Reward = 0.1784,Mask_loss = -0.0082 mask_mean = 0.4058:   4%|▎         | 400/10950 [30:52<11:32:25,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 401: Loss = 0.0720, Reward Loss = 0.0728, Mean Reward = 0.1784,Mask_loss = -0.0082 mask_mean = 0.4058:   4%|▎         | 401/10950 [31:08<24:45:48,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 411: Loss = 0.1173, Reward Loss = 0.1098, Mean Reward = 0.2959,Mask_loss = 0.0748 mask_mean = 0.4822:   4%|▍         | 411/10950 [31:49<10:36:51,  3.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 421: Loss = 0.0499, Reward Loss = 0.0509, Mean Reward = 0.1444,Mask_loss = -0.0095 mask_mean = 0.2049:   4%|▍         | 421/10950 [32:18<8:51:01,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 422: Loss = 0.0886, Reward Loss = 0.0879, Mean Reward = 0.2134,Mask_loss = 0.0072 mask_mean = 0.3238:   4%|▍         | 422/10950 [32:23<13:28:15,  4.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m last_hidden_state\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     43\u001b[0m mask_logits \u001b[38;5;241m=\u001b[39m mask_gen_model(last_hidden_state)\n\u001b[0;32m---> 45\u001b[0m mask_gen_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss, reward_loss, mask_loss, mask_mean, mean_reward \u001b[38;5;241m=\u001b[39m mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     48\u001b[0m log \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     49\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_mean = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_mean\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m )\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model3.py:229\u001b[0m, in \u001b[0;36mMaskGeneratingModel.loss_func\u001b[0;34m(self, model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, num_samples)\u001b[0m\n\u001b[1;32m    226\u001b[0m sim_gt, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_sim(model, gen_tokens, gen_attention_mask, response_mask, labels\u001b[38;5;241m=\u001b[39mgen_tokens)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perturbed_input_ids, perturbed_attention_mask, user_input_mask \u001b[38;5;129;01min\u001b[39;00m perturbed_samples:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# obtain the similarity of the perturbed samples\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     sim, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_reward(sim, sim_gt) \n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# reward = torch.exp(sim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model3.py:183\u001b[0m, in \u001b[0;36mMaskGeneratingModel.calculate_sim\u001b[0;34m(self, model, input_ids, attention_mask, response_mask, labels)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_sim\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, input_ids, attention_mask, response_mask, labels):\n\u001b[0;32m--> 183\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# shift labels and response_mask to the left by one to match the next token prediction format\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:624\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    621\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    623\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, position_ids)\n\u001b[0;32m--> 624\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    628\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:183\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    181\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    182\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 183\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m         cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(x, position_ids)\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cos, sin\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrotate_half\u001b[39m(x):\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_gen_model.train()\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        # (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "        unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "        unpaded_token_mask[:, :-pad_length] = 1\n",
    "        gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "        # print(gen_tokens[0])\n",
    "        # print(gen_attention_mask[0])\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "        context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "        # Get the last hidden state for the prompt + response sequence\n",
    "        with torch.no_grad():\n",
    "            full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden_state = full_outputs.hidden_states[-1]\n",
    "            last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        mask_gen_outputs = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, \n",
    "                                                                           num_samples=5)\n",
    "        loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\n",
    "        log = (f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss.item():.4f}, \" \n",
    "                             f\"Reward Loss = {reward_loss.item():.4f}, \"\n",
    "                             f\"Mean Reward = {mean_reward.mean().item():.4f},\"\n",
    "                             f\"Mask_loss = {mask_loss.item():.4f} \"\n",
    "                             f\"mask_mean = {mask_mean.item():.4f}\"\n",
    "        )\n",
    "        pbar.set_description(log)\n",
    "        logging.debug(log)\n",
    "    \n",
    "        # the parameters before updating\n",
    "        params_before = mask_gen_model.state_dict()\n",
    "\n",
    "        # Train the model (inner loop)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # the mask_prob after the updates\n",
    "        # with torch.no_grad():\n",
    "        #     mask_logits_after = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        #     mask_gen_outputs_after = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_after, response_mask, \n",
    "        #                                                                     num_samples=5)\n",
    "        #     loss_after, reward_loss_after, mask_loss_after, mask_mean_after, mean_reward_after = mask_gen_outputs_after['loss'], mask_gen_outputs_after['reward_loss'], mask_gen_outputs_after['mask_loss'], mask_gen_outputs_after['mask_mean'], mask_gen_outputs_after['mean_reward']\n",
    "        #     mask_prob_after = (torch.sigmoid(mask_logits_after) * context_mask).clone().detach()\n",
    "        #     mean_reward_after = mean_reward_after.clone().detach()\n",
    "\n",
    "        # # load the parameters before the updates\n",
    "        # mask_gen_model.load_state_dict(params_before)\n",
    "        # mask_logits_before = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs_before = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_before, response_mask, \n",
    "        #                                                                    num_samples=5)\n",
    "        # loss_before, reward_loss_before, mask_loss_before, mask_mean_before, mean_reward_before = mask_gen_outputs_before['loss'], mask_gen_outputs_before['reward_loss'], mask_gen_outputs_before['mask_loss'], mask_gen_outputs_before['mask_mean'], mask_gen_outputs_before['mean_reward']\n",
    "        # mask_prob_before = (torch.sigmoid(mask_logits_before) * context_mask)\n",
    "\n",
    "        # # calculate the ratio of the mask probabilities before and after the updates\n",
    "        # ratio = mask_prob_after / (mask_prob_before + 1e-6)\n",
    "\n",
    "        # # 定义PPO的损失函数，假设clip_param是你定义的剪切参数\n",
    "        # clip_param = 0.2\n",
    "        # advantage = (mean_reward_after - mean_reward_before).unsqueeze(-1)  # 计算优势函数（advantage），这是根据任务定义的\n",
    "        # surr1 = ratio * advantage\n",
    "        # surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage\n",
    "        # ppo_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # # 更新模型参数\n",
    "        # optimizer.zero_grad()\n",
    "        # ppo_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print()\n",
    "        if idx % 200 == 0 and idx != 0:\n",
    "            torch.save(mask_gen_model.state_dict(), f'saved_model/mask_gen_model_{epoch}_{idx}.pth') \n",
    "            print()\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "# texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I did not like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "# texts = \"I really love this film. The acting was great, and the story was amazing. I would recommend this movie to everyone.\"\n",
    "# # texts = \"I don't like this movie. The acting was terrible, and the story was boring. I would not recommend this movie to anyone.\"\n",
    "# messages_lambda = lambda texts: [\n",
    "#             {\"role\": \"system\", \"content\": \"Answer the question based on the context.\"},\n",
    "#             # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "#             {\"role\": \"user\", \"content\": texts},\n",
    "#         ]\n",
    "# messages = messages_lambda(texts)\n",
    "# messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "\n",
    "# # test_text = [{\"text\": texts, \"label\": None}]\n",
    "# test_text = [{\"sentence\": texts, \"label\": None}]\n",
    "# test_inputs = collate_fn(test_text).to(device)\n",
    "\n",
    "test_inputs = next(iter(test_dataloader)).to(device)\n",
    "# test_inputs = next(iter(train_dataloader)).to(device)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][idx])\n",
    "\n",
    "# generate the answer for the test inputs\n",
    "gen_outputs = model.generate(\n",
    "            input_ids=test_inputs['input_ids'],\n",
    "            attention_mask=test_inputs['attention_mask'],\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "input_ids = test_inputs['input_ids']\n",
    "attention_mask = test_inputs['attention_mask']\n",
    "gen_tokens = gen_outputs.sequences\n",
    "pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "# get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "context_mask = F.pad(test_inputs['context_mask'], (0, pad_length), mode='constant', value=0)\n",
    "# (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "unpaded_token_mask[:, :-pad_length] = 1\n",
    "gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    # prompt_outputs = model(input_ids=test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "    prompt_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    last_hidden_state = prompt_outputs.hidden_states[-1].float()\n",
    "    mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgb(255, 255, 255); color: black;\"><|start_header_id|>system<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">are</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">chatbot</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">for</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">answering</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">can</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">help</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">users</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">with</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">their</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">via</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">concise</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">responses.<|start_header_id|>user<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(244, 255, 244); color: black;\">Context:</span> <span style=\"background-color: rgb(229, 255, 229); color: black;\">Super</span> <span style=\"background-color: rgb(207, 255, 207); color: black;\">Bowl</span> <span style=\"background-color: rgb(230, 255, 230); color: black;\">50</span> <span style=\"background-color: rgb(187, 255, 187); color: black;\">was</span> <span style=\"background-color: rgb(225, 255, 225); color: black;\">an</span> <span style=\"background-color: rgb(197, 255, 197); color: black;\">American</span> <span style=\"background-color: rgb(47, 255, 47); color: black;\">football</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">game</span> <span style=\"background-color: rgb(205, 255, 205); color: black;\">to</span> <span style=\"background-color: rgb(237, 255, 237); color: black;\">determine</span> <span style=\"background-color: rgb(30, 255, 30); color: black;\">the</span> <span style=\"background-color: rgb(247, 255, 247); color: black;\">champion</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">of</span> <span style=\"background-color: rgb(187, 255, 187); color: black;\">the</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">National</span> <span style=\"background-color: rgb(223, 255, 223); color: black;\">Football</span> <span style=\"background-color: rgb(235, 255, 235); color: black;\">League</span> <span style=\"background-color: rgb(252, 255, 252); color: black;\">(NFL)</span> <span style=\"background-color: rgb(253, 255, 253); color: black;\">for</span> <span style=\"background-color: rgb(240, 255, 240); color: black;\">the</span> <span style=\"background-color: rgb(229, 255, 229); color: black;\">2015</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">season.</span> <span style=\"background-color: rgb(27, 255, 27); color: black;\">The</span> <span style=\"background-color: rgb(146, 255, 146); color: black;\">American</span> <span style=\"background-color: rgb(162, 255, 162); color: black;\">Football</span> <span style=\"background-color: rgb(204, 255, 204); color: black;\">Conference</span> <span style=\"background-color: rgb(159, 255, 159); color: black;\">(AFC)</span> <span style=\"background-color: rgb(180, 255, 180); color: black;\">champion</span> <span style=\"background-color: rgb(202, 255, 202); color: black;\">Denver</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">Broncos</span> <span style=\"background-color: rgb(205, 255, 205); color: black;\">defeated</span> <span style=\"background-color: rgb(27, 255, 27); color: black;\">the</span> <span style=\"background-color: rgb(202, 255, 202); color: black;\">National</span> <span style=\"background-color: rgb(227, 255, 227); color: black;\">Football</span> <span style=\"background-color: rgb(251, 255, 251); color: black;\">Conference</span> <span style=\"background-color: rgb(195, 255, 195); color: black;\">(NFC)</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">champion</span> <span style=\"background-color: rgb(199, 255, 199); color: black;\">Carolina</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">Panthers</span> <span style=\"background-color: rgb(240, 255, 240); color: black;\">24âĢĵ10</span> <span style=\"background-color: rgb(226, 255, 226); color: black;\">to</span> <span style=\"background-color: rgb(241, 255, 241); color: black;\">earn</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">their</span> <span style=\"background-color: rgb(186, 255, 186); color: black;\">third</span> <span style=\"background-color: rgb(134, 255, 134); color: black;\">Super</span> <span style=\"background-color: rgb(220, 255, 220); color: black;\">Bowl</span> <span style=\"background-color: rgb(248, 255, 248); color: black;\">title.</span> <span style=\"background-color: rgb(216, 255, 216); color: black;\">The</span> <span style=\"background-color: rgb(109, 255, 109); color: black;\">game</span> <span style=\"background-color: rgb(173, 255, 173); color: black;\">was</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">played</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">on</span> <span style=\"background-color: rgb(246, 255, 246); color: black;\">February</span> <span style=\"background-color: rgb(245, 255, 245); color: black;\">7,</span> <span style=\"background-color: rgb(251, 255, 251); color: black;\">2016,</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">at</span> <span style=\"background-color: rgb(237, 255, 237); color: black;\">Levi's</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">Stadium</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">in</span> <span style=\"background-color: rgb(225, 255, 225); color: black;\">the</span> <span style=\"background-color: rgb(228, 255, 228); color: black;\">San</span> <span style=\"background-color: rgb(225, 255, 225); color: black;\">Francisco</span> <span style=\"background-color: rgb(220, 255, 220); color: black;\">Bay</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">Area</span> <span style=\"background-color: rgb(252, 255, 252); color: black;\">at</span> <span style=\"background-color: rgb(211, 255, 211); color: black;\">Santa</span> <span style=\"background-color: rgb(252, 255, 252); color: black;\">Clara,</span> <span style=\"background-color: rgb(250, 255, 250); color: black;\">California.</span> <span style=\"background-color: rgb(243, 255, 243); color: black;\">As</span> <span style=\"background-color: rgb(65, 255, 65); color: black;\">this</span> <span style=\"background-color: rgb(220, 255, 220); color: black;\">was</span> <span style=\"background-color: rgb(194, 255, 194); color: black;\">the</span> <span style=\"background-color: rgb(181, 255, 181); color: black;\">50th</span> <span style=\"background-color: rgb(183, 255, 183); color: black;\">Super</span> <span style=\"background-color: rgb(137, 255, 137); color: black;\">Bowl,</span> <span style=\"background-color: rgb(110, 255, 110); color: black;\">the</span> <span style=\"background-color: rgb(158, 255, 158); color: black;\">league</span> <span style=\"background-color: rgb(190, 255, 190); color: black;\">emphasized</span> <span style=\"background-color: rgb(5, 255, 5); color: black;\">the</span> <span style=\"background-color: rgb(108, 255, 108); color: black;\">\"golden</span> <span style=\"background-color: rgb(178, 255, 178); color: black;\">anniversary\"</span> <span style=\"background-color: rgb(47, 255, 47); color: black;\">with</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">various</span> <span style=\"background-color: rgb(51, 255, 51); color: black;\">gold-themed</span> <span style=\"background-color: rgb(140, 255, 140); color: black;\">initiatives,</span> <span style=\"background-color: rgb(62, 255, 62); color: black;\">as</span> <span style=\"background-color: rgb(189, 255, 189); color: black;\">well</span> <span style=\"background-color: rgb(24, 255, 24); color: black;\">as</span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">temporarily</span> <span style=\"background-color: rgb(108, 255, 108); color: black;\">suspending</span> <span style=\"background-color: rgb(67, 255, 67); color: black;\">the</span> <span style=\"background-color: rgb(122, 255, 122); color: black;\">tradition</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">of</span> <span style=\"background-color: rgb(189, 255, 189); color: black;\">naming</span> <span style=\"background-color: rgb(210, 255, 210); color: black;\">each</span> <span style=\"background-color: rgb(230, 255, 230); color: black;\">Super</span> <span style=\"background-color: rgb(252, 255, 252); color: black;\">Bowl</span> <span style=\"background-color: rgb(254, 255, 254); color: black;\">game</span> <span style=\"background-color: rgb(246, 255, 246); color: black;\">with</span> <span style=\"background-color: rgb(232, 255, 232); color: black;\">Roman</span> <span style=\"background-color: rgb(244, 255, 244); color: black;\">numerals</span> <span style=\"background-color: rgb(159, 255, 159); color: black;\">(under</span> <span style=\"background-color: rgb(87, 255, 87); color: black;\">which</span> <span style=\"background-color: rgb(151, 255, 151); color: black;\">the</span> <span style=\"background-color: rgb(189, 255, 189); color: black;\">game</span> <span style=\"background-color: rgb(173, 255, 173); color: black;\">would</span> <span style=\"background-color: rgb(193, 255, 193); color: black;\">have</span> <span style=\"background-color: rgb(195, 255, 195); color: black;\">been</span> <span style=\"background-color: rgb(226, 255, 226); color: black;\">known</span> <span style=\"background-color: rgb(53, 255, 53); color: black;\">as</span> <span style=\"background-color: rgb(85, 255, 85); color: black;\">\"Super</span> <span style=\"background-color: rgb(253, 255, 253); color: black;\">Bowl</span> <span style=\"background-color: rgb(154, 255, 154); color: black;\">L\"),</span> <span style=\"background-color: rgb(69, 255, 69); color: black;\">so</span> <span style=\"background-color: rgb(77, 255, 77); color: black;\">that</span> <span style=\"background-color: rgb(26, 255, 26); color: black;\">the</span> <span style=\"background-color: rgb(172, 255, 172); color: black;\">logo</span> <span style=\"background-color: rgb(98, 255, 98); color: black;\">could</span> <span style=\"background-color: rgb(20, 255, 20); color: black;\">prominently</span> <span style=\"background-color: rgb(193, 255, 193); color: black;\">feature</span> <span style=\"background-color: rgb(26, 255, 26); color: black;\">the</span> <span style=\"background-color: rgb(204, 255, 204); color: black;\">Arabic</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">numerals</span> <span style=\"background-color: rgb(225, 255, 225); color: black;\">50</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(249, 255, 249); color: black;\">Question:</span> <span style=\"background-color: rgb(44, 255, 44); color: black;\">Which</span> <span style=\"background-color: rgb(180, 255, 180); color: black;\">NFL</span> <span style=\"background-color: rgb(233, 255, 233); color: black;\">team</span> <span style=\"background-color: rgb(231, 255, 231); color: black;\">represented</span> <span style=\"background-color: rgb(205, 255, 205); color: black;\">the</span> <span style=\"background-color: rgb(214, 255, 214); color: black;\">AFC</span> <span style=\"background-color: rgb(183, 255, 183); color: black;\">at</span> <span style=\"background-color: rgb(115, 255, 115); color: black;\">Super</span> <span style=\"background-color: rgb(242, 255, 242); color: black;\">Bowl</span> <span style=\"background-color: rgb(251, 255, 251); color: black;\">50?<|start_header_id|>assistant<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">The</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Denver</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Broncos</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">represented</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">American</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Football</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Conference</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">(AFC)</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">at</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Super</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Bowl</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">50.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, 4)\n",
    "test_ids = gen_tokens[idx]\n",
    "test_mask = gen_attention_mask[idx]\n",
    "test_mask_prob = torch.sigmoid(mask_logits[idx])\n",
    "test_context_mask = context_mask[idx]\n",
    "\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_ids)\n",
    "scores = test_mask_prob * test_context_mask\n",
    "def normalize_except_zeros(array):\n",
    "    # Create a mask to identify non-zero elements\n",
    "    mask = array != 0\n",
    "    \n",
    "    # Extract non-zero elements\n",
    "    non_zero_elements = array[mask]\n",
    "    \n",
    "    # Normalize non-zero elements\n",
    "    min_val = np.min(non_zero_elements)\n",
    "    max_val = np.max(non_zero_elements)\n",
    "    normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create a copy of the original array to preserve zero values\n",
    "    normalized_array = np.copy(array)\n",
    "    \n",
    "    # Assign normalized values back to the corresponding positions\n",
    "    normalized_array[mask] = normalized_non_zero_elements\n",
    "    \n",
    "    return normalized_array\n",
    "# scores = normalize_except_zeros(scores.detach().cpu().numpy())\n",
    "\n",
    "# remove special tokens\n",
    "filtered_token_scores = [(token, score) for token, score in zip(test_tokens, scores) if token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# combine subwords\n",
    "merged_tokens_scores = []\n",
    "current_token = \"\"\n",
    "current_score = 0\n",
    "count = 0\n",
    "\n",
    "for token, score in filtered_token_scores:\n",
    "    if token.startswith(\"Ġ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "            # merged_tokens_scores.append((\" \", 0))  # 添加空格\n",
    "        current_token = token[1:] # remove the speical character\n",
    "        current_score = score\n",
    "        count = 1\n",
    "    elif token.endswith(\"Ċ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "        merged_tokens_scores.append((\"<br><br>\", 0))  # 添加换行符\n",
    "        current_token = \"\"\n",
    "        current_score = 0\n",
    "        count = 0\n",
    "    else:\n",
    "        current_token += token\n",
    "        current_score += score\n",
    "        count += 1\n",
    "\n",
    "if current_token:\n",
    "    merged_tokens_scores.append((current_token, current_score / count))\n",
    "\n",
    "\n",
    "# 根据分数高亮文本（示例中使用HTML标签）\n",
    "highlighted_text = \"\"\n",
    "for token, score in merged_tokens_scores:\n",
    "    # 动态设置背景颜色：score为0时为白色，score为1时为绿色\n",
    "    red = int((1 - score) * 255)\n",
    "    green = 255\n",
    "    blue = int((1 - score) * 255)\n",
    "    color = f'rgb({red}, {green}, {blue})'\n",
    "    highlighted_text += f'<span style=\"background-color: {color}; color: black;\">{token}</span> '\n",
    "\n",
    "# 打印高亮后的文本\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(highlighted_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|start_header_id|>system<|end_header_id|>', tensor(0., device='cuda:0')),\n",
       " ('<br><br>', 0),\n",
       " ('You', tensor(0., device='cuda:0')),\n",
       " ('are', tensor(0., device='cuda:0')),\n",
       " ('a', tensor(0., device='cuda:0')),\n",
       " ('chatbot', tensor(0., device='cuda:0')),\n",
       " ('for', tensor(0., device='cuda:0')),\n",
       " ('answering', tensor(0., device='cuda:0')),\n",
       " ('questions.', tensor(0., device='cuda:0')),\n",
       " ('You', tensor(0., device='cuda:0')),\n",
       " ('can', tensor(0., device='cuda:0')),\n",
       " ('help', tensor(0., device='cuda:0')),\n",
       " ('users', tensor(0., device='cuda:0')),\n",
       " ('with', tensor(0., device='cuda:0')),\n",
       " ('their', tensor(0., device='cuda:0')),\n",
       " ('questions', tensor(0., device='cuda:0')),\n",
       " ('via', tensor(0., device='cuda:0')),\n",
       " ('concise', tensor(0., device='cuda:0')),\n",
       " ('responses.<|start_header_id|>user<|end_header_id|>',\n",
       "  tensor(0., device='cuda:0')),\n",
       " ('<br><br>', 0),\n",
       " ('Context:', tensor(0.5974, device='cuda:0')),\n",
       " ('Architecturally,', tensor(0.6714, device='cuda:0')),\n",
       " ('the', tensor(0.4965, device='cuda:0')),\n",
       " ('school', tensor(0.9941, device='cuda:0')),\n",
       " ('has', tensor(0.9663, device='cuda:0')),\n",
       " ('a', tensor(0.9561, device='cuda:0')),\n",
       " ('Catholic', tensor(0.6819, device='cuda:0')),\n",
       " ('character.', tensor(0.9793, device='cuda:0')),\n",
       " ('Atop', tensor(0.7949, device='cuda:0')),\n",
       " ('the', tensor(0.8092, device='cuda:0')),\n",
       " ('Main', tensor(0.1955, device='cuda:0')),\n",
       " (\"Building's\", tensor(0.9718, device='cuda:0')),\n",
       " ('gold', tensor(0.9452, device='cuda:0')),\n",
       " ('dome', tensor(0.8397, device='cuda:0')),\n",
       " ('is', tensor(0.2037, device='cuda:0')),\n",
       " ('a', tensor(0.2035, device='cuda:0')),\n",
       " ('golden', tensor(0.6326, device='cuda:0')),\n",
       " ('statue', tensor(0.8591, device='cuda:0')),\n",
       " ('of', tensor(0.0941, device='cuda:0')),\n",
       " ('the', tensor(0.1459, device='cuda:0')),\n",
       " ('Virgin', tensor(0.5461, device='cuda:0')),\n",
       " ('Mary.', tensor(0.8578, device='cuda:0')),\n",
       " ('Immediately', tensor(0.1131, device='cuda:0')),\n",
       " ('in', tensor(0.9012, device='cuda:0')),\n",
       " ('front', tensor(0.5482, device='cuda:0')),\n",
       " ('of', tensor(0.9571, device='cuda:0')),\n",
       " ('the', tensor(0.8358, device='cuda:0')),\n",
       " ('Main', tensor(0.9833, device='cuda:0')),\n",
       " ('Building', tensor(0.8420, device='cuda:0')),\n",
       " ('and', tensor(0.4897, device='cuda:0')),\n",
       " ('facing', tensor(0.3343, device='cuda:0')),\n",
       " ('it,', tensor(0.2152, device='cuda:0')),\n",
       " ('is', tensor(0.9616, device='cuda:0')),\n",
       " ('a', tensor(0.8361, device='cuda:0')),\n",
       " ('copper', tensor(0.6536, device='cuda:0')),\n",
       " ('statue', tensor(0.9418, device='cuda:0')),\n",
       " ('of', tensor(0.5049, device='cuda:0')),\n",
       " ('Christ', tensor(0.8380, device='cuda:0')),\n",
       " ('with', tensor(0.0553, device='cuda:0')),\n",
       " ('arms', tensor(0.3208, device='cuda:0')),\n",
       " ('upraised', tensor(0.7405, device='cuda:0')),\n",
       " ('with', tensor(0.0892, device='cuda:0')),\n",
       " ('the', tensor(0.0660, device='cuda:0')),\n",
       " ('legend', tensor(0.7704, device='cuda:0')),\n",
       " ('\"Venite', tensor(0.4984, device='cuda:0')),\n",
       " ('Ad', tensor(0.4740, device='cuda:0')),\n",
       " ('Me', tensor(0.8798, device='cuda:0')),\n",
       " ('Omnes\".', tensor(0.8512, device='cuda:0')),\n",
       " ('Next', tensor(0.4892, device='cuda:0')),\n",
       " ('to', tensor(0.8395, device='cuda:0')),\n",
       " ('the', tensor(0.4789, device='cuda:0')),\n",
       " ('Main', tensor(0.7229, device='cuda:0')),\n",
       " ('Building', tensor(0.8276, device='cuda:0')),\n",
       " ('is', tensor(0.9109, device='cuda:0')),\n",
       " ('the', tensor(0.9374, device='cuda:0')),\n",
       " ('Basilica', tensor(0.7037, device='cuda:0')),\n",
       " ('of', tensor(0.5103, device='cuda:0')),\n",
       " ('the', tensor(0.1298, device='cuda:0')),\n",
       " ('Sacred', tensor(0.0286, device='cuda:0')),\n",
       " ('Heart.', tensor(0.9792, device='cuda:0')),\n",
       " ('Immediately', tensor(0.0971, device='cuda:0')),\n",
       " ('behind', tensor(0.8532, device='cuda:0')),\n",
       " ('the', tensor(0.4568, device='cuda:0')),\n",
       " ('basilica', tensor(0.2679, device='cuda:0')),\n",
       " ('is', tensor(0.8837, device='cuda:0')),\n",
       " ('the', tensor(0.9210, device='cuda:0')),\n",
       " ('Grotto,', tensor(0.6862, device='cuda:0')),\n",
       " ('a', tensor(0.7198, device='cuda:0')),\n",
       " ('Marian', tensor(0.8833, device='cuda:0')),\n",
       " ('place', tensor(0.9888, device='cuda:0')),\n",
       " ('of', tensor(0.9533, device='cuda:0')),\n",
       " ('prayer', tensor(0.9938, device='cuda:0')),\n",
       " ('and', tensor(0.8147, device='cuda:0')),\n",
       " ('reflection.', tensor(0.9837, device='cuda:0')),\n",
       " ('It', tensor(0.1778, device='cuda:0')),\n",
       " ('is', tensor(0.4459, device='cuda:0')),\n",
       " ('a', tensor(0.6966, device='cuda:0')),\n",
       " ('replica', tensor(0.8252, device='cuda:0')),\n",
       " ('of', tensor(0.1977, device='cuda:0')),\n",
       " ('the', tensor(0.0284, device='cuda:0')),\n",
       " ('grotto', tensor(0.2396, device='cuda:0')),\n",
       " ('at', tensor(0.8095, device='cuda:0')),\n",
       " ('Lourdes,', tensor(0.6710, device='cuda:0')),\n",
       " ('France', tensor(0.9943, device='cuda:0')),\n",
       " ('where', tensor(0.2345, device='cuda:0')),\n",
       " ('the', tensor(0.4051, device='cuda:0')),\n",
       " ('Virgin', tensor(0.0585, device='cuda:0')),\n",
       " ('Mary', tensor(0.1142, device='cuda:0')),\n",
       " ('reputedly', tensor(0.1153, device='cuda:0')),\n",
       " ('appeared', tensor(0.6197, device='cuda:0')),\n",
       " ('to', tensor(0.6124, device='cuda:0')),\n",
       " ('Saint', tensor(0.8413, device='cuda:0')),\n",
       " ('Bernadette', tensor(0.3881, device='cuda:0')),\n",
       " ('Soubirous', tensor(0.4007, device='cuda:0')),\n",
       " ('in', tensor(0.5173, device='cuda:0')),\n",
       " ('1858.', tensor(0.7425, device='cuda:0')),\n",
       " ('At', tensor(0.7603, device='cuda:0')),\n",
       " ('the', tensor(0.3962, device='cuda:0')),\n",
       " ('end', tensor(0.9508, device='cuda:0')),\n",
       " ('of', tensor(0.9292, device='cuda:0')),\n",
       " ('the', tensor(0.2649, device='cuda:0')),\n",
       " ('main', tensor(0.2604, device='cuda:0')),\n",
       " ('drive', tensor(0.6045, device='cuda:0')),\n",
       " ('(and', tensor(0.3410, device='cuda:0')),\n",
       " ('in', tensor(0.6963, device='cuda:0')),\n",
       " ('a', tensor(0.9185, device='cuda:0')),\n",
       " ('direct', tensor(0.7158, device='cuda:0')),\n",
       " ('line', tensor(0.2233, device='cuda:0')),\n",
       " ('that', tensor(0.1704, device='cuda:0')),\n",
       " ('connects', tensor(0.5676, device='cuda:0')),\n",
       " ('through', tensor(0.8583, device='cuda:0')),\n",
       " ('3', tensor(0.5012, device='cuda:0')),\n",
       " ('statues', tensor(0.4610, device='cuda:0')),\n",
       " ('and', tensor(0.1058, device='cuda:0')),\n",
       " ('the', tensor(0.1088, device='cuda:0')),\n",
       " ('Gold', tensor(0.7601, device='cuda:0')),\n",
       " ('Dome),', tensor(0.5617, device='cuda:0')),\n",
       " ('is', tensor(0.9200, device='cuda:0')),\n",
       " ('a', tensor(0.6096, device='cuda:0')),\n",
       " ('simple,', tensor(0.4601, device='cuda:0')),\n",
       " ('modern', tensor(0.9568, device='cuda:0')),\n",
       " ('stone', tensor(0.9441, device='cuda:0')),\n",
       " ('statue', tensor(0.5584, device='cuda:0')),\n",
       " ('of', tensor(0.5442, device='cuda:0')),\n",
       " ('Mary', tensor(0.2988, device='cuda:0')),\n",
       " ('<br><br>', 0),\n",
       " ('Question:', tensor(0.8829, device='cuda:0')),\n",
       " ('What', tensor(0.2423, device='cuda:0')),\n",
       " ('is', tensor(0.2881, device='cuda:0')),\n",
       " ('the', tensor(0.3282, device='cuda:0')),\n",
       " ('Grotto', tensor(0.3373, device='cuda:0')),\n",
       " ('at', tensor(0.9002, device='cuda:0')),\n",
       " ('Notre', tensor(0.0377, device='cuda:0')),\n",
       " ('Dame?<|start_header_id|>assistant<|end_header_id|>',\n",
       "  tensor(0.3931, device='cuda:0')),\n",
       " ('<br><br>', 0),\n",
       " ('The', tensor(0., device='cuda:0')),\n",
       " ('Grotto', tensor(0., device='cuda:0')),\n",
       " ('at', tensor(0., device='cuda:0')),\n",
       " ('Notre', tensor(0., device='cuda:0')),\n",
       " ('Dame', tensor(0., device='cuda:0')),\n",
       " ('is', tensor(0., device='cuda:0')),\n",
       " ('a', tensor(0., device='cuda:0')),\n",
       " ('Marian', tensor(0., device='cuda:0')),\n",
       " ('place', tensor(0., device='cuda:0')),\n",
       " ('of', tensor(0., device='cuda:0')),\n",
       " ('prayer', tensor(0., device='cuda:0')),\n",
       " ('and', tensor(0., device='cuda:0')),\n",
       " ('reflection', tensor(0., device='cuda:0')),\n",
       " ('that', tensor(0., device='cuda:0')),\n",
       " ('is', tensor(0., device='cuda:0')),\n",
       " ('a', tensor(0., device='cuda:0')),\n",
       " ('replica', tensor(0., device='cuda:0')),\n",
       " ('of', tensor(0., device='cuda:0')),\n",
       " ('the', tensor(0., device='cuda:0')),\n",
       " ('grotto', tensor(0., device='cuda:0')),\n",
       " ('at', tensor(0., device='cuda:0')),\n",
       " ('Lourdes,', tensor(0., device='cuda:0')),\n",
       " ('France,', tensor(0., device='cuda:0')),\n",
       " ('where', tensor(0., device='cuda:0')),\n",
       " ('the', tensor(0., device='cuda:0')),\n",
       " ('Virgin', tensor(0., device='cuda:0')),\n",
       " ('Mary', tensor(0., device='cuda:0')),\n",
       " ('reputedly', tensor(0., device='cuda:0')),\n",
       " ('appeared', tensor(0., device='cuda:0')),\n",
       " ('to', tensor(0., device='cuda:0')),\n",
       " ('Saint', tensor(0., device='cuda:0')),\n",
       " ('Bernadette', tensor(0., device='cuda:0')),\n",
       " ('Soubirous', tensor(0., device='cuda:0')),\n",
       " ('in', tensor(0., device='cuda:0')),\n",
       " ('1858.', tensor(0., device='cuda:0'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2052, 0.0336, 0.0175, 0.0591, 0.0214, 0.0514,\n",
       "        0.0453, 0.1790, 0.3275, 0.0885, 0.4151, 0.2449, 0.7812, 0.2025, 0.1918,\n",
       "        0.1083, 0.4665, 0.1082, 0.5029, 0.7354, 0.1680, 0.1983, 0.0222, 0.1115,\n",
       "        0.2571, 0.0972, 0.0252, 0.3280, 0.3500, 0.7719, 0.1022, 0.4868, 0.6470,\n",
       "        0.4235, 0.3930, 0.1858, 0.3415, 0.2573, 0.2125, 0.4739, 0.7803, 0.9506,\n",
       "        0.1826, 0.5753, 0.7123, 0.3407, 0.8478, 0.1610, 0.5789, 0.7452, 0.4866,\n",
       "        0.3256, 0.4797, 0.1182, 0.1025, 0.0717, 0.3000, 0.4712, 0.3427, 0.5447,\n",
       "        0.0234, 0.9369, 0.0532, 0.6967, 0.0184, 0.0097, 0.3318, 0.9282, 0.5267,\n",
       "        0.8113, 0.4950, 0.2794, 0.3564, 0.3344, 0.3029, 0.6510, 0.1335, 0.3388,\n",
       "        0.2056, 0.0442, 0.2567, 0.8161, 0.3769, 0.0220, 0.0137, 0.0120, 0.0532,\n",
       "        0.0799, 0.0381, 0.0894, 0.0241, 0.2642, 0.1161, 0.1233, 0.6457, 0.5571,\n",
       "        0.0116, 0.0060, 0.2440, 0.0268, 0.0458, 0.0012, 0.0099, 0.0091, 0.2315,\n",
       "        0.1482, 0.1388, 0.0862, 0.0404, 0.0597, 0.3075, 0.3525, 0.2076, 0.6788,\n",
       "        0.5865, 0.2197, 0.0196, 0.2495, 0.0269, 0.0028, 0.0252, 0.0193, 0.0529,\n",
       "        0.3263, 0.2007, 0.0757, 0.0671, 0.2068, 0.0018, 0.3846, 0.8290, 0.0470,\n",
       "        0.8867, 0.4626, 0.2914, 0.1257, 0.1422, 0.3466, 0.8361, 0.6511, 0.1813,\n",
       "        0.0261, 0.3762, 0.7019, 0.4439, 0.8600, 0.9006, 0.1634, 0.8193, 0.8355,\n",
       "        0.9367, 0.2113, 0.8023, 0.4347, 0.8024, 0.5939, 0.0793, 0.6613, 0.5426,\n",
       "        0.4776, 0.3045, 0.6982, 0.1078, 0.6654, 0.8120, 0.2537, 0.0051, 0.0687,\n",
       "        0.0279, 0.0381, 0.1286, 0.0195, 0.0208, 0.0551, 0.0503, 0.0352, 0.0276,\n",
       "        0.0229, 0.9374, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask_prob * test_context_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.63934755, 0.11451219, 0.1225187 , 0.43457144, 0.21965683,\n",
       "       0.28944358, 0.08941142, 0.10701324, 0.09935488, 0.05571543,\n",
       "       0.03000107, 0.07527877, 0.15953447, 0.22993316, 0.15300442,\n",
       "       0.3738694 , 0.22266376, 0.29959634, 0.4525226 , 0.11873144,\n",
       "       0.2386478 , 0.20134698, 0.11468548, 0.33462942, 0.12546155,\n",
       "       0.29114434, 0.3496375 , 0.27918556, 0.8707584 , 0.42532292,\n",
       "       0.18798777, 0.88152486, 0.7778979 , 0.38002315, 0.0177369 ,\n",
       "       0.34096608, 0.19461636, 0.55110186, 0.40781736, 0.6573673 ,\n",
       "       0.56073976, 0.7197896 , 0.5075476 , 0.49354845, 0.57938504,\n",
       "       0.2639287 , 0.53644824, 0.74284345, 0.43071514, 0.17025682,\n",
       "       0.2519394 , 0.24771284, 0.76228124, 0.03937887, 0.31321433,\n",
       "       0.3951155 , 0.12060348, 0.8391926 , 0.19060197, 0.14503507,\n",
       "       0.38200086, 0.09805975, 0.53167886, 0.45860618, 0.34170774,\n",
       "       0.38000423, 0.49717534, 0.12770228, 0.54148155, 0.88480836,\n",
       "       0.5025979 , 0.2180987 , 0.44277272, 0.7061062 , 0.6001298 ,\n",
       "       0.4190885 , 0.15418284, 0.01286074, 0.03205981, 0.7166164 ,\n",
       "       0.42272195, 0.1009967 , 0.5880632 , 0.06157847, 0.03148317,\n",
       "       0.01150252, 0.14847231, 0.40222263, 0.45739612, 0.29020867,\n",
       "       0.20442684, 0.01632834, 0.00439288, 0.22261323, 0.10954274,\n",
       "       0.735632  , 0.250885  , 0.23412147, 0.52762663, 0.6082385 ,\n",
       "       0.70563006, 0.18747613, 0.03181224, 0.1954265 , 0.46577016,\n",
       "       0.08638231, 0.587938  , 0.17073946, 0.19899334, 0.44022155,\n",
       "       0.28126696, 0.18912274, 0.606753  , 0.2354937 , 0.30333954,\n",
       "       0.9088377 , 0.760515  , 0.00247158, 0.65670216, 0.37467483,\n",
       "       0.8893074 , 0.49679863, 0.8678529 , 0.64698756, 0.921829  ,\n",
       "       0.32185513, 0.11627685, 0.19978185, 0.5822873 , 0.46432415,\n",
       "       0.27071175, 0.26083463, 0.35565257, 0.11644382, 0.12587062,\n",
       "       0.02788724, 0.04585992, 0.23267932, 0.52556354, 0.12978171,\n",
       "       0.05500807, 0.86675435, 0.59887177, 0.13369308, 0.37950924,\n",
       "       0.836148  , 0.1870308 , 0.6357755 , 0.18462983, 0.05141451,\n",
       "       0.2866516 , 0.3967239 , 0.9536885 , 0.36054942, 0.44063833,\n",
       "       0.2609197 , 0.61237025, 0.15894488, 0.21989977, 0.69814706,\n",
       "       0.37813613, 0.8174311 , 0.8459768 , 0.77438444, 0.1087799 ,\n",
       "       0.03142452, 0.8144604 , 0.42925656, 0.87952363, 0.12044684,\n",
       "       0.32651398, 0.3073709 , 0.15482455, 0.03003251, 0.5409264 ,\n",
       "       0.06059632, 0.23830052, 0.28520328, 0.5085325 , 0.29540935,\n",
       "       0.17819822, 0.05908529, 0.49207908, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1241, 0.2500, 0.7787, 0.0909, 0.1908, 0.1361, 0.3316, 0.5104, 0.2696,\n",
       "        0.1001, 0.5931, 0.0812, 0.4139, 0.3334, 0.4853, 0.2987],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob = torch.sigmoid(mask_logits)\n",
    "(mask_prob * context_mask).sum(-1) / context_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9998e-01, 1.3412e-02, 2.1168e-05, 7.2758e-04, 9.5831e-01, 2.8712e-06,\n",
       "         1.9530e-03, 6.8251e-04, 9.6814e-01, 7.6582e-04, 2.2164e-06, 1.7009e-02,\n",
       "         2.4596e-09, 1.4519e-04, 3.0463e-06, 7.9092e-04, 4.0336e-06, 5.2874e-05,\n",
       "         1.4755e-07, 5.3417e-09, 1.0859e-07, 8.2496e-07, 1.0534e-05, 2.1604e-06,\n",
       "         6.6980e-10, 1.8266e-01, 5.4030e-07, 4.0081e-02, 3.5185e-04, 3.1933e-01,\n",
       "         3.2836e-09, 1.5197e-06, 8.2054e-01, 8.0839e-01, 5.4567e-01, 4.8829e-01,\n",
       "         1.0000e+00, 4.3639e-05, 3.5097e-07, 3.8482e-09, 1.9672e-04, 1.1242e-07,\n",
       "         2.7982e-13, 5.4683e-09, 4.5730e-01, 9.5505e-01, 6.7815e-04, 7.4368e-02,\n",
       "         9.9924e-01, 6.6332e-03, 1.0508e-08, 1.8774e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,   2675,    527,    264,   6369,\n",
       "          6465,    369,  27065,   6492,     13,   1472,    649,   1520,   3932,\n",
       "           449,    872,   4860,   4669,  64694,  14847,    315,  27592,  45450,\n",
       "            11,    477,  85165,  24093,     13, 128009, 128006,    882, 128007,\n",
       "           271,   2028,   5818,    574,    279,   1888,   5818,    358,    617,\n",
       "          3596,   3970,      0,   1063,  16451,   1051,  27873,     11,    719,\n",
       "         15718,    574,   2294,     13, 128009, 128006,  78191, 128007,    271],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs['input_ids'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĊĊ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(271)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.6432 0.8566 0.5179\n",
      " 0.2417 0.     0.1211 0.3355 0.618  0.5345 0.1401 0.3401 0.4729 0.3531\n",
      " 0.661  0.7049 0.0297 0.1724 0.9905 1.     0.1606 0.1107 0.2363 0.2891\n",
      " 0.116  0.0777 0.     0.     0.     0.     0.     0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.8527, 0.1770, 0.1719, 0.1660, 0.1613, 0.1549, 0.1581, 0.1622, 0.1666,\n",
       "        0.1635, 0.1568, 0.1589, 0.1618, 0.1634, 0.1668, 0.1647, 0.1546, 0.1576,\n",
       "        0.1788, 0.1779, 0.1592, 0.1581, 0.1607, 0.1619, 0.1586, 0.1583, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sigmoid(mask_logits) * context_mask)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4822, 0.4821, 0.4816,  ..., 0.4920, 0.4874, 0.4901],\n",
       "        [0.4843, 0.4851, 0.4855,  ..., 0.4941, 0.4861, 0.4891],\n",
       "        [0.4785, 0.4805, 0.4805,  ..., 0.4918, 0.4823, 0.4864],\n",
       "        ...,\n",
       "        [0.4753, 0.4758, 0.4761,  ..., 0.4847, 0.4761, 0.4802],\n",
       "        [0.4876, 0.4883, 0.4882,  ..., 0.4887, 0.4880, 0.4943],\n",
       "        [0.4843, 0.4851, 0.4852,  ..., 0.4946, 0.4853, 0.4947]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(mask_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskGeneratingModel(\n",
       "  (explain_map): MLP(\n",
       "    (input_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (attention_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tokens[35])\n",
    "print(expl[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I really didn't like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "messages_lambda = lambda texts: [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"},\n",
    "    # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": texts},\n",
    "]\n",
    "messages = messages_lambda(texts)\n",
    "messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "print(messages_with_template_applied)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
