{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from transformers import LlamaTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from llmexp.helper import LlmExpHelper\n",
    "class LlmExpHelper:\n",
    "    def __init__(self, tokenizer, model, device):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def get_collate_fun(self):\n",
    "        return lambda examples: self.collate_fn(examples)\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        def num_words(x):\n",
    "            return len(x.split())\n",
    "        def get_first_k_words(x, k):\n",
    "            return ' '.join(x.split()[:k])\n",
    "        tokenizer = self.tokenizer\n",
    "        max_len = 256 # characters limit other than token limit\n",
    "        # texts = [example['sentence'] for example in examples]\n",
    "        texts = [example['text'] for example in examples]\n",
    "        texts = [text if num_words(text) <= max_len else get_first_k_words(text, max_len) for text in texts]\n",
    "        labels = [example['label'] for example in examples]\n",
    "        messages_lambda = lambda texts: [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"},\n",
    "            # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": texts},\n",
    "        ]\n",
    "        messages = list(map(messages_lambda, texts))\n",
    "\n",
    "        messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        batch = tokenizer(\n",
    "                    messages_with_template_applied,\n",
    "                    add_special_tokens=False,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    )\n",
    "        \n",
    "        # find the template boundaries\n",
    "        text_lens = [len(tokenizer.encode(text)) - 1 for text in texts] # note that the tokenizer.encode adds the eos token\n",
    "        text_lens_tensor = torch.tensor(text_lens, dtype=torch.long)\n",
    "        \n",
    "        def apply_mask(mask_tensor, text_lens_tensor):\n",
    "            batch_size, seq_len = mask_tensor.shape\n",
    "            for i in range(batch_size):\n",
    "                text_len = text_lens_tensor[i].item()\n",
    "                mask_tensor[i, -text_len-5:-5] = 0\n",
    "            return 1- mask_tensor\n",
    "\n",
    "        mask_tensor = apply_mask(torch.ones_like(batch['input_ids']), text_lens_tensor)\n",
    "\n",
    "        batch['context_mask'] = mask_tensor\n",
    "        \n",
    "        return batch\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "train_ds = imdb['train']\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/sst2\")\n",
    "# train_ds = ds['train']\n",
    "llm_exp_helper = LlmExpHelper(tokenizer, model, device)\n",
    "collate_fn = llm_exp_helper.get_collate_fun()\n",
    "\n",
    "# Define batch size here!\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.model4 import MaskGeneratingModel\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(hidden_size=4096, mlp_hidden_dim=1024, mlp_bottleneck_dim=768, mlp_num_blocks=2)\n",
    "mask_gen_model.to(device)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 52,  81,  30,  40,  26,  94,  92, 102,  58,  55,  53, 105,  42,  45,\n",
      "         95,  63], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = 0.5636, Reward Loss = 0.5636, Mean Reward = 0.9149,Mask_loss = 0.5800 mask_mean = 0.5800:   0%|          | 1/1563 [00:01<49:48,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 86,  51,  23,  32,  42,  26,  60, 102,  93,  52,  44, 101,  88,  51,\n",
      "         99, 105], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 2: Loss = 0.4318, Reward Loss = 0.4318, Mean Reward = 0.8772,Mask_loss = 0.1782 mask_mean = 0.1782:   0%|          | 2/1563 [00:03<44:46,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 45,  99,  66,  66,  22,  90,  89,  96,  42,  29, 102,  93,  46, 105,\n",
      "         89,  96], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 3: Loss = 0.5403, Reward Loss = 0.5403, Mean Reward = 0.9548,Mask_loss = 0.1163 mask_mean = 0.1163:   0%|          | 3/1563 [00:05<43:19,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([102,  42,  42,  48,  51,  98,  92,  96,  40,  59,  53,  46, 108,  97,\n",
      "         46,  71], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 4: Loss = 0.5120, Reward Loss = 0.5120, Mean Reward = 0.9515,Mask_loss = 0.1285 mask_mean = 0.1285:   0%|          | 4/1563 [00:06<42:55,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([69, 50,  4, 46, 50, 63, 57, 31, 36, 44, 52, 49, 57, 78, 33, 49],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 5: Loss = 0.3781, Reward Loss = 0.3781, Mean Reward = 0.8423,Mask_loss = 0.1867 mask_mean = 0.1867:   0%|          | 5/1563 [00:07<39:14,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 52,  95,  39, 104,  77,  78,  86,  94, 109,  51,  61,  45, 103,  97,\n",
      "         45,  64], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 6: Loss = 0.3466, Reward Loss = 0.3466, Mean Reward = 0.8265,Mask_loss = 0.3812 mask_mean = 0.3812:   0%|          | 6/1563 [00:09<40:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([101,  57,  68,  94,  45,  68,  32, 101,  47,  81,  38,  93,  99,  96,\n",
      "         57,  89], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 7: Loss = 0.4073, Reward Loss = 0.4073, Mean Reward = 0.8694,Mask_loss = 0.4888 mask_mean = 0.4888:   0%|          | 7/1563 [00:11<39:54,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 74,  99,  92,  36, 108, 102,  21,  69,  24,  96,  15,  97,  47,  64,\n",
      "         97,  32], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 8: Loss = 0.3257, Reward Loss = 0.3257, Mean Reward = 0.7201,Mask_loss = 0.4786 mask_mean = 0.4786:   1%|          | 8/1563 [00:12<40:43,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 49,  46,  27,  95,  94,  86,  90,  42,  60,  64,  86,  70,  99,  77,\n",
      "        106,  76], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 9: Loss = 0.3296, Reward Loss = 0.3296, Mean Reward = 0.8463,Mask_loss = 0.4308 mask_mean = 0.4308:   1%|          | 9/1563 [00:14<41:03,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42, 50, 91, 94, 49, 29, 51, 33, 95, 48, 19, 35, 91, 49, 56, 65],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 10: Loss = 0.2991, Reward Loss = 0.2991, Mean Reward = 0.8506,Mask_loss = 0.3868 mask_mean = 0.3868:   1%|          | 10/1563 [00:15<40:12,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 62,  24,  87,  92,  53,  83,  39,  45,  63,  89, 113,  68,  37,  24,\n",
      "         87,  95], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.2635, Reward Loss = 0.2635, Mean Reward = 0.8151,Mask_loss = 0.3082 mask_mean = 0.3082:   1%|          | 11/1563 [00:17<41:21,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 32,  14,  54,  21,  21,  67,  98,  69,  97,  73,  93,  55,  72, 108,\n",
      "        103,  96], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 12: Loss = 0.2536, Reward Loss = 0.2536, Mean Reward = 0.8240,Mask_loss = 0.2745 mask_mean = 0.2745:   1%|          | 12/1563 [00:19<41:33,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 57,  90,  54,  48,  45,  51,  72,  93,  72,  78,  22,  33,  28, 105,\n",
      "         79,  56], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 13: Loss = 0.2158, Reward Loss = 0.2158, Mean Reward = 0.7484,Mask_loss = 0.2371 mask_mean = 0.2371:   1%|          | 13/1563 [00:20<41:29,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 17,  53,  55,  17,  54,  61, 103,  53,  96,  49,  91,  45,  91,  51,\n",
      "         79,  64], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 14: Loss = 0.2060, Reward Loss = 0.2060, Mean Reward = 0.6676,Mask_loss = 0.2021 mask_mean = 0.2021:   1%|          | 14/1563 [00:22<41:17,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 61, 104,  44,  89,  59,  51,  71,  51,  48,  94,  10,  37,  25,  47,\n",
      "         28,  87], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 15: Loss = 0.2163, Reward Loss = 0.2163, Mean Reward = 0.8197,Mask_loss = 0.2332 mask_mean = 0.2332:   1%|          | 15/1563 [00:23<41:12,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 58,  40,  50,  76,  81,  24,  90, 100,  67,  84,  84,  87,  63,  47,\n",
      "         96,  88], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 16: Loss = 0.2017, Reward Loss = 0.2017, Mean Reward = 0.8390,Mask_loss = 0.2247 mask_mean = 0.2247:   1%|          | 16/1563 [00:25<40:28,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 97,  42,  84,  75,  53,  66,  54,  67,  98,  99,  51,  45,  90,  52,\n",
      "         58, 105], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 17: Loss = 0.1681, Reward Loss = 0.1681, Mean Reward = 0.6952,Mask_loss = 0.2389 mask_mean = 0.2389:   1%|          | 17/1563 [00:30<1:03:36,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46, 96, 77, 62, 80, 91, 16, 95, 68, 12, 47, 52, 45, 45, 93, 67],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 18: Loss = 0.1680, Reward Loss = 0.1680, Mean Reward = 0.8282,Mask_loss = 0.2939 mask_mean = 0.2939:   1%|          | 18/1563 [00:31<55:54,  2.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 64,  29,  46,  97,  59,  35,  56,  16,  24,  99,  50, 100,  45,  68,\n",
      "         36,  51], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 19: Loss = 0.1809, Reward Loss = 0.1809, Mean Reward = 0.8868,Mask_loss = 0.3355 mask_mean = 0.3355:   1%|          | 19/1563 [00:33<50:43,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 26,  41,  60,  49,  23,  33,  98,  53, 106,  55,  51,  64,  57,  47,\n",
      "         78, 103], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 20: Loss = 0.1722, Reward Loss = 0.1722, Mean Reward = 0.9795,Mask_loss = 0.3380 mask_mean = 0.3380:   1%|▏         | 20/1563 [00:34<47:57,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100,  19,  65,  61,  99,  52,  98,  89,  44,  24,  94,  98,  62,  61,\n",
      "         85,  85], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 21: Loss = 0.1473, Reward Loss = 0.1473, Mean Reward = 0.8420,Mask_loss = 0.3590 mask_mean = 0.3590:   1%|▏         | 21/1563 [00:36<47:16,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 41,  86, 102,  96, 114,  57,  23,  51,  92,  15,  88,  84,  71,  92,\n",
      "         74,  67], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 22: Loss = 0.1485, Reward Loss = 0.1485, Mean Reward = 0.8366,Mask_loss = 0.3593 mask_mean = 0.3593:   1%|▏         | 22/1563 [00:38<46:17,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([55, 45, 94, 66, 66, 91, 61, 30, 92, 20, 48, 93, 75, 95, 25, 99],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 23: Loss = 0.1234, Reward Loss = 0.1234, Mean Reward = 0.8674,Mask_loss = 0.3280 mask_mean = 0.3280:   1%|▏         | 23/1563 [00:39<44:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 72,  98,  24,  54,  43,  84,  15,  91,  51, 104,  60,  99,  85,  88,\n",
      "        104,  42], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 24: Loss = 0.1214, Reward Loss = 0.1214, Mean Reward = 0.8540,Mask_loss = 0.3343 mask_mean = 0.3343:   2%|▏         | 24/1563 [00:41<43:06,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 95, 107,  41,  71, 101,  69, 104,  92,  96,  78,  94,  17,  43,  15,\n",
      "        100,  11], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 25: Loss = 0.1032, Reward Loss = 0.1032, Mean Reward = 0.8923,Mask_loss = 0.3080 mask_mean = 0.3080:   2%|▏         | 25/1563 [00:42<42:48,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 95,  43,  48,  57,  69,  94, 109,  61,  95,  42,  98,  93,  75,  92,\n",
      "         86,  29], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 26: Loss = 0.0991, Reward Loss = 0.0991, Mean Reward = 0.9124,Mask_loss = 0.3007 mask_mean = 0.3007:   2%|▏         | 26/1563 [00:44<42:38,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([93, 40, 27, 47, 17, 83, 58, 29, 57, 92, 55, 61, 24, 95, 56, 46],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 27: Loss = 0.1010, Reward Loss = 0.1010, Mean Reward = 0.7861,Mask_loss = 0.2830 mask_mean = 0.2830:   2%|▏         | 27/1563 [00:46<44:12,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 44,  76, 101,  44,  63,  93, 108,  68,  81,  16,  50,  90,  67,  55,\n",
      "         99,  97], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 28: Loss = 0.1111, Reward Loss = 0.1111, Mean Reward = 0.8433,Mask_loss = 0.2639 mask_mean = 0.2639:   2%|▏         | 28/1563 [00:48<43:30,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 80,  90, 104,  49,  56,  53, 105, 110,  15,  78,  55,  59,  65,  42,\n",
      "         37,  53], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 29: Loss = 0.1285, Reward Loss = 0.1285, Mean Reward = 0.9934,Mask_loss = 0.2551 mask_mean = 0.2551:   2%|▏         | 29/1563 [00:49<43:08,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([95, 25, 84, 92, 45, 92, 96, 25, 96, 39, 84, 45, 40, 63, 78, 28],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 30: Loss = 0.0756, Reward Loss = 0.0756, Mean Reward = 0.7231,Mask_loss = 0.2763 mask_mean = 0.2763:   2%|▏         | 30/1563 [00:51<41:33,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 86,  99,  48,  43,  87,  76, 105,  95,  50,  38,  39,  41,  55,  51,\n",
      "        103,  50], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 31: Loss = 0.1128, Reward Loss = 0.1128, Mean Reward = 0.9290,Mask_loss = 0.2721 mask_mean = 0.2721:   2%|▏         | 31/1563 [00:52<41:28,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([105,  45,  95, 100,  92,  97, 101,  21,  51,  82,  70,  81,  35,  95,\n",
      "         46,  21], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 32: Loss = 0.0835, Reward Loss = 0.0835, Mean Reward = 0.8282,Mask_loss = 0.2787 mask_mean = 0.2787:   2%|▏         | 32/1563 [00:54<41:22,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 63,  64,  56,  90,  64,  30,  47,  51,  59,  54, 102,  23,  94, 101,\n",
      "         88, 101], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 33: Loss = 0.0669, Reward Loss = 0.0669, Mean Reward = 0.7684,Mask_loss = 0.3037 mask_mean = 0.3037:   2%|▏         | 33/1563 [00:56<42:47,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 48,  54,  89, 106,  44,  93,  41,  77,  48,  98,  42,  65, 109,  66,\n",
      "         92,  44], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 34: Loss = 0.0751, Reward Loss = 0.0751, Mean Reward = 0.9185,Mask_loss = 0.2934 mask_mean = 0.2934:   2%|▏         | 34/1563 [00:57<42:34,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 97,  34,  46,  63,  52,  96,  61,  96,  84,  48,  60,  42,  18,  39,\n",
      "        104,  50], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 35: Loss = 0.0828, Reward Loss = 0.0828, Mean Reward = 0.8540,Mask_loss = 0.3183 mask_mean = 0.3183:   2%|▏         | 35/1563 [00:59<42:09,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 47,  67,  39,  48,  55,  97,  90, 103,  99,  42,  72,  60,  37,  51,\n",
      "         43,  78], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 36: Loss = 0.0664, Reward Loss = 0.0664, Mean Reward = 0.7716,Mask_loss = 0.3055 mask_mean = 0.3055:   2%|▏         | 36/1563 [01:01<41:47,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 55,  60,  57,  47, 102,  95, 109,  48,  17,  74,  57,  24,  80,  46,\n",
      "         99,  52], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 37: Loss = 0.0681, Reward Loss = 0.0681, Mean Reward = 0.7679,Mask_loss = 0.2997 mask_mean = 0.2997:   2%|▏         | 37/1563 [01:02<41:51,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83, 76, 47, 54, 46, 96, 95, 81, 94, 54, 28, 80, 90, 46, 62, 54],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 38: Loss = 0.1048, Reward Loss = 0.1048, Mean Reward = 0.9832,Mask_loss = 0.3362 mask_mean = 0.3362:   2%|▏         | 38/1563 [01:04<40:32,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27, 42, 44, 62, 22, 43, 96, 34, 38, 35, 78, 34,  9, 18, 74, 47],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 39: Loss = 0.0849, Reward Loss = 0.0849, Mean Reward = 0.9428,Mask_loss = 0.2996 mask_mean = 0.2996:   2%|▏         | 39/1563 [01:05<39:45,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 48,  15,  29,  91,  45, 102,  15, 102,  86,  51,  75,  46,  83,  29,\n",
      "         88,  91], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 40: Loss = 0.0789, Reward Loss = 0.0789, Mean Reward = 0.9042,Mask_loss = 0.3168 mask_mean = 0.3168:   3%|▎         | 40/1563 [01:07<40:03,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([66, 61, 93, 32, 57, 62, 64, 94, 53, 48, 75, 69, 94, 50, 42, 82],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 41: Loss = 0.0471, Reward Loss = 0.0471, Mean Reward = 0.6835,Mask_loss = 0.3086 mask_mean = 0.3086:   3%|▎         | 41/1563 [01:08<39:10,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 88,  54,  45,  57,  96,  48,  59,  45,  86,  50,  67, 103, 100,  63,\n",
      "         82,  75], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 42: Loss = 0.0511, Reward Loss = 0.0511, Mean Reward = 0.6714,Mask_loss = 0.3030 mask_mean = 0.3030:   3%|▎         | 42/1563 [01:10<39:40,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 81,  44,  96,  22,  31,  50,  98,  35,  94,  50,  52, 105,  59,  61,\n",
      "         81,  96], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 43: Loss = 0.0607, Reward Loss = 0.0607, Mean Reward = 0.7981,Mask_loss = 0.2883 mask_mean = 0.2883:   3%|▎         | 43/1563 [01:12<39:59,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 96,  89,  24,  19,  52,  77,  51,  56,  97,  49,  51, 115,  23,  61,\n",
      "         47,  99], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 44: Loss = 0.0538, Reward Loss = 0.0538, Mean Reward = 0.8286,Mask_loss = 0.2924 mask_mean = 0.2924:   3%|▎         | 44/1563 [01:13<41:10,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 46,  95,  45,  16, 104,  67,  99,  52,  63,  27,  42,  39,  45,  47,\n",
      "         69,  64], device='cuda:0', dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 45: Loss = 0.0775, Reward Loss = 0.0775, Mean Reward = 0.9086,Mask_loss = 0.2698 mask_mean = 0.2698:   3%|▎         | 45/1563 [01:16<42:52,  1.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m last_hidden_state\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     41\u001b[0m mask_logits \u001b[38;5;241m=\u001b[39m mask_gen_model(last_hidden_state)\n\u001b[0;32m---> 43\u001b[0m mask_gen_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss, reward_loss, mask_loss, mask_mean, mean_reward \u001b[38;5;241m=\u001b[39m mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_mean\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_gen_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m log \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     47\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_mean = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_mean\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/model4.py:259\u001b[0m, in \u001b[0;36mMaskGeneratingModel.loss_func\u001b[0;34m(self, model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, num_samples)\u001b[0m\n\u001b[1;32m    255\u001b[0m mask_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(mask_logits)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# print('mask_prob', mask_prob[0])\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# obtain the perturbed_samples\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m perturbed_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_one_batch_multi_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# obtain the groundtruth similarity (similarity of the generated tokens without perturbation)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m reward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/model4.py:218\u001b[0m, in \u001b[0;36mMaskGeneratingModel.sample_one_batch_multi_samples\u001b[0;34m(self, input_ids, attention_mask, mask_logits, context_mask, num_samples)\u001b[0m\n\u001b[1;32m    216\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m--> 218\u001b[0m     masked_input_ids, masked_attention_mask, user_input_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend((masked_input_ids, masked_attention_mask, user_input_mask))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/model4.py:199\u001b[0m, in \u001b[0;36mMaskGeneratingModel.sample_one_batch\u001b[0;34m(self, input_ids, attention_mask, mask_logits, context_mask)\u001b[0m\n\u001b[1;32m    197\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    198\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m mask_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, prompt_len)\u001b[39;00m\n\u001b[1;32m    200\u001b[0m pad_length \u001b[38;5;241m=\u001b[39m seq_len \u001b[38;5;241m-\u001b[39m prompt_len\n\u001b[1;32m    201\u001b[0m padded_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(mask, (\u001b[38;5;241m0\u001b[39m, pad_length), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/model4.py:171\u001b[0m, in \u001b[0;36mMaskGeneratingModel.generate_mask\u001b[0;34m(self, mask_logits, context_mask)\u001b[0m\n\u001b[1;32m    169\u001b[0m k_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m    170\u001b[0m k_values \u001b[38;5;241m=\u001b[39m (k_ratio \u001b[38;5;241m*\u001b[39m context_mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mint()\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28mprint\u001b[39m(k_values)\n\u001b[1;32m    172\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(mask_logits)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mask_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:461\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    458\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    459\u001b[0m     )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor_str.py:677\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    676\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor_str.py:597\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 597\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    600\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor_str.py:133\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[0;32m--> 133\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:965\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_gen_model.train()\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        # (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "        unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "        unpaded_token_mask[:, :-pad_length] = 1\n",
    "        gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "        context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "        # Get the last hidden state for the prompt + response sequence\n",
    "        with torch.no_grad():\n",
    "            full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden_state = full_outputs.hidden_states[-1]\n",
    "            last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        mask_gen_outputs = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, \n",
    "                                                                           num_samples=1)\n",
    "        loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\n",
    "        log = (f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss.item():.4f}, \" \n",
    "                             f\"Reward Loss = {reward_loss.item():.4f}, \"\n",
    "                             f\"Mean Reward = {mean_reward.item():.4f},\"\n",
    "                             f\"Mask_loss = {mask_loss.item():.4f} \"\n",
    "                             f\"mask_mean = {mask_mean.item():.4f}\"\n",
    "        )\n",
    "        pbar.set_description(log)\n",
    "        logging.debug(log)\n",
    "        # Train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #                      )\n",
    "        # if idx % 10 == 0:\n",
    "        #     print()\n",
    "        if idx % 200 == 0 and idx != 0:\n",
    "            torch.save(mask_gen_model.state_dict(), f'saved_model/mask_gen_model_{epoch}_{idx}.pth') \n",
    "            print()\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8184, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|begin_of_text|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> You                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chat                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bot                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sentiment                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> analysis                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> You                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> can                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> help                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> users                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> their                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> questions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> via                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> concise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> responses                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> POS                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ITIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> NEG                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ATIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> user                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> assistant                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> POS                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ITIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.00)</b></text></td><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|begin_of_text|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> You                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> chat                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bot                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sentiment                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> analysis                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> You                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> can                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> help                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> users                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> their                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> questions                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> via                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> concise                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> responses                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> POS                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ITIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> or                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> NEG                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ATIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> user                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|start_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> assistant                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|end_header_id|                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> POS                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ITIVE                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> #|eot_id|                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = 0\n",
    "from captum.attr import visualization as viz\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I really did not like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "# texts = \"I really love this film. The acting was great, and the story was amazing. I would recommend this movie to everyone.\"\n",
    "# texts = \"I don't like this movie. The acting was terrible, and the story was boring. I would not recommend this movie to anyone.\"\n",
    "messages_lambda = lambda texts: [\n",
    "            {\"role\": \"system\", \"content\": \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"},\n",
    "            # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": texts},\n",
    "        ]\n",
    "messages = messages_lambda(texts)\n",
    "messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "test_text = [{\"text\": texts, \"label\": None}]\n",
    "test_inputs = collate_fn(test_text).to(device)\n",
    "\n",
    "# test_inputs = next(iter(train_dataloader)).to(device)\n",
    "tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][idx])\n",
    "\n",
    "# generate the answer for the test inputs\n",
    "gen_outputs = model.generate(\n",
    "            input_ids=test_inputs['input_ids'],\n",
    "            attention_mask=test_inputs['attention_mask'],\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "input_ids = test_inputs['input_ids']\n",
    "attention_mask = test_inputs['attention_mask']\n",
    "gen_tokens = gen_outputs.sequences\n",
    "pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "# get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "context_mask = F.pad(test_inputs['context_mask'], (0, pad_length), mode='constant', value=0)\n",
    "# (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "unpaded_token_mask[:, :-pad_length] = 1\n",
    "gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    # prompt_outputs = model(input_ids=test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "    prompt_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    last_hidden_state = prompt_outputs.hidden_states[-1].float()\n",
    "    mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "# Function to clean tokens\n",
    "def clean_token(token):\n",
    "    # Remove special characters like \"Ġ\" or \"Ċ\"\n",
    "    return token.replace(\"Ġ\", \"\").replace(\"Ċ\", \"\").replace(\"Ġ\", \"\").replace(\"Ċ\", \"\").replace(\"Ġ\", \"\")\n",
    "\n",
    "# Apply cleaning to each token\n",
    "tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "tokens = [clean_token(token) for token in tokens]\n",
    "\n",
    "def normalize_except_zeros(array):\n",
    "    # Create a mask to identify non-zero elements\n",
    "    mask = array != 0\n",
    "    \n",
    "    # Extract non-zero elements\n",
    "    non_zero_elements = array[mask]\n",
    "    \n",
    "    # Normalize non-zero elements\n",
    "    min_val = np.min(non_zero_elements)\n",
    "    max_val = np.max(non_zero_elements)\n",
    "    # normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    # get the 50% persentile\n",
    "    # min_val = np.percentile(non_zero_elements, 50)\n",
    "    normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create a copy of the original array to preserve zero values\n",
    "    normalized_array = np.copy(array)\n",
    "    \n",
    "    # Assign normalized values back to the corresponding positions\n",
    "    normalized_array[mask] = normalized_non_zero_elements\n",
    "    \n",
    "    return normalized_array\n",
    "\n",
    "expl = (torch.sigmoid(mask_logits) * context_mask)[idx]\n",
    "expl = F.pad(expl, (0, len(tokens) - expl.size(0)), mode='constant', value=0)\n",
    "expl_raw = expl.detach().cpu().numpy()\n",
    "# expl = (expl - 0) / (expl_raw.max(axis=-1) - 0)\n",
    "\n",
    "# expl = expl_raw\n",
    "expl = normalize_except_zeros(expl_raw)\n",
    "\n",
    "\n",
    "\n",
    "# expl = response_mask[idx]\n",
    "\n",
    "\n",
    "vis_data_records = [viz.VisualizationDataRecord(\n",
    "                                expl,\n",
    "                                0,\n",
    "                                0,\n",
    "                                0,\n",
    "                                0,\n",
    "                                1,       \n",
    "                                tokens,\n",
    "                                1)]\n",
    "                            \n",
    "viz.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.0279 0.8979 0.1684\n",
      " 1.     0.3327 0.9915 0.9641 0.3456 0.     0.2309 0.3376 0.2255 0.329\n",
      " 0.265  0.2508 0.3121 0.7915 0.5378 0.0591 0.1465 0.0985 0.5066 0.0857\n",
      " 0.387  0.8436 0.     0.     0.     0.     0.     0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Set print options for precision\n",
    "torch.set_printoptions(precision=4)\n",
    "np.set_printoptions(precision=4)\n",
    "print(expl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.9358044e-03, 8.5238426e-04, 4.9257861e-03,\n",
       "       3.3561827e-04, 9.9662030e-01, 9.9794489e-01, 1.6490246e-04,\n",
       "       3.7765391e-02, 4.5216787e-03, 9.9155957e-01, 2.1190707e-02,\n",
       "       4.3913804e-02, 2.5236185e-04, 2.5347209e-02, 2.0404863e-03,\n",
       "       9.9968982e-01, 9.9894804e-01, 1.6779628e-03, 6.8065450e-03,\n",
       "       9.7157842e-01, 1.0806271e-01, 9.6391785e-01, 3.8870864e-02,\n",
       "       9.9888331e-01, 9.9811769e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1241, 0.2500, 0.7787, 0.0909, 0.1908, 0.1361, 0.3316, 0.5104, 0.2696,\n",
       "        0.1001, 0.5931, 0.0812, 0.4139, 0.3334, 0.4853, 0.2987],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob = torch.sigmoid(mask_logits)\n",
    "(mask_prob * context_mask).sum(-1) / context_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9998e-01, 1.3412e-02, 2.1168e-05, 7.2758e-04, 9.5831e-01, 2.8712e-06,\n",
       "         1.9530e-03, 6.8251e-04, 9.6814e-01, 7.6582e-04, 2.2164e-06, 1.7009e-02,\n",
       "         2.4596e-09, 1.4519e-04, 3.0463e-06, 7.9092e-04, 4.0336e-06, 5.2874e-05,\n",
       "         1.4755e-07, 5.3417e-09, 1.0859e-07, 8.2496e-07, 1.0534e-05, 2.1604e-06,\n",
       "         6.6980e-10, 1.8266e-01, 5.4030e-07, 4.0081e-02, 3.5185e-04, 3.1933e-01,\n",
       "         3.2836e-09, 1.5197e-06, 8.2054e-01, 8.0839e-01, 5.4567e-01, 4.8829e-01,\n",
       "         1.0000e+00, 4.3639e-05, 3.5097e-07, 3.8482e-09, 1.9672e-04, 1.1242e-07,\n",
       "         2.7982e-13, 5.4683e-09, 4.5730e-01, 9.5505e-01, 6.7815e-04, 7.4368e-02,\n",
       "         9.9924e-01, 6.6332e-03, 1.0508e-08, 1.8774e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,   2675,    527,    264,   6369,\n",
       "          6465,    369,  27065,   6492,     13,   1472,    649,   1520,   3932,\n",
       "           449,    872,   4860,   4669,  64694,  14847,    315,  27592,  45450,\n",
       "            11,    477,  85165,  24093,     13, 128009, 128006,    882, 128007,\n",
       "           271,   2028,   5818,    574,    279,   1888,   5818,    358,    617,\n",
       "          3596,   3970,      0,   1063,  16451,   1051,  27873,     11,    719,\n",
       "         15718,    574,   2294,     13, 128009, 128006,  78191, 128007,    271],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs['input_ids'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĊĊ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(271)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.6432 0.8566 0.5179\n",
      " 0.2417 0.     0.1211 0.3355 0.618  0.5345 0.1401 0.3401 0.4729 0.3531\n",
      " 0.661  0.7049 0.0297 0.1724 0.9905 1.     0.1606 0.1107 0.2363 0.2891\n",
      " 0.116  0.0777 0.     0.     0.     0.     0.     0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.8527, 0.1770, 0.1719, 0.1660, 0.1613, 0.1549, 0.1581, 0.1622, 0.1666,\n",
       "        0.1635, 0.1568, 0.1589, 0.1618, 0.1634, 0.1668, 0.1647, 0.1546, 0.1576,\n",
       "        0.1788, 0.1779, 0.1592, 0.1581, 0.1607, 0.1619, 0.1586, 0.1583, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sigmoid(mask_logits) * context_mask)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4822, 0.4821, 0.4816,  ..., 0.4920, 0.4874, 0.4901],\n",
       "        [0.4843, 0.4851, 0.4855,  ..., 0.4941, 0.4861, 0.4891],\n",
       "        [0.4785, 0.4805, 0.4805,  ..., 0.4918, 0.4823, 0.4864],\n",
       "        ...,\n",
       "        [0.4753, 0.4758, 0.4761,  ..., 0.4847, 0.4761, 0.4802],\n",
       "        [0.4876, 0.4883, 0.4882,  ..., 0.4887, 0.4880, 0.4943],\n",
       "        [0.4843, 0.4851, 0.4852,  ..., 0.4946, 0.4853, 0.4947]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(mask_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskGeneratingModel(\n",
       "  (explain_map): MLP(\n",
       "    (input_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (attention_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tokens[35])\n",
    "print(expl[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I really didn't like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "messages_lambda = lambda texts: [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"},\n",
    "    # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": texts},\n",
    "]\n",
    "messages = messages_lambda(texts)\n",
    "messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "print(messages_with_template_applied)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
