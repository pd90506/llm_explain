{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.llm.smollm import LLMWrapper\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "# saved_mab_model = \"checkpoints/mab_model_100.pth\"\n",
    "saved_mab_model = \"checkpoints/mab_model_40.pth\"\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "\n",
    "llm = LLMWrapper(checkpoint, device=device)\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "The service at this restaurant was fantastic, and the staff were so friendly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "The service at this restaurant was fantastic, and the staff were so friendly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sentiment of this sentence is overwhelmingly positive. The use of words like \"fantastic\" and \"friendly\" convey a high level of satisfaction and appreciation for the service.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# instruction = \"Analyze the sentiment of the following sentence. Be brief.\"\n",
    "# instruction = \"Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.\"\n",
    "instruction = \"Analyze the sentiment of the following sentence and respond concisely.\"\n",
    "# user_input = \"I am extremely disappointed with the quality; it broke after just one day.\"\n",
    "user_input = \"The service at this restaurant was fantastic, and the staff were so friendly.\"\n",
    "# user_input = \"<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\"\n",
    "# user_input = \"I'm so happy with the product!\"\n",
    "# user_input = \"The bright sunshine and gentle breeze made my afternoon truly delightful.\"\n",
    "# user_input = \"I felt deeply disappointed and frustrated after the meeting went completely off track.\"\n",
    "# user_input = \"Although the food at the restaurant was excellent, the service left much to be desired.\"\n",
    "\n",
    "\n",
    "content = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": instruction\n",
    "            },\n",
    "\n",
    "            {\"role\": \"sentence\", \n",
    "            \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "template = tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)\n",
    "# print(template)\n",
    "\n",
    "# The generated outputs \n",
    "gen_output = llm.generate_from_texts(template)\n",
    "print(template)\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2127,  56956,\n",
      "            279,  27065,    315,    279,   2768,  11914,    323,   6013,   3613,\n",
      "            285,    989,     13, 128009, 128006,  52989, 128007,    271,    791,\n",
      "           2532,    520,    420,  10960,    574,  14964,     11,    323,    279,\n",
      "           5687,   1051,    779,  11919,     13, 128009, 128006,  78191, 128007]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'context_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from llmexp.utils.data_utils import DataCollator\n",
    "data_collator = DataCollator(tokenizer, max_length=512, instruction=instruction)\n",
    "\n",
    "example = {\n",
    "    'sentence': user_input,\n",
    "    'label': 1\n",
    "}\n",
    "\n",
    "example = data_collator([example]).to(device)\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "The service at this restaurant was fantastic, and the staff were so friendly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(example['input_ids'][0] * example['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2127,  56956,\n",
      "            279,  27065,    315,    279,   2768,  11914,    323,   6013,   3613,\n",
      "            285,    989,     13, 128009, 128006,  52989, 128007,    271,    791,\n",
      "           2532,    520,    420,  10960,    574,  14964,     11,    323,    279,\n",
      "           5687,   1051,    779,  11919,     13, 128009, 128006,  78191, 128007,\n",
      "            271,    791,  27065,    315,    420,  11914,    374,  55734,   6928,\n",
      "             11,  19392,    264,   1579,   2237,    315,  24617,    449,    279,\n",
      "           2532,    323,    279,   5687,    596,   4333,  49958,     13, 128009]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "gen_output = llm.generate(example['input_ids'], example['attention_mask'])\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "           220,   1627,  10263,    220,   2366,     19,    271,   2127,  56956,\n",
       "           279,  27065,    315,    279,   2768,  11914,    323,   6013,   3613,\n",
       "           285,    989,     13, 128009, 128006,  52989, 128007,    271,    791,\n",
       "          2532,    520,    420,  10960,    574,  14964,     11,    323,    279,\n",
       "          5687,   1051,    779,  11919,     13, 128009, 128006,  78191, 128007,\n",
       "           271,    791,  27065,    315,    420,  11914,    374,  55734,   6928,\n",
       "            11,  19392,    264,   1579,   2237,    315,  24617,    449,    279,\n",
       "          2532,    323,    279,   5687,    596,   4333,  49958,     13, 128009],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_output['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,   1627,  10263,    220,   2366,     19,    271,   2127,  56956,\n",
      "            279,  27065,    315,    279,   2768,  11914,    323,   6013,   3613,\n",
      "            285,    989,     13, 128009, 128006,  52989, 128007,    271,    791,\n",
      "           2532,    520,    420,  10960,    574,  14964,     11,    323,    279,\n",
      "           5687,   1051,    779,  11919,     13, 128009, 128006,  78191, 128007,\n",
      "            271,    791,  27065]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'context_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from llmexp.trainer.mab_trainer import randomly_cut_and_pad_generations\n",
    "cut_and_pad_gen_output = randomly_cut_and_pad_generations(example, gen_output, tokenizer)\n",
    "print(cut_and_pad_gen_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "The service at this restaurant was fantastic, and the staff were so friendly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sentiment\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(cut_and_pad_gen_output['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/1157213.1.gpu/ipykernel_4053507/3922162976.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mab_model = MABModel.load_with_base_model(torch.load(saved_mab_model), llm, hidden_size=1024)\n"
     ]
    }
   ],
   "source": [
    "from llmexp.explainer.mab_model import MABModel\n",
    "mab_model = MABModel.load_with_base_model(torch.load(saved_mab_model), llm, hidden_size=1024)\n",
    "mab_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = cut_and_pad_gen_output['input_ids']\n",
    "attention_mask = cut_and_pad_gen_output['attention_mask']\n",
    "logits, values = mab_model.get_logits_value(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "# mab_values = torch.softmax(logits, dim=-1)\n",
    "\n",
    "context_mask = cut_and_pad_gen_output['context_mask']\n",
    "# fill the \n",
    "masked_logits = logits.masked_fill(~context_mask[:,:-1].bool(), float('-inf'))\n",
    "mab_values = torch.softmax(masked_logits, dim=-1)\n",
    "# mab_values = dist.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mab_values.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 9.9999e-01, 9.0104e-07, 6.9004e-07, 3.3748e-07,\n",
      "         5.2253e-06, 3.9949e-07, 4.0677e-07, 4.3013e-07, 3.6211e-07, 3.5029e-07,\n",
      "         8.0093e-07, 2.6332e-07, 3.6234e-07, 6.5032e-07, 1.9447e-07, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([1, 65])\n"
     ]
    }
   ],
   "source": [
    "print(mab_values)\n",
    "print(mab_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens_with_values(input_ids, mab_values, context_mask, tokenizer):\n",
    "    # Decode tokens one by one to preserve alignment\n",
    "    tokens = []\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        token = tokenizer.decode(input_ids[0, i:i+1])\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Normalize MAB values to [0,1] for color intensity first\n",
    "    mab_values = mab_values * context_mask[:,:-1]\n",
    "    # Create a mask for non-zero values\n",
    "    non_zero_mask = mab_values[0] != 0\n",
    "    # normalized_values = (mab_values[0] - mab_values[0].min()) / (mab_values[0].max() - mab_values[0].min())\n",
    "    normalized_values = torch.zeros_like(mab_values[0])\n",
    "    # Only normalize non-zero values\n",
    "    if non_zero_mask.any():  # Check if there are any non-zero values\n",
    "        non_zero_values = mab_values[0][non_zero_mask]\n",
    "        normalized_non_zero = (non_zero_values - non_zero_values.min()) / (non_zero_values.max() - non_zero_values.min())\n",
    "        normalized_values[non_zero_mask] = normalized_non_zero\n",
    "    \n",
    "    # Pad normalized_values with a zero at the end\n",
    "    padded_normalized_values = torch.cat([normalized_values, torch.zeros(1, device=mab_values.device)], dim=0)\n",
    "    # Pad original mab_values with the last actual value\n",
    "    padded_mab_values = torch.cat([mab_values[0], mab_values[0][-1:]], dim=0)\n",
    "    \n",
    "    # Generate HTML with colored text and values\n",
    "    html_output = \"<div style='font-family: monospace; line-height: 2; background-color: white; padding: 10px;'>\"\n",
    "    for token, value, orig_value in zip(tokens, padded_normalized_values, padded_mab_values):\n",
    "        # Use a gradient from white to green\n",
    "        intensity = float(value)\n",
    "        green_color = int(intensity * 200)  # Control the maximum intensity\n",
    "        html_output += f'<span style=\"color: black; background-color: rgba(0, {green_color}, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: {orig_value:.3f}, Norm: {value:.3f}\">{token}</span>'\n",
    "    html_output += \"</div>\"\n",
    "    \n",
    "    # Print the values\n",
    "    print(\"Token\\tNormalized Value\\tOriginal MAB Value\")\n",
    "    print(\"-\" * 50)\n",
    "    for token, value, orig_value in zip(tokens, padded_normalized_values, padded_mab_values):\n",
    "        print(f\"{token}\\t{value:.3f}\\t\\t{orig_value:.3f}\")\n",
    "    \n",
    "    from IPython.display import HTML\n",
    "    return HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tNormalized Value\tOriginal MAB Value\n",
      "--------------------------------------------------\n",
      "<|begin_of_text|>\t0.000\t\t0.000\n",
      "<|start_header_id|>\t0.000\t\t0.000\n",
      "system\t0.000\t\t0.000\n",
      "<|end_header_id|>\t0.000\t\t0.000\n",
      "\n",
      "\n",
      "\t0.000\t\t0.000\n",
      "Cut\t0.000\t\t0.000\n",
      "ting\t0.000\t\t0.000\n",
      " Knowledge\t0.000\t\t0.000\n",
      " Date\t0.000\t\t0.000\n",
      ":\t0.000\t\t0.000\n",
      " December\t0.000\t\t0.000\n",
      " \t0.000\t\t0.000\n",
      "202\t0.000\t\t0.000\n",
      "3\t0.000\t\t0.000\n",
      "\n",
      "\t0.000\t\t0.000\n",
      "Today\t0.000\t\t0.000\n",
      " Date\t0.000\t\t0.000\n",
      ":\t0.000\t\t0.000\n",
      " \t0.000\t\t0.000\n",
      "26\t0.000\t\t0.000\n",
      " Jul\t0.000\t\t0.000\n",
      " \t0.000\t\t0.000\n",
      "202\t0.000\t\t0.000\n",
      "4\t0.000\t\t0.000\n",
      "\n",
      "\n",
      "\t0.000\t\t0.000\n",
      "An\t0.000\t\t0.000\n",
      "alyze\t0.000\t\t0.000\n",
      " the\t0.000\t\t0.000\n",
      " sentiment\t0.000\t\t0.000\n",
      " of\t0.000\t\t0.000\n",
      " the\t0.000\t\t0.000\n",
      " following\t0.000\t\t0.000\n",
      " sentence\t0.000\t\t0.000\n",
      " and\t0.000\t\t0.000\n",
      " respond\t0.000\t\t0.000\n",
      " conc\t0.000\t\t0.000\n",
      "is\t0.000\t\t0.000\n",
      "ely\t0.000\t\t0.000\n",
      ".\t0.000\t\t0.000\n",
      "<|eot_id|>\t0.000\t\t0.000\n",
      "<|start_header_id|>\t0.000\t\t0.000\n",
      "sentence\t0.000\t\t0.000\n",
      "<|end_header_id|>\t0.000\t\t0.000\n",
      "\n",
      "\n",
      "\t0.000\t\t0.000\n",
      "The\t1.000\t\t1.000\n",
      " service\t0.000\t\t0.000\n",
      " at\t0.000\t\t0.000\n",
      " this\t0.000\t\t0.000\n",
      " restaurant\t0.000\t\t0.000\n",
      " was\t0.000\t\t0.000\n",
      " fantastic\t0.000\t\t0.000\n",
      ",\t0.000\t\t0.000\n",
      " and\t0.000\t\t0.000\n",
      " the\t0.000\t\t0.000\n",
      " staff\t0.000\t\t0.000\n",
      " were\t0.000\t\t0.000\n",
      " so\t0.000\t\t0.000\n",
      " friendly\t0.000\t\t0.000\n",
      ".\t0.000\t\t0.000\n",
      "<|eot_id|>\t0.000\t\t0.000\n",
      "<|start_header_id|>\t0.000\t\t0.000\n",
      "assistant\t0.000\t\t0.000\n",
      "<|end_header_id|>\t0.000\t\t0.000\n",
      "\n",
      "\n",
      "\t0.000\t\t0.000\n",
      "The\t0.000\t\t0.000\n",
      " sentiment\t0.000\t\t0.000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='font-family: monospace; line-height: 2; background-color: white; padding: 10px;'><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|begin_of_text|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|start_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">system</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|end_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">\n",
       "\n",
       "</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">Cut</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">ting</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> Knowledge</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> Date</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">:</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> December</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> </span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">202</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">3</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">\n",
       "</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">Today</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> Date</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">:</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> </span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">26</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> Jul</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> </span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">202</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">4</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">\n",
       "\n",
       "</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">An</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">alyze</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> the</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> sentiment</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> of</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> the</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> following</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> sentence</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> and</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> respond</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> conc</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">is</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">ely</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">.</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|eot_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|start_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">sentence</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|end_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">\n",
       "\n",
       "</span><span style=\"color: black; background-color: rgba(0, 200, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 1.000, Norm: 1.000\">The</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> service</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> at</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> this</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> restaurant</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> was</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> fantastic</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">,</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> and</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> the</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> staff</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> were</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> so</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> friendly</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">.</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|eot_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|start_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">assistant</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"><|end_header_id|></span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">\n",
       "\n",
       "</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\">The</span><span style=\"color: black; background-color: rgba(0, 0, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: 0.000, Norm: 0.000\"> sentiment</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage:\n",
    "visualization = visualize_tokens_with_values(input_ids, mab_values, context_mask, tokenizer)\n",
    "display(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
