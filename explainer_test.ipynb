{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os \n",
    "import json\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df4359c65dc47079997b1acf2ceafcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llmexp.llm.smollm import LLMWrapper, Template\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "llm = LLMWrapper(checkpoint, device=device, access_token=access_token)\n",
    "tokenizer = llm.tokenizer\n",
    "template = Template(tokenizer, task='qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llmexp.explainer.mab_explainer import MABExplainer\n",
    "# from llmexp.explainer.mab_explainer_advantage import MABExplainerAdvantage\n",
    "from llmexp.explainer.mab_explainer import MABExplainer\n",
    "\n",
    "mab_explainer = MABExplainer(llm, tokenizer, template)\n",
    "# mab_explainer = MABExplainerAdvantage(llm, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.utils.data_utils import LLMDataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = LLMDataset(\"hotpot_qa\", split=\"train\")\n",
    "\n",
    "from llmexp.utils.hotpot_helper import HotpotHelper, HotpotSample\n",
    "# helper = HotpotHelper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, Arthur's Magazine was started in 1844, while First for Women was started in 1989. Therefore, Arthur's Magazine was started first.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "hpsample = HotpotSample(dataset[idx])\n",
    "sentences = hpsample.flattened_contexts\n",
    "question = hpsample.question\n",
    "query = sentences + [question]\n",
    "query\n",
    "\n",
    "response = mab_explainer.get_response(sentences, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      " 0 Radio City is India's first private FM radio station and was started on 3 July 2001.\n",
      "1  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).\n",
      "2  It plays Hindi, English and regional songs.\n",
      "3  It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.\n",
      "4  Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features. \n",
      "...\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "supporting facts indices: [25, 33]\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\\n\", \"\\n\".join([f\"{idx} {x}\" for idx, x in enumerate(hpsample.flattened_contexts[:5])]), \"\\n...\")\n",
    "print(\"Question:\", hpsample.question)\n",
    "print(\"supporting facts indices:\", hpsample.flattened_supporting_facts_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.30644426, -0.2688241 , -0.39993867, -0.39629516, -0.24347259,\n",
       "       -0.46292529, -0.107263  , -0.48387897, -0.34414169, -0.20506722,\n",
       "       -0.06440838, -0.49595651, -0.52092165, -0.36010492, -0.0055213 ,\n",
       "       -0.28138241, -0.39494956, -0.43563432, -0.20169032, -0.27577958,\n",
       "       -0.48041481, -0.36646467, -0.43796539, -0.39905867, -0.37742791,\n",
       "       -0.02177167, -0.02649677, -0.23753779, -0.38597018, -0.32334548,\n",
       "       -0.29833668, -0.64995503, -0.26415506, -0.40566549,  0.05760736,\n",
       "       -0.18212007, -0.50258273, -0.41072154, -0.43376026, -0.28246805,\n",
       "       -0.63809258, -0.396759  , -0.36103016, -0.61072016, -0.49916637,\n",
       "       -0.31854135])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = mab_explainer.thompson_sampling(sentences, question, response, n_iter=20)\n",
    "# theta = mab_explainer.thompson_sampling_gaussian(sentences, response, n_iter=100)\n",
    "# theta = mab_explainer.policy_gradient_with_advantage(sentences, response, n_iter=30, lr=0.1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "A: According to the context, Arthur's Magazine was started in 1844, while First for Women was started\n",
      "in 1989. Therefore, Arthur's Magazine was started first.<|eot_id|>\n",
      "Supporting indices: [25, 33]\n",
      "Ground truth answer: Arthur's Magazine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.057607</td>\n",
       "      <td>The magazine was started in 1989.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.005521</td>\n",
       "      <td>until they signed to Warner Bros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.021772</td>\n",
       "      <td>Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.026497</td>\n",
       "      <td>Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.064408</td>\n",
       "      <td>In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.107263</td>\n",
       "      <td>Abraham Thomas is the CEO of the company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.182120</td>\n",
       "      <td>It is based in Englewood Cliffs, New Jersey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.201690</td>\n",
       "      <td>Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.205067</td>\n",
       "      <td>Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.237538</td>\n",
       "      <td>In May 1846 it was merged into \"Godey's Lady's Book\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.243473</td>\n",
       "      <td>Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.264155</td>\n",
       "      <td>In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.268824</td>\n",
       "      <td>It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.275780</td>\n",
       "      <td>The band's debut album, \"Talking Dreams\", was released on October 8, 2013.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.281382</td>\n",
       "      <td>Records in May 2012.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.282468</td>\n",
       "      <td>The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.298337</td>\n",
       "      <td>Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.306444</td>\n",
       "      <td>Radio City is India's first private FM radio station and was started on 3 July 2001.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.318541</td>\n",
       "      <td>The company started first as a denim line, later evolving into a men’s and women’s clothing line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.323345</td>\n",
       "      <td>Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.344142</td>\n",
       "      <td>This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.360105</td>\n",
       "      <td>Echosmith started first as \"Ready Set Go!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.361030</td>\n",
       "      <td>It is most known for their premium jeans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.366465</td>\n",
       "      <td>Many started first as girls' seminaries or academies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.377428</td>\n",
       "      <td>The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.385970</td>\n",
       "      <td>The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.394950</td>\n",
       "      <td>They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.396295</td>\n",
       "      <td>It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.396759</td>\n",
       "      <td>William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.399059</td>\n",
       "      <td>Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.399939</td>\n",
       "      <td>It plays Hindi, English and regional songs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.405665</td>\n",
       "      <td>First for Women is a woman's magazine published by Bauer Media Group in the USA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.410722</td>\n",
       "      <td>The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.433760</td>\n",
       "      <td>The fire started as two separate fires on November 15, 2008.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.435634</td>\n",
       "      <td>The song was Warner Bros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.437965</td>\n",
       "      <td>Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.462925</td>\n",
       "      <td>The Radio station currently plays a mix of Hindi and Regional music.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.480415</td>\n",
       "      <td>Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.483879</td>\n",
       "      <td>Football in Albania existed before the Albanian Football Federation (FSHF) was created.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.495957</td>\n",
       "      <td>Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.499166</td>\n",
       "      <td>The label also produces other clothing items such as jackets and tops.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.502583</td>\n",
       "      <td>In 2011 the circulation of the magazine was 1,310,696 copies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.520922</td>\n",
       "      <td>Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.610720</td>\n",
       "      <td>On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.638093</td>\n",
       "      <td>These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.649955</td>\n",
       "      <td>The regular season included just 12 rounds, where all the teams went to the semifinals.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       theta  \\\n",
       "34  0.057607   \n",
       "14 -0.005521   \n",
       "25 -0.021772   \n",
       "26 -0.026497   \n",
       "10 -0.064408   \n",
       "6  -0.107263   \n",
       "35 -0.182120   \n",
       "18 -0.201690   \n",
       "9  -0.205067   \n",
       "27 -0.237538   \n",
       "4  -0.243473   \n",
       "32 -0.264155   \n",
       "1  -0.268824   \n",
       "19 -0.275780   \n",
       "15 -0.281382   \n",
       "39 -0.282468   \n",
       "30 -0.298337   \n",
       "0  -0.306444   \n",
       "45 -0.318541   \n",
       "29 -0.323345   \n",
       "8  -0.344142   \n",
       "13 -0.360105   \n",
       "42 -0.361030   \n",
       "21 -0.366465   \n",
       "24 -0.377428   \n",
       "28 -0.385970   \n",
       "16 -0.394950   \n",
       "3  -0.396295   \n",
       "41 -0.396759   \n",
       "23 -0.399059   \n",
       "2  -0.399939   \n",
       "33 -0.405665   \n",
       "37 -0.410722   \n",
       "38 -0.433760   \n",
       "17 -0.435634   \n",
       "22 -0.437965   \n",
       "5  -0.462925   \n",
       "20 -0.480415   \n",
       "7  -0.483879   \n",
       "11 -0.495957   \n",
       "44 -0.499166   \n",
       "36 -0.502583   \n",
       "12 -0.520922   \n",
       "43 -0.610720   \n",
       "40 -0.638093   \n",
       "31 -0.649955   \n",
       "\n",
       "                                                                                                                                                                                                                                                               sentences  \n",
       "34                                                                                                                                                                                                                                     The magazine was started in 1989.  \n",
       "14                                                                                                                                                                                                                                     until they signed to Warner Bros.  \n",
       "25                                                                                                                                                      Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.  \n",
       "26                                                                                                                                              Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.  \n",
       "10                                                                                                                                             In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.  \n",
       "6                                                                                                                                                                                                                              Abraham Thomas is the CEO of the company.  \n",
       "35                                                                                                                                                                                                                          It is based in Englewood Cliffs, New Jersey.  \n",
       "18                                                                                                                                                                                 Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.  \n",
       "9                                                                                                       Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.  \n",
       "27                                                                                                                                                                                                                 In May 1846 it was merged into \"Godey's Lady's Book\".  \n",
       "4                                                                        Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.  \n",
       "32                                                                                                                                                                                             In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.  \n",
       "1                                                                                       It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).  \n",
       "19                                                                                                                                                                                            The band's debut album, \"Talking Dreams\", was released on October 8, 2013.  \n",
       "15                                                                                                                                                                                                                                                  Records in May 2012.  \n",
       "39                                                                                                                                                     The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.  \n",
       "30                                                                                                                         Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.  \n",
       "0                                                                                                                                                                                   Radio City is India's first private FM radio station and was started on 3 July 2001.  \n",
       "45                                                                                                                                                                     The company started first as a denim line, later evolving into a men’s and women’s clothing line.  \n",
       "29                                                                                                                       Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.  \n",
       "8    This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .  \n",
       "13                                                                                                                                                                                                                            Echosmith started first as \"Ready Set Go!\"  \n",
       "42                                                                                                                                                                                                                             It is most known for their premium jeans.  \n",
       "21                                                                                                                                                                                                                 Many started first as girls' seminaries or academies.  \n",
       "24                                                                                                                                   The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.  \n",
       "28                                                                                                                                                                   The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.  \n",
       "16                    They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.  \n",
       "3                                                                                                                                                               It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.  \n",
       "41                                                                                                                                                                               William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.  \n",
       "23                                                                                                                                                   Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.  \n",
       "2                                                                                                                                                                                                                            It plays Hindi, English and regional songs.  \n",
       "33                                                                                                                                                                                      First for Women is a woman's magazine published by Bauer Media Group in the USA.  \n",
       "37                                                                                                                                                               The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.  \n",
       "38                                                                                                                                                                                                          The fire started as two separate fires on November 15, 2008.  \n",
       "17                                                                                                                                                                                                                                             The song was Warner Bros.  \n",
       "22                                                                                                   Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.  \n",
       "5                                                                                                                                                                                                   The Radio station currently plays a mix of Hindi and Regional music.  \n",
       "20    Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.  \n",
       "7                                                                                                                                                                                Football in Albania existed before the Albanian Football Federation (FSHF) was created.  \n",
       "11                                                                                                                                                                      Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.  \n",
       "44                                                                                                                                                                                                The label also produces other clothing items such as jackets and tops.  \n",
       "36                                                                                                                                                                                                         In 2011 the circulation of the magazine was 1,310,696 copies.  \n",
       "12                                                                                             Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.  \n",
       "43                                                                                                                                On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.  \n",
       "40                                                                                                                                                 These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.  \n",
       "31                                                                                                                                                                               The regular season included just 12 rounds, where all the teams went to the semifinals.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine theta and messages, create a pandas dataframe\n",
    "data_dict = {\n",
    "    \"theta\": theta,\n",
    "    \"sentences\": sentences\n",
    "}\n",
    "import pandas as pd \n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Set pandas display options to show full text\n",
    "pd.set_option('display.max_colwidth', None)  # No limit on column width\n",
    "pd.set_option('display.width', None)         # No limit on display width\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "\n",
    "# Sort the dataframe\n",
    "df_sorted = df.sort_values(by='theta', ascending=False)\n",
    "\n",
    "import textwrap\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "width = 100\n",
    "# Format question and answer with proper wrapping\n",
    "wrapped_question = textwrap.fill(question, width=width)\n",
    "wrapped_response = textwrap.fill(response, width=width)\n",
    "\n",
    "# Print with f-strings\n",
    "print(f\"Q: {wrapped_question}\")\n",
    "print(f\"A: {wrapped_response}\")\n",
    "print(f\"Supporting indices: {hpsample.flattened_supporting_facts_indices}\")\n",
    "print(f\"Ground truth answer: {hpsample.answer}\")\n",
    "\n",
    "# # Alternatively, for better formatting in notebooks:\n",
    "# display(Markdown(f\"**Q:** {wrapped_question}\"))\n",
    "# display(Markdown(f\"**A:** {wrapped_response}\"))\n",
    "# Display the sorted dataframe\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.7901\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Example: original and perturbed responses\n",
    "response1 = \"The Oberoi family is part of a hotel company that has a head office in Delhi.\"\n",
    "response2 = \"The Oberoi Group is a hotel company with its head office in Delhi\"\n",
    "\n",
    "# BERTScore expects lists of strings\n",
    "P, R, F1 = score([response2], [response1], lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "\n",
    "# Print the F1 similarity score\n",
    "print(f\"BERTScore F1: {F1.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MABExplainer' object has no attribute 'get_baseline_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmab_explainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_baseline_tokens\u001b[49m(query)\n\u001b[1;32m      2\u001b[0m output\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MABExplainer' object has no attribute 'get_baseline_tokens'"
     ]
    }
   ],
   "source": [
    "output = mab_explainer.get_baseline_tokens(query)\n",
    "output.shape\n",
    "response = tokenizer.decode(output[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_output = mab_explainer.get_baseline_tokens(masked_query)\n",
    "masked_output.shape\n",
    "masked_response = tokenizer.decode(masked_output[0])\n",
    "masked_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_logits = mab_explainer.get_response_logits(query, response)\n",
    "response_logits.shape\n",
    "reward = mab_explainer.get_reward(response_logits, baseline_tokens=output, reward_type=\"cross_entropy\")\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_response_logits = mab_explainer.get_response_logits(masked_query, masked_response)\n",
    "masked_response_logits.shape\n",
    "masked_reward = mab_explainer.get_reward(masked_response_logits, baseline_tokens=masked_output, reward_type=\"cross_entropy\")\n",
    "masked_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the argmax of the response logits\n",
    "response_logits_argmax = torch.argmax(response_logits, dim=-1)\n",
    "print(response_logits_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(response_logits_argmax[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = mab_explainer.get_reward(response_logits, baseline_tokens=output, reward_type=\"cross_entropy\")\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.utils.data_utils import LLMDataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = LLMDataset(\"hotpot_qa\", split=\"train\")\n",
    "\n",
    "from llmexp.utils.hotpot_helper import HotpotHelper, HotpotSample\n",
    "# helper = HotpotHelper()\n",
    "hpsample = HotpotSample(dataset[0])\n",
    "\n",
    "print(hpsample.flattened_supporting_facts_indices)\n",
    "print(hpsample.sentence_mask)\n",
    "print(hpsample.flattened_contexts)\n",
    "\n",
    "bernoulli_mask = np.random.binomial(1, 0.5, size=hpsample.sentence_mask.shape)\n",
    "\n",
    "print(hpsample.get_contexts_from_mask(bernoulli_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hpsample.flattened_contexts[25])\n",
    "print(hpsample.flattened_contexts[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[2]\n",
    "question = example[\"question\"]\n",
    "titles = example[\"context\"][\"title\"]\n",
    "contexts = example[\"context\"][\"sentences\"]\n",
    "supporting_facts = example[\"supporting_facts\"]\n",
    "# top_res = [23, 33] # {'title': [\"Arthur's Magazine\", 'First for Women'], 'sent_id': [0, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (title, context) in enumerate(zip(titles, contexts)):\n",
    "    for j, sent in enumerate(context):\n",
    "        print(f\"({idx}, {j}) {title}: {sent}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supporting_facts_indices(supporting_facts, titles):\n",
    "    supporting_facts_title_indices = [titles.index(t) for t in supporting_facts[\"title\"]]\n",
    "    \n",
    "    return list(zip(supporting_facts_title_indices, supporting_facts[\"sent_id\"]))\n",
    "\n",
    "def get_iter_indices(contexts):\n",
    "    ind_tuples = []\n",
    "    iter_indices = []\n",
    "    cur_idx = 0\n",
    "    for a, context_list in enumerate(contexts):\n",
    "        for b, _ in enumerate(context_list):\n",
    "            ind_tuples.append((a, b))\n",
    "            iter_indices.append(cur_idx)\n",
    "            cur_idx += 1\n",
    "    ind_map = dict(zip(ind_tuples, iter_indices))\n",
    "    return ind_map\n",
    "\n",
    "\n",
    "class QASample:\n",
    "    def __init__(self, question, contexts, supporting_facts, titles):\n",
    "        self.question = question\n",
    "        self.contexts = contexts\n",
    "        self.supporting_facts = supporting_facts\n",
    "        self.titles = titles\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_facts_indices = get_supporting_facts_indices(supporting_facts, titles)\n",
    "\n",
    "ind_map = get_iter_indices(contexts)\n",
    "\n",
    "supporting_facts_indices_iter = [ind_map[tuple(fact)] for fact in supporting_facts_indices]\n",
    "supporting_facts_indices_iter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Context Sentences section.\n",
    "contexts_str = \"\"\n",
    "for title, sentences in zip(titles, contexts):\n",
    "    # Join the list of sentences into one string.\n",
    "    sentence_str = \"\\n\".join(sentences)\n",
    "    contexts_str += f\"{sentence_str}\\n\"\n",
    "\n",
    "context_str = f\"Context Sentences:\\n{contexts_str}\"\n",
    "question_str = f\"Question: {question}\"\n",
    "\n",
    "query = context_str + \"\\n\" + question_str + \"\\n\"\n",
    "print(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.explainer.mab_explainer import MABExplainer, MABTemplate, MABRecorder\n",
    "instruction = \"Answer the question based on the context provided.\"\n",
    "\n",
    "template = MABTemplate(tokenizer, instruction)\n",
    "explainer = MABExplainer(llm, tokenizer, template)\n",
    "\n",
    "input_mapper = explainer.input_mapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mapper.process_text(query)\n",
    "# may need to sort the keys by (first, second, third) elements of the tuple\n",
    "all_sentences = [input_mapper.sentences[key] for key in input_mapper.sentences.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(all_sentences):\n",
    "    print(idx, sentence)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" \".join(all_sentences)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_query = template.format(query)\n",
    "print(template_query)\n",
    "\n",
    "tokenized_template_query = tokenizer(template_query, return_tensors=\"pt\").to(device)\n",
    "input_ids = tokenized_template_query.input_ids \n",
    "attention_mask = tokenized_template_query.attention_mask \n",
    "\n",
    "template_query_with_response = llm.generate(input_ids, attention_mask, max_new_tokens=100)\n",
    "template_query_with_response_ids = template_query_with_response['input_ids']\n",
    "template_query_with_response_attention_mask = template_query_with_response['attention_mask']\n",
    "\n",
    "response_mask = explainer.get_response_mask(attention_mask, template_query_with_response_attention_mask)\n",
    "\n",
    "# print the response texts \n",
    "response_tokens = template_query_with_response_ids[response_mask == 1]\n",
    "response_texts = tokenizer.decode(response_tokens, skip_special_tokens=False)\n",
    "print(response_texts)\n",
    "\n",
    "# print the resources \n",
    "for top_res_idx in top_res:\n",
    "    print(top_res_idx)\n",
    "    print(all_sentences[top_res_idx])\n",
    "    print(\"-\"*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mab_pull_iter = explainer.random_clip_query_words(query, response_texts, num_trials=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import torch\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from llmexp.explainer.mab_explainer import MultiLevelInputMapper\n",
    "\n",
    "class ThompsonMABExplainer:\n",
    "    def __init__(self, explainer, input_mapper, initial_alpha=1.0, initial_beta=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the Thompson Sampling Multi-Armed Bandit explainer.\n",
    "        \n",
    "        Args:\n",
    "            explainer: The base explainer object\n",
    "            input_mapper: The MultiLevelInputMapper for the text\n",
    "            initial_alpha: Initial alpha parameter for Beta distributions\n",
    "            initial_beta: Initial beta parameter for Beta distributions\n",
    "        \"\"\"\n",
    "        self.explainer = explainer\n",
    "        self.input_mapper = input_mapper\n",
    "        self.recorder = MABRecorder(input_mapper)\n",
    "        \n",
    "        # Thompson sampling parameters for each content unit (Beta distribution)\n",
    "        self.alphas = {}\n",
    "        self.betas = {}\n",
    "        \n",
    "        # Initialize Beta distribution parameters for all content units\n",
    "        for k in input_mapper.sentences.keys():\n",
    "            self.alphas[k] = initial_alpha\n",
    "            self.betas[k] = initial_beta\n",
    "            \n",
    "        # for k in input_mapper.phrases.keys():\n",
    "        #     self.alphas[k] = initial_alpha\n",
    "        #     self.betas[k] = initial_beta\n",
    "            \n",
    "        # for k in input_mapper.words.keys():\n",
    "        #     self.alphas[k] = initial_alpha\n",
    "        #     self.betas[k] = initial_beta\n",
    "        \n",
    "        # Track best observed clips and their rewards\n",
    "        self.best_clip = None\n",
    "        self.best_reward = float('-inf')\n",
    "        self.all_rewards = []\n",
    "        \n",
    "    def sample_content_units(self, p_threshold=0.5) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Sample content units based on current Beta distributions.\n",
    "        \n",
    "        Args:\n",
    "            p_threshold: Probability threshold for including a unit\n",
    "            \n",
    "        Returns:\n",
    "            List of sampled content unit indices\n",
    "        \"\"\"\n",
    "        sampled_indices = []\n",
    "        \n",
    "        # Sample based on Thompson sampling probabilities\n",
    "        for key in self.alphas.keys():\n",
    "            # Sample from Beta distribution\n",
    "            p = beta.rvs(self.alphas[key], self.betas[key], size=1)[0]\n",
    "            \n",
    "            # Include this unit if its probability exceeds threshold\n",
    "            if p > p_threshold:\n",
    "                sampled_indices.append(key)\n",
    "                \n",
    "        return sampled_indices\n",
    "    \n",
    "    def construct_clip_from_indices(self, indices: List[Tuple]) -> str:\n",
    "        \"\"\"\n",
    "        Construct a clip of text from the sampled indices.\n",
    "        \n",
    "        Args:\n",
    "            indices: List of content unit indices to include\n",
    "            \n",
    "        Returns:\n",
    "            The constructed text clip\n",
    "        \"\"\"\n",
    "        # Sort indices to maintain the original text order\n",
    "        sorted_indices = sorted(indices, key=lambda x: (x[0] if len(x) > 0 else 0, \n",
    "                                                      x[1] if len(x) > 1 else 0,\n",
    "                                                      x[2] if len(x) > 2 else 0))\n",
    "        \n",
    "        # Extract content for each index\n",
    "        contents = []\n",
    "        for idx in sorted_indices:\n",
    "            if len(idx) == 1:  # Sentence\n",
    "                contents.append(self.input_mapper.get_content(idx[0]))\n",
    "            elif len(idx) == 2:  # Phrase\n",
    "                contents.append(self.input_mapper.get_content(idx[0], idx[1]))\n",
    "            elif len(idx) == 3:  # Word\n",
    "                contents.append(self.input_mapper.get_content(idx[0], idx[1], idx[2]))\n",
    "        \n",
    "        # Join with spaces\n",
    "        return \" \".join(contents)\n",
    "    \n",
    "    def update_parameters(self, sampled_indices: List[Tuple], reward: float, reward_threshold=28.0):\n",
    "        \"\"\"\n",
    "        Update Beta distribution parameters based on observed reward.\n",
    "        \n",
    "        Args:\n",
    "            sampled_indices: The indices of content units that were sampled\n",
    "            reward: The observed reward\n",
    "            reward_threshold: Threshold to consider a reward as positive\n",
    "        \"\"\"\n",
    "        # Determine success/failure based on reward\n",
    "        is_success = reward > reward_threshold\n",
    "        # thresholds are usually fixed.\n",
    "        # adversarial bandits (literature)\n",
    "        # is_success = reward > self.best_reward\n",
    "        # value running average of reward / advantage / -- could cause change of distributions.\n",
    "        # Max reward as threshold\n",
    "\n",
    "        \n",
    "        # Update parameters for sampled indices\n",
    "        for idx in sampled_indices:\n",
    "            if idx in self.alphas:\n",
    "                if is_success:\n",
    "                    self.alphas[idx] += 1  # Increase alpha on success\n",
    "                else:\n",
    "                    self.betas[idx] += 1   # Increase beta on failure\n",
    "                    \n",
    "        # Record this pull in the recorder\n",
    "        self.recorder.record(sampled_indices, reward)\n",
    "        \n",
    "        # Update best clip if this is the best so far\n",
    "        if reward > self.best_reward:\n",
    "            self.best_reward = reward\n",
    "            self.best_clip = sampled_indices\n",
    "            \n",
    "        self.all_rewards.append(reward)\n",
    "    \n",
    "    def run_iteration(self, query: str, response_text: str, p_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Run a single iteration of the Thompson sampling algorithm.\n",
    "        \n",
    "        Args:\n",
    "            query: The original query text\n",
    "            response_text: The model's response text\n",
    "            p_threshold: Probability threshold for sampling\n",
    "            \n",
    "        Returns:\n",
    "            The reward obtained in this iteration\n",
    "        \"\"\"\n",
    "        # Sample content units based on current distributions\n",
    "        sampled_indices = self.sample_content_units(p_threshold)\n",
    "        \n",
    "        if not sampled_indices:\n",
    "            # If nothing was sampled, pick a random sentence to avoid empty clips\n",
    "            sentence_keys = list(self.input_mapper.sentences.keys())\n",
    "            sampled_indices = [random.choice(sentence_keys)]\n",
    "        \n",
    "        # Construct the clip\n",
    "        clip_text = self.construct_clip_from_indices(sampled_indices)\n",
    "        \n",
    "        # Format the clip with response\n",
    "        template_clip_text = self.explainer.template.format(clip_text)\n",
    "        clip_with_response = template_clip_text + response_text\n",
    "        \n",
    "        # Get reward (logits mean)\n",
    "        reward = self.explainer.get_response_logits_mean_from_clips(template_clip_text, clip_with_response)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters(sampled_indices, reward.item())\n",
    "        \n",
    "        return reward.item()\n",
    "    \n",
    "    def run(self, query: str, response_text: str, num_iterations=50, p_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Run the Thompson sampling algorithm for multiple iterations.\n",
    "        \n",
    "        Args:\n",
    "            query: The original query text\n",
    "            response_text: The model's response text\n",
    "            num_iterations: Number of iterations to run\n",
    "            p_threshold: Probability threshold for sampling\n",
    "            \n",
    "        Returns:\n",
    "            The best clip found and its reward\n",
    "        \"\"\"\n",
    "        for i in range(num_iterations):\n",
    "            reward = self.run_iteration(query, response_text, p_threshold)\n",
    "            \n",
    "            # Optional: Print progress\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"Iteration {i+1}/{num_iterations}, Reward: {reward:.4f}, Best: {self.best_reward:.4f}\")\n",
    "        \n",
    "        # Return the best clip and its content\n",
    "        best_clip_text = self.construct_clip_from_indices(self.best_clip)\n",
    "        return self.best_clip, best_clip_text, self.best_reward\n",
    "    \n",
    "    def analyze_importance(self, normalize=True):\n",
    "        \"\"\"\n",
    "        Analyze the importance of different content units based on Thompson sampling parameters.\n",
    "        \n",
    "        Args:\n",
    "            normalize: Whether to normalize importance scores to [0,1] range\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping content units to their importance scores\n",
    "        \"\"\"\n",
    "        importance_scores = {}\n",
    "        \n",
    "        # Calculate importance as probability of being selected (alpha/(alpha+beta))\n",
    "        for key in self.alphas.keys():\n",
    "            importance_scores[key] = self.alphas[key] / (self.alphas[key] + self.betas[key])\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize and importance_scores:\n",
    "            min_score = min(importance_scores.values())\n",
    "            max_score = max(importance_scores.values())\n",
    "            if max_score > min_score:  # Avoid division by zero\n",
    "                for key in importance_scores:\n",
    "                    importance_scores[key] = (importance_scores[key] - min_score) / (max_score - min_score)\n",
    "        \n",
    "        return importance_scores\n",
    "    \n",
    "    def get_top_content_units(self, n=5):\n",
    "        \"\"\"\n",
    "        Get the top N most important content units.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of top units to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (content_unit, importance_score) tuples\n",
    "        \"\"\"\n",
    "        importance_scores = self.analyze_importance()\n",
    "        \n",
    "        # Sort by importance score in descending order\n",
    "        sorted_units = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N units with their content\n",
    "        top_units = []\n",
    "        for idx, score in sorted_units[:n]:\n",
    "            if len(idx) == 1:\n",
    "                content = self.input_mapper.get_content(idx[0])\n",
    "                unit_type = \"sentence\"\n",
    "            elif len(idx) == 2:\n",
    "                content = self.input_mapper.get_content(idx[0], idx[1])\n",
    "                unit_type = \"phrase\"\n",
    "            elif len(idx) == 3:\n",
    "                content = self.input_mapper.get_content(idx[0], idx[1], idx[2])\n",
    "                unit_type = \"word\"\n",
    "            \n",
    "            top_units.append((idx, unit_type, content, score))\n",
    "        \n",
    "        return top_units\n",
    "\n",
    "# Enhanced MABRecorder class\n",
    "class MABRecorder:\n",
    "    def __init__(self, input_mapper: MultiLevelInputMapper):\n",
    "        # Create a single combined recorder dictionary\n",
    "        self.content_recorder = {}\n",
    "        \n",
    "        # Add all content unit keys\n",
    "        for k in input_mapper.sentences.keys():\n",
    "            self.content_recorder[k] = {'pulls': 0, 'rewards': [], 'avg_reward': 0}\n",
    "            \n",
    "        for k in input_mapper.phrases.keys():\n",
    "            self.content_recorder[k] = {'pulls': 0, 'rewards': [], 'avg_reward': 0}\n",
    "            \n",
    "        for k in input_mapper.words.keys():\n",
    "            self.content_recorder[k] = {'pulls': 0, 'rewards': [], 'avg_reward': 0}\n",
    "        \n",
    "        self.input_mapper = input_mapper\n",
    "\n",
    "    def record(self, clip_indices, reward):\n",
    "        \"\"\"\n",
    "        Record the usage and reward of content elements.\n",
    "        \n",
    "        Args:\n",
    "            clip_indices: List of indices tuples to record\n",
    "            reward: The reward obtained for this clip\n",
    "        \"\"\"\n",
    "        # Update statistics for each index in the clip\n",
    "        for idx in clip_indices:\n",
    "            if idx in self.content_recorder:\n",
    "                self.content_recorder[idx]['pulls'] += 1\n",
    "                self.content_recorder[idx]['rewards'].append(reward)\n",
    "                self.content_recorder[idx]['avg_reward'] = sum(self.content_recorder[idx]['rewards']) / len(self.content_recorder[idx]['rewards'])\n",
    "    \n",
    "    def get_top_units_by_reward(self, n=5):\n",
    "        \"\"\"\n",
    "        Get the top N content units by average reward.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of top units to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (content_unit, avg_reward, pull_count) tuples\n",
    "        \"\"\"\n",
    "        # Filter to only units that have been pulled\n",
    "        pulled_units = {k: v for k, v in self.content_recorder.items() if v['pulls'] > 0}\n",
    "        \n",
    "        # Sort by average reward\n",
    "        sorted_units = sorted(pulled_units.items(), key=lambda x: x[1]['avg_reward'], reverse=True)\n",
    "        \n",
    "        # Return top N\n",
    "        return [(idx, stats['avg_reward'], stats['pulls']) for idx, stats in sorted_units[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text with the input mapper\n",
    "input_mapper.process_text(query)\n",
    "\n",
    "# Create the Thompson MAB explainer\n",
    "thompson_explainer = ThompsonMABExplainer(explainer, input_mapper)\n",
    "\n",
    "# Run the algorithm\n",
    "best_indices, best_clip, best_reward = thompson_explainer.run(\n",
    "    query=query, \n",
    "    response_text=response_texts, \n",
    "    num_iterations=50,\n",
    "    p_threshold=0.5\n",
    ")\n",
    "\n",
    "# Analyze results\n",
    "print(f\"Best reward: {best_reward}\")\n",
    "print(f\"Best clip: {best_clip}\")\n",
    "\n",
    "# Get the most important content units\n",
    "top_units = thompson_explainer.get_top_content_units(n=5)\n",
    "for idx, unit_type, content, score in top_units:\n",
    "    print(f\"{unit_type} {idx}: '{content}' (importance: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = \"Hello world! This is a test, or not a test. How are you?\"\n",
    "user_input = \"The service at this restaurant was fantastic, and the staff were so friendly.\"\n",
    "template_input = template.format(user_input) \n",
    "tokenized_template_input = tokenizer(template_input, return_tensors=\"pt\").to(device)\n",
    "# get input_ids and attention_mask\n",
    "input_ids = tokenized_template_input.input_ids \n",
    "attention_mask = tokenized_template_input.attention_mask \n",
    "\n",
    "# get the output_ids and output_attention_mask\n",
    "output = llm.generate(input_ids, attention_mask, max_new_tokens=100)\n",
    "output_ids = output['input_ids']\n",
    "output_attention_mask = output['attention_mask']\n",
    "\n",
    "# get the response mask\n",
    "response_mask = explainer.get_response_mask(attention_mask, output_attention_mask)\n",
    "\n",
    "response_ids = output_ids[response_mask == 1]\n",
    "response_texts = tokenizer.decode(response_ids[0], skip_special_tokens=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.explainer.mab_explainer import MABRecorder\n",
    "random_clips = explainer.random_clip_query_words(user_input, 'positive')\n",
    "\n",
    "recorder = MABRecorder(explainer.input_mapper)\n",
    "\n",
    "for clip, clip_with_response, clipped_query_indices in random_clips:\n",
    "    # print(clip)\n",
    "    print(clipped_query_indices)\n",
    "    # print(clip)\n",
    "    # print(clip_with_response)\n",
    "    logits_mean = explainer.get_response_logits_mean_from_clips(clip, clip_with_response)\n",
    "    print(logits_mean)\n",
    "    # logits = explainer.get_response_logits_from_clips(clip, clip_with_response)\n",
    "    recorder.record(clipped_query_indices, logits_mean)\n",
    "\n",
    "    \n",
    "    # explainer.get_response_logits_mean(input_ids, attention_mask, response_mask)\n",
    "    print(\"-\"*100)\n",
    "\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(template)\n",
    "input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "output = llm.generate(input_ids, attention_mask, max_new_tokens=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = output['input_ids']\n",
    "output_attention_mask = output['attention_mask']\n",
    "\n",
    "output_texts = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "print(output_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
