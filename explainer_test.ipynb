{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.llm.smollm import LLMWrapper\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "llm = LLMWrapper(checkpoint, device=device)\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.explainer.mab_explainer import MABExplainer, MABTemplate\n",
    "instruction = \"Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.\"\n",
    "\n",
    "template = MABTemplate(tokenizer, instruction)\n",
    "explainer = MABExplainer(llm, tokenizer, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Hello world! This is a test, or not a test. How are you?\"\n",
    "template_input = template.format(user_input) \n",
    "tokenized_template_input = tokenizer(template_input, return_tensors=\"pt\").to(device)\n",
    "# get input_ids and attention_mask\n",
    "input_ids = tokenized_template_input.input_ids \n",
    "attention_mask = tokenized_template_input.attention_mask \n",
    "\n",
    "# get the output_ids and output_attention_mask\n",
    "output = llm.generate(input_ids, attention_mask, max_new_tokens=100)\n",
    "output_ids = output['input_ids']\n",
    "output_attention_mask = output['attention_mask']\n",
    "\n",
    "# get the response mask\n",
    "response_mask = explainer.get_response_mask(attention_mask, output_attention_mask)\n",
    "\n",
    "response_ids = output_ids[response_mask == 1]\n",
    "response_texts = tokenizer.decode(response_ids[0], skip_special_tokens=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This a a How you ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "is a test , not test How are you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "is a test or not a How<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "a , not are you ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello ! This is a , not a<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_clips = explainer.random_clip_query_words(user_input, response_texts)\n",
    "\n",
    "for clip in random_clips:\n",
    "    print(clip)\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.explainer.mab_explainer import MultiLevelInputMapper\n",
    "mapper = MultiLevelInputMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.process_text(\"Hello world! This is a test, or not a test. How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'or not a test.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper.get_content(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello world!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user_input = \"The service at this restaurant was fantastic, and the staff were so friendly.\"\n",
    "mapper.process_text(user_input) \n",
    "\n",
    "clipped_content_indices = mapper.sample_phrases(p=0.5)\n",
    "# concatenate the content of the clipped_content_indices\n",
    "clipped_content = \" \".join([mapper.get_content(*i) for i in clipped_content_indices])\n",
    "\n",
    "\n",
    "content = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": instruction\n",
    "            },\n",
    "\n",
    "            {\"role\": \"user\", \n",
    "            \"content\": clipped_content\n",
    "            }\n",
    "        ]\n",
    "template = tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   1627,  10263,    220,   2366,     19,    271,   2127,\n",
      "          56956,    279,  27065,    315,    279,   2768,  11914,    323,   6013,\n",
      "            449,   1193,    832,   3492,     25,    364,  31587,    518,    364,\n",
      "          43324,    518,    477,    364,  60668,    518,   3196,    389,    279,\n",
      "           8244,  16630,    323,   7438,    315,    279,  11914,     11,    422,\n",
      "            912,   3403,   2038,   3984,     11,   6013,    449,    364,   1962,\n",
      "           2867,      6,    449,    459,  16540,     13, 128009, 128006,    882,\n",
      "         128007,    271,   9906,   1917,      0, 128009, 128006,  78191, 128007,\n",
      "            271,  60668, 128009]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(template)\n",
    "input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "attention_mask = torch.tensor(inputs[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "output = llm.generate(input_ids, attention_mask, max_new_tokens=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive', 'negative', or 'neutral', based on the overall tone and meaning of the sentence, if no enough information provided, respond with 'not clear' with an explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello world!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "neutral<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "output_ids = output['input_ids']\n",
    "output_attention_mask = output['attention_mask']\n",
    "\n",
    "output_texts = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "print(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.explainer.mab_explainer import MABExplainer\n",
    "explainer = MABExplainer(llm, tokenizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mask = explainer.get_response_mask(attention_mask, output_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "results = explainer.get_response_logits(output_ids, output_attention_mask, response_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.2944], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.get_response_logits_mean(output_ids, output_attention_mask, response_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
