{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from llmexp.llm.smollm import LLMWrapper\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "\n",
    "llm = LLMWrapper(checkpoint, device=device)\n",
    "tokenizer = llm.tokenizer\n",
    "llm.hidden_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "I hate this movie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instruction = \"Analyze the sentiment of the following sentence. Be brief.\"\n",
    "instruction = \"Analyze the sentiment of the following sentence and respond concisely.\"\n",
    "user_input = \"I hate this movie.\"\n",
    "\n",
    "content = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": instruction\n",
    "            },\n",
    "\n",
    "            {\"role\": \"sentence\", \n",
    "            \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "\n",
    "template = tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "I hate this movie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "begin_marker = \"<|begin_of_text|>\"\n",
    "system_markers = (\"<|start_header_id|>system<|end_header_id|>\", \"<|eot_id|>\")\n",
    "user_markers = (\"<|start_header_id|>sentence<|end_header_id|>\", \"<|eot_id|>\")\n",
    "prompt_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "prefix = system_markers[0] + \"\\n\\n\" + instruction + system_markers[1]\n",
    "user_prefix = user_markers[0] + \"\\n\\n\"\n",
    "\n",
    "user_suffix = user_markers[1]\n",
    "\n",
    "suffix = prompt_marker\n",
    "\n",
    "print(begin_marker + prefix + user_prefix + user_input + user_suffix + suffix)\n",
    "\n",
    "\n",
    "# prefix + user_prefix, user_input, user_suffix + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond concisely.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "I hate this movie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sentiment of the sentence is strongly negative, with a tone of intense dislike and frustration.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# instruction = \"Analyze the sentiment of the following sentence. Be brief.\"\n",
    "# instruction = \"Analyze the sentiment of the following sentence and respond with only one word: 'positive,' 'negative,' or 'neutral,' based on the overall tone and meaning of the sentence. Do not provide any additional explanation.\"\n",
    "instruction = \"Analyze the sentiment of the following sentence and respond concisely.\"\n",
    "user_input = \"I hate this movie.\"\n",
    "\n",
    "content = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": instruction\n",
    "            },\n",
    "\n",
    "            {\"role\": \"sentence\", \n",
    "            \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "template = tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)\n",
    "# print(template)\n",
    "\n",
    "# The generated outputs \n",
    "gen_output = llm.generate_from_texts(template)\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.utils.data_utils import LLMDataset, create_dataloader\n",
    "dataloader = create_dataloader('sst2', tokenizer, instruction=instruction)\n",
    "# batch = collate_fun(dataset[:10])\n",
    "iterator = iter(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iterator).to(device)\n",
    "outputs = llm.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128009, 128009, 128009,  ...,    271,  31587, 128009],\n",
       "         [128009, 128009, 128009,  ...,    271,  31587, 128009],\n",
       "         [128009, 128009, 128009,  ...,    271,  43324, 128009],\n",
       "         ...,\n",
       "         [128009, 128009, 128009,  ...,    271,  43324, 128009],\n",
       "         [128009, 128009, 128009,  ...,    271,  31587, 128009],\n",
       "         [128009, 128009, 128009,  ...,    271,  43324, 128009]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.trainer.mab_trainer import randomly_cut_and_pad_generations\n",
    "cut_outputs = randomly_cut_and_pad_generations(inputs, outputs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!elegantly!!!!\n",
      "\n",
      "positive\n",
      "!!!!!!!!!!!!!!!!!!!!!<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive,' 'negative,' or 'neutral,' based on the! tone and meaning of the sentence. Do not provide any additional explanation.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "elegantly <|eot_id|>!assistant<|end_header_id|>\n",
      "\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "cut_outputs\n",
    "print(tokenizer.decode(cut_outputs['input_ids'][0] * cut_outputs['context_mask'][0]))\n",
    "print(tokenizer.decode(cut_outputs['input_ids'][0] * cut_outputs['attention_mask'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m2\u001b[39m]))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Analyze the sentiment of the following sentence. Be brief.!<|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "completely honest!<|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs['input_ids'][0] * inputs['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LLMWrapper.generate() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m generated_outputs\n",
      "\u001b[0;31mTypeError\u001b[0m: LLMWrapper.generate() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "generated_outputs = llm.generate(inputs,  max_new_tokens=50)\n",
    "generated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive,' 'negative,' or 'neutral,' based on the overall tone and meaning of the sentence. Do not provide any additional explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "a minor miracle<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "from llmexp.utils import decode_batch_outputs\n",
    "\n",
    "decoded_outputs = decode_batch_outputs(**generated_outputs, tokenizer=tokenizer)\n",
    "print(decoded_outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.0502,  -9.7437,  -8.4180,  ...,   9.6519,   9.6512,   9.6513],\n",
      "         [ -7.5367,  -9.8883,  -7.2537,  ...,   9.1260,   9.1252,   9.1252],\n",
      "         [ -7.0737,  -9.6388,  -6.2391,  ...,   9.5185,   9.5179,   9.5182],\n",
      "         ...,\n",
      "         [  7.4884,  16.6444,  12.5971,  ...,   3.8764,   3.8753,   3.8754],\n",
      "         [ 12.2231,  10.0633,   1.0912,  ...,   2.6447,   2.6436,   2.6442],\n",
      "         [ -1.7467,  -4.1949,  -4.9134,  ...,   8.8656,   8.8649,   8.8649]],\n",
      "\n",
      "        [[ -6.4945, -10.2150,  -8.4819,  ...,   9.8243,   9.8233,   9.8233],\n",
      "         [ -8.9331, -11.6798,  -8.0162,  ...,   9.9392,   9.9385,   9.9382],\n",
      "         [ -7.6666, -10.0353,  -6.7793,  ...,  10.0226,  10.0221,  10.0221],\n",
      "         ...,\n",
      "         [  8.1064,  15.2264,  13.7822,  ...,   5.4619,   5.4608,   5.4613],\n",
      "         [ 11.7456,  10.9452,   1.7162,  ...,   3.1175,   3.1164,   3.1170],\n",
      "         [ -2.6772,  -7.5897,  -5.3533,  ...,   9.3135,   9.3124,   9.3125]],\n",
      "\n",
      "        [[ -6.5032,  -8.3647,  -8.1158,  ...,   9.6742,   9.6731,   9.6732],\n",
      "         [ -8.2497, -10.3459,  -8.3313,  ...,   9.4715,   9.4709,   9.4708],\n",
      "         [ -7.7183,  -9.1067,  -7.5906,  ...,   9.5321,   9.5317,   9.5320],\n",
      "         ...,\n",
      "         [  8.0554,  17.0699,  13.5888,  ...,   4.0589,   4.0580,   4.0586],\n",
      "         [ 12.1119,  10.7101,   2.3401,  ...,   3.1737,   3.1726,   3.1734],\n",
      "         [ -2.8501,  -5.5048,  -5.4820,  ...,   9.0517,   9.0508,   9.0509]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -5.9940,  -9.7569,  -8.3702,  ...,   9.5702,   9.5692,   9.5693],\n",
      "         [ -9.3157, -11.9396,  -8.8739,  ...,   9.4078,   9.4073,   9.4070],\n",
      "         [ -7.9731, -10.2143,  -7.4524,  ...,  10.0004,  10.0001,  10.0001],\n",
      "         ...,\n",
      "         [  8.2237,  14.2282,  12.7251,  ...,   4.6681,   4.6671,   4.6677],\n",
      "         [ 11.0303,   9.2180,  -0.2849,  ...,   2.9902,   2.9892,   2.9899],\n",
      "         [ -3.3504,  -8.9320,  -6.2993,  ...,   9.8177,   9.8167,   9.8169]],\n",
      "\n",
      "        [[ -5.8792,  -7.6956,  -8.3241,  ...,   9.9320,   9.9314,   9.9313],\n",
      "         [ -7.8983,  -8.5761,  -8.5490,  ...,  10.0231,  10.0227,  10.0227],\n",
      "         [ -8.0954,  -9.3003,  -8.9953,  ...,  10.8826,  10.8824,  10.8828],\n",
      "         ...,\n",
      "         [  8.6895,  15.4320,  13.6028,  ...,   4.8386,   4.8375,   4.8381],\n",
      "         [ 10.8270,   9.4972,  -0.4262,  ...,   2.8259,   2.8252,   2.8258],\n",
      "         [ -3.5896,  -9.4353,  -6.7606,  ...,   9.8566,   9.8556,   9.8558]],\n",
      "\n",
      "        [[ -5.4583,  -9.8569,  -8.3007,  ...,   9.4310,   9.4301,   9.4302],\n",
      "         [ -8.7559, -11.3434,  -8.2541,  ...,   9.5213,   9.5209,   9.5207],\n",
      "         [ -7.5737,  -9.8480,  -6.8407,  ...,   9.9271,   9.9268,   9.9269],\n",
      "         ...,\n",
      "         [  7.6882,  14.4787,  12.4847,  ...,   4.7189,   4.7175,   4.7179],\n",
      "         [ 11.4118,  10.3536,   0.8518,  ...,   3.1524,   3.1512,   3.1518],\n",
      "         [ -2.7543,  -6.5899,  -5.7148,  ...,   9.1449,   9.1440,   9.1441]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([32, 114, 128256])\n"
     ]
    }
   ],
   "source": [
    "output_logits = llm(**generated_outputs).logits\n",
    "print(output_logits)\n",
    "print(output_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(128009, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.argmax(output_logits[0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128009, 128009, 128009,  ...,  78191, 128007,    271],\n",
      "        [128009, 128009, 128009,  ...,  78191, 128007,    271],\n",
      "        [128009, 128009, 128009,  ...,  78191, 128007,    271],\n",
      "        ...,\n",
      "        [128009, 128009, 128009,  ...,  78191, 128007,    271],\n",
      "        [128009, 128009, 128009,  ...,  78191, 128007,    271],\n",
      "        [128009, 128009, 128009,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1], device='cuda:0')}\n",
      "{'input_ids': tensor([[128009, 128009, 128009,  ..., 128009, 128009, 128009],\n",
      "        [128009, 128009, 128009,  ..., 128009, 128009, 128009],\n",
      "        [128009, 128009, 128009,  ..., 128009, 128009, 128009],\n",
      "        ...,\n",
      "        [128009, 128009, 128009,  ..., 128009, 128009, 128009],\n",
      "        [128009, 128009, 128009,  ..., 128009, 128009, 128009],\n",
      "        [128009, 128009, 128009,  ..., 128009, 128009, 128009]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.trainer.mab_trainer import randomly_cut_and_pad_generations\n",
    "random_cropped_inputs =randomly_cut_and_pad_generations(inputs, generated_outputs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0263, -0.0066, -0.0134,  ...,  0.0087, -0.0066,  0.0210],\n",
       "        [-0.0017,  0.0019, -0.0087,  ..., -0.0011,  0.0053,  0.0127],\n",
       "        [-0.0095, -0.0140, -0.0140,  ...,  0.0107, -0.0203,  0.0189],\n",
       "        ...,\n",
       "        [-0.0130, -0.0130, -0.0140,  ..., -0.0132, -0.0129,  0.0084],\n",
       "        [ 0.0066,  0.0051,  0.0033,  ...,  0.0083, -0.0102,  0.0258],\n",
       "        [-0.0003, -0.0062, -0.0081,  ..., -0.0084, -0.0175,  0.0106]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmexp.explainer.mab_model import MABModel\n",
    "mab_model = MABModel(llm, hidden_size=2048).to(device)\n",
    "mu_logits = mab_model.forward(**random_cropped_inputs)\n",
    "mu_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 119])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 120])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cropped_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' text'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(output_logits[0,53]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor(0.7496, device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor(128009, device='cuda:0'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.softmax(output_logits[0,19], dim=-1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' following'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0,54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import re\n",
    "dataset = load_dataset(\"google-research-datasets/tydiqa\", \"primary_task\", split='train')\n",
    "# dataset = load_dataset(\"tydiqa\", \"goldp\", split='train')\n",
    "english_train = dataset.filter(lambda x: x[\"language\"] == \"english\")\n",
    "item = english_train[0]\n",
    "\n",
    "# words = re.findall(r'\\w+|[^\\w\\s]', item['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = item['document_plaintext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did the art deco movement begin?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmexp.data.tydi import TydiDataset\n",
    "\n",
    "dataset = TydiDataset()\n",
    "\n",
    "item = dataset[0]\n",
    "\n",
    "item['sentences']\n",
    "item['question']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item['sentences'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
