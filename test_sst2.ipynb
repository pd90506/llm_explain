{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.llm.smollm import LLMWrapper\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "# checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "checkpoint = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "saved_mab_model = \"checkpoints/mab_model_100.pth\"\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "\n",
    "llm = LLMWrapper(checkpoint, device=device)\n",
    "tokenizer = llm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Analyze the sentiment of the following sentence and respond with only one word: 'positive,' 'negative,' or 'neutral,' based on the overall tone and meaning of the sentence. Do not provide any additional explanation.<|eot_id|><|start_header_id|>sentence<|end_header_id|>\n",
      "\n",
      "I am extremely disappointed with the quality; it broke after just one day.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# instruction = \"Analyze the sentiment of the following sentence. Be brief.\"\n",
    "instruction = \"Analyze the sentiment of the following sentence and respond with only one word: 'positive,' 'negative,' or 'neutral,' based on the overall tone and meaning of the sentence. Do not provide any additional explanation.\"\n",
    "user_input = \"I am extremely disappointed with the quality; it broke after just one day.\"\n",
    "# user_input = \"The service at this restaurant was fantastic, and the staff were so friendly.\"\n",
    "# user_input = \"I like this movie!\"\n",
    "\n",
    "content = [\n",
    "            {\"role\": \"system\", \n",
    "            \"content\": instruction\n",
    "            },\n",
    "\n",
    "            {\"role\": \"sentence\", \n",
    "            \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "template = tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)\n",
    "# print(template)\n",
    "\n",
    "# The generated outputs \n",
    "gen_output = llm.generate_from_texts(template)\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/1107694.1.gpu/ipykernel_678723/3922162976.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mab_model = MABModel.load_with_base_model(torch.load(saved_mab_model), llm, hidden_size=1024)\n"
     ]
    }
   ],
   "source": [
    "from llmexp.explainer.mab_model import MABModel\n",
    "mab_model = MABModel.load_with_base_model(torch.load(saved_mab_model), llm, hidden_size=1024)\n",
    "mab_model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "gen_inputs = tokenizer(gen_output, return_tensors=\"pt\").to(device)\n",
    "input_ids = gen_inputs['input_ids'][:, :-1]\n",
    "attention_mask = gen_inputs['attention_mask'][:, :-1]\n",
    "dist, values = mab_model.get_dist_value(input_ids, attention_mask)\n",
    "\n",
    "mab_values = torch.sigmoid(dist.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000, 0.9000, 0.9000]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "torch.Size([1, 95])\n"
     ]
    }
   ],
   "source": [
    "print(mab_values)\n",
    "print(mab_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,\n",
      "          33025,   2696,     25,   6790,    220,   2366,     18,    198,  15724,\n",
      "           2696,     25,    220,   1627,  10263,    220,   2366,     19,    271,\n",
      "           2127,  56956,    279,  27065,    315,    279,   2768,  11914,    323,\n",
      "           6013,    449,   1193,    832,   3492,     25,    364,  31587,   2965,\n",
      "            364,  43324,   2965,    477,    364,  60668,   2965,   3196,    389,\n",
      "            279,   8244,  16630,    323,   7438,    315,    279,  11914,     13,\n",
      "           3234,    539,   3493,    904,   5217,  16540,     13, 128009, 128006,\n",
      "          52989, 128007,    271,     40,   1097,   9193,  25406,    449,    279,\n",
      "           4367,     26,    433,  14760,   1306,   1120,    832,   1938,     13,\n",
      "         128009, 128006,  78191, 128007,    271,  43324]], device='cuda:0')\n",
      "torch.Size([1, 96])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens_with_values(input_ids, mab_values, tokenizer):\n",
    "    # Decode tokens one by one to preserve alignment\n",
    "    tokens = []\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        token = tokenizer.decode(input_ids[0, i:i+1])\n",
    "        tokens.append(token)\n",
    "    \n",
    "    # Normalize MAB values to [0,1] for color intensity first\n",
    "    normalized_values = (mab_values[0] - mab_values[0].min()) / (mab_values[0].max() - mab_values[0].min())\n",
    "    \n",
    "    # Pad normalized_values with a zero at the end\n",
    "    padded_normalized_values = torch.cat([normalized_values, torch.zeros(1, device=mab_values.device)], dim=0)\n",
    "    # Pad original mab_values with the last actual value\n",
    "    padded_mab_values = torch.cat([mab_values[0], mab_values[0][-1:]], dim=0)\n",
    "    \n",
    "    # Generate HTML with colored text and values\n",
    "    html_output = \"<div style='font-family: monospace; line-height: 2; background-color: white; padding: 10px;'>\"\n",
    "    for token, value, orig_value in zip(tokens, padded_normalized_values, padded_mab_values):\n",
    "        # Use a gradient from white to green\n",
    "        intensity = float(value)\n",
    "        green_color = int(intensity * 200)  # Control the maximum intensity\n",
    "        html_output += f'<span style=\"color: black; background-color: rgba(0, {green_color}, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\" title=\"MAB: {orig_value:.3f}, Norm: {value:.3f}\">{token}</span>'\n",
    "    html_output += \"</div>\"\n",
    "    \n",
    "    # Print the values\n",
    "    print(\"Token\\tNormalized Value\\tOriginal MAB Value\")\n",
    "    print(\"-\" * 50)\n",
    "    for token, value, orig_value in zip(tokens, padded_normalized_values, padded_mab_values):\n",
    "        print(f\"{token}\\t{value:.3f}\\t\\t{orig_value:.3f}\")\n",
    "    \n",
    "    from IPython.display import HTML\n",
    "    return HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m visualization \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_tokens_with_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmab_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display(visualization)\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mvisualize_tokens_with_values\u001b[0;34m(input_ids, mab_values, tokenizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, value, orig_value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens, padded_normalized_values, padded_mab_values):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Use a gradient from white to green\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     intensity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(value)\n\u001b[0;32m---> 21\u001b[0m     green_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mintensity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Control the maximum intensity\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     html_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<span style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor: black; background-color: rgba(0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgreen_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 0, 0.3); padding: 0.2em; margin: 0.1em; border-radius: 3px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Norm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</span>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m html_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</div>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "visualization = visualize_tokens_with_values(input_ids, mab_values, tokenizer)\n",
    "display(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
