{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:49<00:00, 12.39s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, LlamaForTokenClassification #, LlamaRotaryEmbedding\n",
    "from transformers import LlamaTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device,\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "config = model.config\n",
    "\n",
    "# model_2 = LlamaForTokenClassification.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     # device_map=device,\n",
    "#     token=access_token,\n",
    "# )\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# rotary_emb = LlamaRotaryEmbedding(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(examples):\n",
    "#     def num_words(x):\n",
    "#         return len(x.split())\n",
    "#     def get_first_k_words(x, k):\n",
    "#         return ' '.join(x.split()[:k])\n",
    "#     def get_cliped_text(texts, max_len):\n",
    "#         return [text if num_words(text) <= max_len else get_first_k_words(text, max_len) for text in texts]\n",
    "#     tokenizer = self.tokenizer\n",
    "#     max_len = 1024 # characters limit other than token limit\n",
    "#     if self.dataset == 'imdb':\n",
    "#         texts = [example['text'] for example in examples]\n",
    "#         texts = get_cliped_text(texts, max_len)\n",
    "#         sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "#     elif self.dataset == 'sst2':\n",
    "#         texts = [example['sentence'] for example in examples]\n",
    "#         texts = get_cliped_text(texts, max_len)\n",
    "#         sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "#     elif self.dataset == 'squad':\n",
    "#         context = [example['context'] for example in examples]\n",
    "#         context = get_cliped_text(context, max_len)\n",
    "#         question = [example['question'] for example in examples]\n",
    "#         # texts = [f\"Context: {context[i]}\\nQuestion: {question[i]}\" for i in range(len(context))]\n",
    "#         texts = [f\"Question: {question[i]}\\nContext: {context[i]}\" for i in range(len(context))]\n",
    "#         sys_context = \"You are a chatbot for answering questions. You can help users with their questions via concise responses.\"\n",
    "\n",
    "#     messages_lambda = lambda texts: [\n",
    "#         {\"role\": \"system\", \"content\": sys_context},\n",
    "#         {\"role\": \"user\", \"content\": texts},\n",
    "#     ]\n",
    "\n",
    "#     messages = list(map(messages_lambda, texts))\n",
    "\n",
    "#     messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#     )\n",
    "#     batch = tokenizer(\n",
    "#                 messages_with_template_applied,\n",
    "#                 add_special_tokens=False,\n",
    "#                 padding=True,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 )\n",
    "    \n",
    "#     # find the template boundaries\n",
    "#     text_lens = [len(tokenizer.encode(text)) for text in texts]\n",
    "#     text_lens_tensor = torch.tensor(text_lens, dtype=torch.long)\n",
    "    \n",
    "#     def apply_mask(mask_tensor, text_lens_tensor):\n",
    "#         batch_size, seq_len = mask_tensor.shape\n",
    "#         for i in range(batch_size):\n",
    "#             text_len = text_lens_tensor[i].item()\n",
    "#             mask_tensor[i, -text_len-5:-5] = 0\n",
    "#         return 1- mask_tensor\n",
    "\n",
    "#     mask_tensor = apply_mask(torch.ones_like(batch['input_ids']), text_lens_tensor)\n",
    "\n",
    "#     batch['context_mask'] = mask_tensor\n",
    "\n",
    "#     if self.dataset == 'squad':\n",
    "#         answers_start = [example['answers']['answer_start'][0] for example in examples]\n",
    "#         answers_end = [example['answers']['answer_start'][0] + len(example['answers']['text'][0]) for example in examples]\n",
    "#         batch['answers_start'] = torch.tensor(answers_start).long()\n",
    "#         batch['answers_end'] = torch.tensor(answers_end).long()\n",
    "    \n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.helper import LlmExpHelper\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['validation']\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/sst2\")\n",
    "# train_ds = ds['train']\n",
    "llm_exp_helper = LlmExpHelper(tokenizer, 'squad')\n",
    "collate_fn = llm_exp_helper.get_collate_fun()\n",
    "\n",
    "# Define batch size here!\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_dataloader))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in mask_gen_model.named_parameters():\n",
    "#     print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.squad_model_lora import MaskGeneratingModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(hidden_size=4096)\n",
    "mask_gen_model.to(device)\n",
    "\n",
    "# target_modules = []\n",
    "# num_layers = 6  # BERT-base 有 12 层\n",
    "# for i in range(num_layers):\n",
    "#     target_modules.extend([\n",
    "#         f\"explain_map.layer.{i}.attention.self.query\",\n",
    "#         f\"explain_map.layer.{i}.attention.self.key\",\n",
    "#         f\"explain_map.layer.{i}.attention.self.value\",\n",
    "#         f\"explain_map.layer.{i}.attention.output.dense\",\n",
    "#         f\"explain_map.layer.{i}.intermediate.dense\",\n",
    "#         f\"explain_map.layer.{i}.output.dense\"\n",
    "#     ])\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=4,  # 低秩矩阵的秩\n",
    "#     lora_alpha=32,  # LoRA 的缩放因子\n",
    "#     target_modules= target_modules,  # 目标模块\n",
    "#     lora_dropout=0.1  # Dropout 概率\n",
    "# )\n",
    "# mask_gen_model = get_peft_model(mask_gen_model, lora_config)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5475 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Epoch 1, Step 81: Loss = -0.0353, Actor Loss = -0.0555, Critic Loss = 0.0539, Entropy = 0.6817, Returns = 2.1297Value = 2.1272:   1%|▏         | 81/5475 [23:09<25:42:30, 17.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m response_mask[:, :\u001b[38;5;241m-\u001b[39mpad_length] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# TODO: 有问题. 有问题吗？\u001b[39;00m\n\u001b[1;32m     35\u001b[0m context_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(context_mask, (\u001b[38;5;241m0\u001b[39m, pad_length), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# # Get the last hidden state for the prompt + response sequence\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     # full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#                                                                    num_samples=1)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\u001b[39;00m\n\u001b[1;32m     55\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     56\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     57\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCritic Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     58\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     59\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturns = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     60\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model_lora.py:439\u001b[0m, in \u001b[0;36mMaskGeneratingModel.train_one_batch\u001b[0;34m(self, model, input_ids, attention_mask, context_mask, response_mask, optimizer, num_steps, mini_batch_size, ppo_epochs)\u001b[0m\n\u001b[1;32m    429\u001b[0m     advantages \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;241m-\u001b[39m values\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# print(\"states\", states.shape)\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# print(\"actions\", actions.shape)\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# print(\"log_probs\", log_probs.shape)\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# print(\"returns\", returns.shape)\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# print(\"advantages\", advantages.shape)\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# print(\"labels\", labels.shape)\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_dict\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model_lora.py:249\u001b[0m, in \u001b[0;36mMaskGeneratingModel.ppo_update\u001b[0;34m(self, model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, labels, clip_param)\u001b[0m\n\u001b[1;32m    246\u001b[0m input_ids, attention_mask, context_mask, response_mask \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    247\u001b[0m mask \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m--> 249\u001b[0m dist, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m entropy \u001b[38;5;241m=\u001b[39m ((dist\u001b[38;5;241m.\u001b[39mentropy() \u001b[38;5;241m*\u001b[39m context_mask)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m context_mask\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# print(\"dist.logits\", dist.logits[0])\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# print(\"context_mask * dist.entropy():\", (dist.entropy() * context_mask).sum(-1))\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# print(\"entropy:\", entropy)\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# print(\"context_mask:\", context_mask.sum(-1))\u001b[39;00m\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model_lora.py:181\u001b[0m, in \u001b[0;36mMaskGeneratingModel.get_dist_critic\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m    177\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Bernoulli(logits\u001b[38;5;241m=\u001b[39mmask_logits)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dist \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dist_critic\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, state):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    pred_features: torch.Tensor of shape [N, L, hidden_size]\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     input_ids, attention_mask, context_mask, response_mask \u001b[38;5;241m=\u001b[39m state\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_gen_model.train()\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        # (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "        unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "        unpaded_token_mask[:, :-pad_length] = 1\n",
    "        gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "        # print(gen_tokens[0])\n",
    "        # print(gen_attention_mask[0])\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "        context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "        loss_dict = mask_gen_model.train_one_batch(model, gen_tokens, gen_attention_mask, context_mask, response_mask, optimizer,\n",
    "                                                   num_steps=5, mini_batch_size=16, ppo_epochs=1)\n",
    "\n",
    "        # # Get the last hidden state for the prompt + response sequence\n",
    "        # with torch.no_grad():\n",
    "        #     # full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        #     # last_hidden_state = full_outputs.hidden_states[-1]\n",
    "        #     # last_hidden_state = last_hidden_state.float()\n",
    "        #     embedded = model.get_input_embeddings()(gen_tokens)\n",
    "        #     # last_hidden_state = model.get_encoder()(embedded, attention_mask=gen_attention_mask)[0]\n",
    "        #     last_hidden_state = embedded\n",
    "        #     last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        # mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, \n",
    "        #                                                                    num_samples=1)\n",
    "        # loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\n",
    "        log = f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss']:.4f}, \" \\\n",
    "               f\"Actor Loss = {loss_dict['actor_loss']:.4f}, \" \\\n",
    "               f\"Critic Loss = {loss_dict['critic_loss']:.4f}, \" \\\n",
    "               f\"Entropy = {loss_dict['entropy']:.4f}, \" \\\n",
    "               f\"Returns = {loss_dict['returns']:.4f}\" \\\n",
    "               f\"Value = {loss_dict['value']:.4f}\"\n",
    "        pbar.set_description(log)\n",
    "        # logging.debug(log)\n",
    "    \n",
    "\n",
    "\n",
    "        # # the mask_prob after the updates\n",
    "        # with torch.no_grad():\n",
    "        #     mask_logits_after = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        #     mask_gen_outputs_after = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_after, response_mask, \n",
    "        #                                                                     num_samples=5)\n",
    "        #     loss_after, reward_loss_after, mask_loss_after, mask_mean_after, mean_reward_after = mask_gen_outputs_after['loss'], mask_gen_outputs_after['reward_loss'], mask_gen_outputs_after['mask_loss'], mask_gen_outputs_after['mask_mean'], mask_gen_outputs_after['mean_reward']\n",
    "        #     mask_prob_after = (torch.sigmoid(mask_logits_after) * context_mask).clone().detach()\n",
    "        #     mean_reward_after = mean_reward_after.clone().detach()\n",
    "\n",
    "        # # load the parameters before the updates\n",
    "        # mask_gen_model.load_state_dict(params_before)\n",
    "        # mask_logits_before = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs_before = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_before, response_mask, \n",
    "        #                                                                    num_samples=5)\n",
    "        # loss_before, reward_loss_before, mask_loss_before, mask_mean_before, mean_reward_before = mask_gen_outputs_before['loss'], mask_gen_outputs_before['reward_loss'], mask_gen_outputs_before['mask_loss'], mask_gen_outputs_before['mask_mean'], mask_gen_outputs_before['mean_reward']\n",
    "        # mask_prob_before = (torch.sigmoid(mask_logits_before) * context_mask)\n",
    "\n",
    "        # # calculate the ratio of the mask probabilities before and after the updates\n",
    "        # ratio = mask_prob_after / (mask_prob_before + 1e-6)\n",
    "\n",
    "        # # 定义PPO的损失函数，假设clip_param是你定义的剪切参数\n",
    "        # clip_param = 0.2\n",
    "        # advantage = (mean_reward_after - mean_reward_before).unsqueeze(-1)  # 计算优势函数（advantage），这是根据任务定义的\n",
    "        # surr1 = ratio * advantage\n",
    "        # surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage\n",
    "        # ppo_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # # 更新模型参数\n",
    "        # optimizer.zero_grad()\n",
    "        # ppo_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # if idx % 10 == 0:\n",
    "        #     print()\n",
    "        # if idx % 10 == 0 and idx != 0:\n",
    "        #     torch.save(mask_gen_model.state_dict(), f'saved_model/mask_gen_model_lora_{epoch}_{idx}.pth') \n",
    "        #     print()\n",
    "        #     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# mask_gen_model.load_state_dict(torch.load('saved_model/mask_gen_model_0_1600.pth',map_location=device))\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "# texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I did not like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "# texts = \"I really love this film. The acting was great, and the story was amazing. I would recommend this movie to everyone.\"\n",
    "# # texts = \"I don't like this movie. The acting was terrible, and the story was boring. I would not recommend this movie to anyone.\"\n",
    "# messages_lambda = lambda texts: [\n",
    "#             {\"role\": \"system\", \"content\": \"Answer the question based on the context.\"},\n",
    "#             # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "#             {\"role\": \"user\", \"content\": texts},\n",
    "#         ]\n",
    "# messages = messages_lambda(texts)\n",
    "# messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "\n",
    "# # test_text = [{\"text\": texts, \"label\": None}]\n",
    "# test_text = [{\"sentence\": texts, \"label\": None}]\n",
    "# test_inputs = collate_fn(test_text).to(device)\n",
    "\n",
    "test_inputs = next(iter(test_dataloader)).to(device)\n",
    "# test_inputs = next(iter(train_dataloader)).to(device)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][idx])\n",
    "\n",
    "# generate the answer for the test inputs\n",
    "gen_outputs = model.generate(\n",
    "            input_ids=test_inputs['input_ids'],\n",
    "            attention_mask=test_inputs['attention_mask'],\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "input_ids = test_inputs['input_ids']\n",
    "attention_mask = test_inputs['attention_mask']\n",
    "gen_tokens = gen_outputs.sequences\n",
    "pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "# get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "context_mask = F.pad(test_inputs['context_mask'], (0, pad_length), mode='constant', value=0)\n",
    "# (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "unpaded_token_mask[:, :-pad_length] = 1\n",
    "gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "\n",
    "response_mask = gen_attention_mask.clone()\n",
    "response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "# context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # prompt_outputs = model(input_ids=test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "#     prompt_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "#     last_hidden_state = prompt_outputs.hidden_states[-1].float()\n",
    "#     mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state = gen_tokens, gen_attention_mask, context_mask, response_mask\n",
    "    dist, value = mask_gen_model.get_dist_critic(model, state)\n",
    "\n",
    "mask_logits = dist.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgb(255, 255, 255); color: black;\"><|start_header_id|>system<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">are</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">chatbot</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">for</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">answering</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">can</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">help</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">users</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">with</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">their</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">via</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">concise</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">responses.<|start_header_id|>user<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Question:</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">What</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Grotto</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">at</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Notre</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Dame</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(138, 255, 138); color: black;\">Context:</span> <span style=\"background-color: rgb(96, 255, 96); color: black;\">Architecturally,</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(126, 255, 126); color: black;\">school</span> <span style=\"background-color: rgb(108, 255, 108); color: black;\">has</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(170, 255, 170); color: black;\">Catholic</span> <span style=\"background-color: rgb(115, 255, 115); color: black;\">character.</span> <span style=\"background-color: rgb(170, 255, 170); color: black;\">Atop</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">Main</span> <span style=\"background-color: rgb(105, 255, 105); color: black;\">Building's</span> <span style=\"background-color: rgb(59, 255, 59); color: black;\">gold</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">dome</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(196, 255, 196); color: black;\">golden</span> <span style=\"background-color: rgb(165, 255, 165); color: black;\">statue</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(147, 255, 147); color: black;\">Virgin</span> <span style=\"background-color: rgb(191, 255, 191); color: black;\">Mary.</span> <span style=\"background-color: rgb(128, 255, 128); color: black;\">Immediately</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">in</span> <span style=\"background-color: rgb(125, 255, 125); color: black;\">front</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">Main</span> <span style=\"background-color: rgb(128, 255, 128); color: black;\">Building</span> <span style=\"background-color: rgb(109, 255, 109); color: black;\">and</span> <span style=\"background-color: rgb(207, 255, 207); color: black;\">facing</span> <span style=\"background-color: rgb(120, 255, 120); color: black;\">it,</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(105, 255, 105); color: black;\">copper</span> <span style=\"background-color: rgb(165, 255, 165); color: black;\">statue</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(162, 255, 162); color: black;\">Christ</span> <span style=\"background-color: rgb(105, 255, 105); color: black;\">with</span> <span style=\"background-color: rgb(88, 255, 88); color: black;\">arms</span> <span style=\"background-color: rgb(150, 255, 150); color: black;\">upraised</span> <span style=\"background-color: rgb(105, 255, 105); color: black;\">with</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(106, 255, 106); color: black;\">legend</span> <span style=\"background-color: rgb(152, 255, 152); color: black;\">\"Venite</span> <span style=\"background-color: rgb(228, 255, 228); color: black;\">Ad</span> <span style=\"background-color: rgb(198, 255, 198); color: black;\">Me</span> <span style=\"background-color: rgb(123, 255, 123); color: black;\">Omnes\".</span> <span style=\"background-color: rgb(209, 255, 209); color: black;\">Next</span> <span style=\"background-color: rgb(157, 255, 157); color: black;\">to</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(217, 255, 217); color: black;\">Main</span> <span style=\"background-color: rgb(128, 255, 128); color: black;\">Building</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(193, 255, 193); color: black;\">Basilica</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(210, 255, 210); color: black;\">Sacred</span> <span style=\"background-color: rgb(143, 255, 143); color: black;\">Heart.</span> <span style=\"background-color: rgb(128, 255, 128); color: black;\">Immediately</span> <span style=\"background-color: rgb(147, 255, 147); color: black;\">behind</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(170, 255, 170); color: black;\">basilica</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(143, 255, 143); color: black;\">Grotto,</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(183, 255, 183); color: black;\">Marian</span> <span style=\"background-color: rgb(179, 255, 179); color: black;\">place</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(128, 255, 128); color: black;\">prayer</span> <span style=\"background-color: rgb(109, 255, 109); color: black;\">and</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">reflection.</span> <span style=\"background-color: rgb(60, 255, 60); color: black;\">It</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(193, 255, 193); color: black;\">replica</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(162, 255, 162); color: black;\">grotto</span> <span style=\"background-color: rgb(145, 255, 145); color: black;\">at</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">Lourdes,</span> <span style=\"background-color: rgb(150, 255, 150); color: black;\">France</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">where</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(147, 255, 147); color: black;\">Virgin</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Mary</span> <span style=\"background-color: rgb(163, 255, 163); color: black;\">reputedly</span> <span style=\"background-color: rgb(175, 255, 175); color: black;\">appeared</span> <span style=\"background-color: rgb(157, 255, 157); color: black;\">to</span> <span style=\"background-color: rgb(69, 255, 69); color: black;\">Saint</span> <span style=\"background-color: rgb(121, 255, 121); color: black;\">Bernadette</span> <span style=\"background-color: rgb(150, 255, 150); color: black;\">Soubirous</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">in</span> <span style=\"background-color: rgb(152, 255, 152); color: black;\">1858.</span> <span style=\"background-color: rgb(185, 255, 185); color: black;\">At</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(168, 255, 168); color: black;\">end</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(175, 255, 175); color: black;\">main</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">drive</span> <span style=\"background-color: rgb(114, 255, 114); color: black;\">(and</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">in</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(102, 255, 102); color: black;\">direct</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">line</span> <span style=\"background-color: rgb(172, 255, 172); color: black;\">that</span> <span style=\"background-color: rgb(130, 255, 130); color: black;\">connects</span> <span style=\"background-color: rgb(94, 255, 94); color: black;\">through</span> <span style=\"background-color: rgb(147, 255, 147); color: black;\">3</span> <span style=\"background-color: rgb(95, 255, 95); color: black;\">statues</span> <span style=\"background-color: rgb(109, 255, 109); color: black;\">and</span> <span style=\"background-color: rgb(133, 255, 133); color: black;\">the</span> <span style=\"background-color: rgb(105, 255, 105); color: black;\">Gold</span> <span style=\"background-color: rgb(101, 255, 101); color: black;\">Dome),</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">is</span> <span style=\"background-color: rgb(155, 255, 155); color: black;\">a</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">simple,</span> <span style=\"background-color: rgb(166, 255, 166); color: black;\">modern</span> <span style=\"background-color: rgb(112, 255, 112); color: black;\">stone</span> <span style=\"background-color: rgb(165, 255, 165); color: black;\">statue</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">of</span> <span style=\"background-color: rgb(229, 255, 229); color: black;\">Mary.<|start_header_id|>assistant<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">The</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Grotto</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">at</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Notre</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Dame</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Marian</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">place</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">prayer</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">and</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">reflection,</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">located</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">behind</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Basilica</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Sacred</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Heart.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">It</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">replica</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">grotto</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">at</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Lourdes,</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">France,</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">where</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Virgin</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Mary</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">said</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">to</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">have</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">appeared</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">to</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Saint</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Bernadette</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Soubirous</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">in</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">1858.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, 4)\n",
    "test_ids = gen_tokens[idx]\n",
    "test_mask = gen_attention_mask[idx]\n",
    "test_mask_prob = torch.sigmoid(mask_logits[idx])\n",
    "# inverse TODO\n",
    "# test_mask_prob = 1 - test_mask_prob\n",
    "test_context_mask = context_mask[idx]\n",
    "\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_ids)\n",
    "scores = test_mask_prob * test_context_mask\n",
    "def normalize_except_zeros(array):\n",
    "    # Create a mask to identify non-zero elements\n",
    "    mask = array != 0\n",
    "    \n",
    "    # Extract non-zero elements\n",
    "    non_zero_elements = array[mask]\n",
    "    \n",
    "    # Normalize non-zero elements\n",
    "    min_val = np.min(non_zero_elements)\n",
    "    max_val = np.max(non_zero_elements)\n",
    "    normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create a copy of the original array to preserve zero values\n",
    "    normalized_array = np.copy(array)\n",
    "    \n",
    "    # Assign normalized values back to the corresponding positions\n",
    "    normalized_array[mask] = normalized_non_zero_elements\n",
    "    \n",
    "    return normalized_array\n",
    "scores = normalize_except_zeros(scores.detach().cpu().numpy())\n",
    "\n",
    "# remove special tokens\n",
    "filtered_token_scores = [(token, score) for token, score in zip(test_tokens, scores) if token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# combine subwords\n",
    "merged_tokens_scores = []\n",
    "current_token = \"\"\n",
    "current_score = 0\n",
    "count = 0\n",
    "\n",
    "for token, score in filtered_token_scores:\n",
    "    if token.startswith(\"Ġ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "            # merged_tokens_scores.append((\" \", 0))  # 添加空格\n",
    "        current_token = token[1:] # remove the speical character\n",
    "        current_score = score\n",
    "        count = 1\n",
    "    elif token.endswith(\"Ċ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "        merged_tokens_scores.append((\"<br><br>\", 0))  # 添加换行符\n",
    "        current_token = \"\"\n",
    "        current_score = 0\n",
    "        count = 0\n",
    "    else:\n",
    "        current_token += token\n",
    "        current_score += score\n",
    "        count += 1\n",
    "\n",
    "if current_token:\n",
    "    merged_tokens_scores.append((current_token, current_score / count))\n",
    "\n",
    "\n",
    "# 根据分数高亮文本（示例中使用HTML标签）\n",
    "highlighted_text = \"\"\n",
    "for token, score in merged_tokens_scores:\n",
    "    # 动态设置背景颜色：score为0时为白色，score为1时为绿色\n",
    "    red = int((1 - score) * 255)\n",
    "    green = 255\n",
    "    blue = int((1 - score) * 255)\n",
    "    color = f'rgb({red}, {green}, {blue})'\n",
    "    highlighted_text += f'<span style=\"background-color: {color}; color: black;\">{token}</span> '\n",
    "\n",
    "# 打印高亮后的文本\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(highlighted_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tokens_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask_prob * test_context_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " tensor([[50, 82, 98, 16, 84, 65,  6, 23, 75, 35],\n",
      "        [94, 55, 60, 49, 89, 10, 15, 23, 11, 57]])\n",
      "Probs:\n",
      " tensor([[0.1724, 0.5458, 0.9700, 0.3024, 0.7733, 0.0584, 0.8063, 0.6017, 0.9210,\n",
      "         0.9588],\n",
      "        [0.9951, 0.1759, 0.8418, 0.7502, 0.4592, 0.2548, 0.2959, 0.5222, 0.9853,\n",
      "         0.8102]])\n",
      "Mask:\n",
      " tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_segmented_mask(tokens, probs, punctuation_tokens):\n",
    "    \"\"\"\n",
    "    根据停顿标点将tokens进行分块，计算每段内的平均概率，然后根据平均概率生成一个mask。\n",
    "    \n",
    "    tokens: Tensor of shape [N, L] 表示文本tokens\n",
    "    probs: Tensor of shape [N, L] 表示Bernoulli分布的概率\n",
    "    punctuation_tokens: Set of punctuation tokens (e.g., {\",\", \".\", \"?\"})\n",
    "\n",
    "    Returns:\n",
    "    mask: Tensor of shape [N, L] 表示最终的mask\n",
    "    \"\"\"\n",
    "    N, L = tokens.shape\n",
    "    mask = torch.zeros_like(tokens, dtype=torch.bool)  # 初始化mask，和tokens形状相同\n",
    "    \n",
    "    for i in range(N):\n",
    "        start_idx = 0\n",
    "        segment_probs = []\n",
    "        for j in range(L):\n",
    "            if tokens[i, j].item() in punctuation_tokens or j == L - 1:  # 检查是否到达标点或行末\n",
    "                end_idx = j + 1  # 包含标点\n",
    "                \n",
    "                # 获取当前区块的概率\n",
    "                segment_prob = probs[i, start_idx:end_idx].mean()\n",
    "                segment_probs.append(segment_prob)\n",
    "                \n",
    "                # 对区块的概率进行采样\n",
    "                segment_sample = torch.bernoulli(segment_prob)\n",
    "                \n",
    "                # 设置mask\n",
    "                mask[i, start_idx:end_idx] = segment_sample\n",
    "                \n",
    "                # 更新起始索引\n",
    "                start_idx = end_idx\n",
    "                \n",
    "    return mask\n",
    "\n",
    "# 示例用法\n",
    "N, L = 2, 10  # 示例形状\n",
    "tokens = torch.randint(0, 100, (N, L))  # 示例tokens\n",
    "probs = torch.rand(N, L)  # 示例Bernoulli分布的概率\n",
    "punctuation_tokens = {20, 30, 40}  # 假设20, 30, 40为标点符号token\n",
    "\n",
    "mask = create_segmented_mask(tokens, probs, punctuation_tokens)\n",
    "print(\"Tokens:\\n\", tokens)\n",
    "print(\"Probs:\\n\", probs)\n",
    "print(\"Mask:\\n\", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 13, 30, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([',', '.', '?', '!'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
