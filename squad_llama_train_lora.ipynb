{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, LlamaForTokenClassification #, LlamaRotaryEmbedding\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device,\n",
    "    token=access_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff12037827f484b866a64e56d0f23b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e8da04f0a46f9a94c20811d5177f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca07506d33f54410a30d35176584bda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb759627c749400d8f6cd474be8bf0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, LlamaForTokenClassification #, LlamaRotaryEmbedding\n",
    "from transformers import LlamaTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device,\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "config = model.config\n",
    "\n",
    "# model_2 = LlamaForTokenClassification.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     # device_map=device,\n",
    "#     token=access_token,\n",
    "# )\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# rotary_emb = LlamaRotaryEmbedding(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlmExpHelper:\n",
    "    def __init__(self, tokenizer, dataset):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_collate_fun(self):\n",
    "        return lambda examples: self.collate_fn(examples)\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        def num_words(x):\n",
    "            return len(x.split())\n",
    "        def get_first_k_words(x, k):\n",
    "            return ' '.join(x.split()[:k])\n",
    "        def get_cliped_text(texts, max_len):\n",
    "            return [text if num_words(text) <= max_len else get_first_k_words(text, max_len) for text in texts]\n",
    "        tokenizer = self.tokenizer\n",
    "        max_len = 512 # characters limit other than token limit\n",
    "        if self.dataset == 'imdb':\n",
    "            texts = [example['text'] for example in examples]\n",
    "            texts = get_cliped_text(texts, max_len)\n",
    "            sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "        elif self.dataset == 'sst2':\n",
    "            texts = [example['sentence'] for example in examples]\n",
    "            texts = get_cliped_text(texts, max_len)\n",
    "            sys_context = \"You are a chatbot for sentiment analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"\n",
    "        elif self.dataset == 'squad':\n",
    "            context = [example['context'] for example in examples]\n",
    "            context = get_cliped_text(context, max_len)\n",
    "            question = [example['question'] for example in examples]\n",
    "            # texts = [f\"Context: {context[i]}\\nQuestion: {question[i]}\" for i in range(len(context))]\n",
    "            texts = [f\"Question: {question[i]}\\nContext: {context[i]}\" for i in range(len(context))]\n",
    "            sys_context = \"You are a chatbot for answering questions. You can help users with their questions via concise responses.\"\n",
    "\n",
    "        # labels = [example['label'] for example in examples]\n",
    "        messages_lambda = lambda texts: [\n",
    "            {\"role\": \"system\", \"content\": sys_context},\n",
    "            {\"role\": \"user\", \"content\": texts},\n",
    "        ]\n",
    "\n",
    "        messages = list(map(messages_lambda, texts))\n",
    "\n",
    "        messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        batch = tokenizer(\n",
    "                    messages_with_template_applied,\n",
    "                    add_special_tokens=False,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    )\n",
    "        \n",
    "        # find the template boundaries\n",
    "        text_lens = [len(tokenizer.encode(text)) for text in texts]\n",
    "        text_lens_tensor = torch.tensor(text_lens, dtype=torch.long)\n",
    "\n",
    "        \n",
    "        def apply_mask(mask_tensor, text_lens_tensor):\n",
    "            batch_size, seq_len = mask_tensor.shape\n",
    "            for i in range(batch_size):\n",
    "                text_len = text_lens_tensor[i].item()\n",
    "                mask_tensor[i, -text_len-5:-5] = 0\n",
    "            return 1- mask_tensor\n",
    "\n",
    "        mask_tensor = apply_mask(torch.ones_like(batch['input_ids']), text_lens_tensor)\n",
    "\n",
    "        batch['context_mask'] = mask_tensor\n",
    "\n",
    "        if self.dataset == 'squad':\n",
    "            answers_start = [example['answers']['answer_start'][0] for example in examples]\n",
    "            answers_end = [example['answers']['answer_start'][0] + len(example['answers']['text'][0]) for example in examples]\n",
    "            batch['answers_start'] = torch.tensor(answers_start).long()\n",
    "            batch['answers_end'] = torch.tensor(answers_end).long()\n",
    "\n",
    "            context_lens = [len(tokenizer.encode(context)) for context in context]\n",
    "            context_lens_tensor = torch.tensor(context_lens, dtype=torch.long)\n",
    "            mask_tensor_v2 = apply_mask(torch.ones_like(batch['input_ids']), context_lens_tensor)\n",
    "            batch['context_mask'] = mask_tensor_v2 * batch['attention_mask']\n",
    "            \n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.helper import LlmExpHelper\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = load_dataset(\"imdb\")\n",
    "# ds = load_dataset(\"rajpurkar/squad\")\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['test']\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/sst2\")\n",
    "# train_ds = ds['train']\n",
    "llm_exp_helper = LlmExpHelper(tokenizer, 'imdb')\n",
    "collate_fn = llm_exp_helper.get_collate_fun()\n",
    "\n",
    "# Define batch size here!\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_dataloader))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in mask_gen_model.named_parameters():\n",
    "#     print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.squad_model_lora import MaskGeneratingModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# model.to(device)\n",
    "# emb_weights = model.get_input_embeddings().weight.clone().float().to(\"cpu\")\n",
    "# mask_gen_model = MaskGeneratingModel(hidden_size=4096, emb_weights=emb_weights)\n",
    "mask_gen_model = MaskGeneratingModel()\n",
    "mask_gen_model.to(device)\n",
    "\n",
    "# target_modules = []\n",
    "# num_layers = 6  # BERT-base 有 12 层\n",
    "# for i in range(num_layers):\n",
    "#     target_modules.extend([\n",
    "#         f\"explain_map.layer.{i}.attention.self.query\",\n",
    "#         f\"explain_map.layer.{i}.attention.self.key\",\n",
    "#         f\"explain_map.layer.{i}.attention.self.value\",\n",
    "#         f\"explain_map.layer.{i}.attention.output.dense\",\n",
    "#         f\"explain_map.layer.{i}.intermediate.dense\",\n",
    "#         f\"explain_map.layer.{i}.output.dense\"\n",
    "#     ])\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=4,  # 低秩矩阵的秩\n",
    "#     lora_alpha=32,  # LoRA 的缩放因子\n",
    "#     target_modules= target_modules,  # 目标模块\n",
    "#     lora_dropout=0.1  # Dropout 概率\n",
    "# )\n",
    "# mask_gen_model = get_peft_model(mask_gen_model, lora_config)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = -1.1637, Actor Loss = -1.8603, Critic Loss = 1.3932, Entropy = 0.6931, Returns = 1.8845, Value = 0.7368, mask_loss = 0.4970std_loss = 0.0020:   0%|          | 1/1563 [00:16<7:14:09, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 2: Loss = 0.8660, Actor Loss = 0.0335, Critic Loss = 1.6650, Entropy = 0.6931, Returns = 1.9212, Value = 0.6392, mask_loss = 0.4997std_loss = 0.0026:   0%|          | 2/1563 [00:32<7:03:14, 16.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 3: Loss = -1.1394, Actor Loss = -1.2621, Critic Loss = 0.2455, Entropy = 0.6921, Returns = 1.8447, Value = 1.6390, mask_loss = 0.5208std_loss = 0.0083:   0%|          | 3/1563 [00:48<7:01:15, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 4: Loss = 0.3594, Actor Loss = 0.2164, Critic Loss = 0.2861, Entropy = 0.6926, Returns = 2.0121, Value = 1.5645, mask_loss = 0.5145std_loss = 0.0067:   0%|          | 4/1563 [01:03<6:42:07, 15.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 5: Loss = 0.3434, Actor Loss = 0.1167, Critic Loss = 0.4534, Entropy = 0.6929, Returns = 1.3937, Value = 1.7078, mask_loss = 0.5104std_loss = 0.0051:   0%|          | 5/1563 [01:20<6:57:19, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 6: Loss = -0.2967, Actor Loss = -0.3745, Critic Loss = 0.1557, Entropy = 0.6930, Returns = 1.9596, Value = 1.6513, mask_loss = 0.5077std_loss = 0.0048:   0%|          | 6/1563 [01:41<7:42:39, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 7: Loss = 0.2329, Actor Loss = 0.0740, Critic Loss = 0.3179, Entropy = 0.6930, Returns = 1.6605, Value = 1.8943, mask_loss = 0.5079std_loss = 0.0046:   0%|          | 7/1563 [01:58<7:37:32, 17.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 8: Loss = 0.5171, Actor Loss = 0.2740, Critic Loss = 0.4865, Entropy = 0.6931, Returns = 1.4967, Value = 1.5297, mask_loss = 0.5016std_loss = 0.0028:   1%|          | 8/1563 [02:15<7:32:39, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 9: Loss = 0.1285, Actor Loss = -0.0705, Critic Loss = 0.3980, Entropy = 0.6931, Returns = 1.7056, Value = 1.9452, mask_loss = 0.5036std_loss = 0.0032:   1%|          | 9/1563 [02:32<7:26:38, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 10: Loss = -0.1226, Actor Loss = -0.1435, Critic Loss = 0.0420, Entropy = 0.6931, Returns = 1.9458, Value = 2.0273, mask_loss = 0.5055std_loss = 0.0038:   1%|          | 10/1563 [02:48<7:12:38, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.3939, Actor Loss = 0.1731, Critic Loss = 0.4418, Entropy = 0.6931, Returns = 1.7870, Value = 1.3052, mask_loss = 0.4968std_loss = 0.0019:   1%|          | 10/1563 [03:08<7:12:38, 16.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.3939, Actor Loss = 0.1731, Critic Loss = 0.4418, Entropy = 0.6931, Returns = 1.7870, Value = 1.3052, mask_loss = 0.4968std_loss = 0.0019:   1%|          | 11/1563 [03:18<7:46:04, 18.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m response_mask[:, :\u001b[38;5;241m-\u001b[39mpad_length] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# TODO: 有问题. 有问题吗？\u001b[39;00m\n\u001b[1;32m     35\u001b[0m context_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(context_mask, (\u001b[38;5;241m0\u001b[39m, pad_length), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmask_gen_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# # Get the last hidden state for the prompt + response sequence\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     # full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#                                                                    num_samples=1)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\u001b[39;00m\n\u001b[1;32m     55\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     56\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     57\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCritic Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model_lora.py:413\u001b[0m, in \u001b[0;36mMaskGeneratingModel.train_one_batch\u001b[0;34m(self, model, input_ids, attention_mask, context_mask, response_mask, optimizer, num_steps, mini_batch_size, ppo_epochs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m--> 413\u001b[0m         dist, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m         action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    415\u001b[0m         action \u001b[38;5;241m=\u001b[39m action \u001b[38;5;241m*\u001b[39m context_mask\n",
      "File \u001b[0;32m~/wd/llm_explain/llmexp/squad_model_lora.py:205\u001b[0m, in \u001b[0;36mMaskGeneratingModel.get_dist_critic\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m    199\u001b[0m policy_logits, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(last_hidden_state, context_mask, response_mask\u001b[38;5;241m=\u001b[39mresponse_mask)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# print(\"policy_logits_origin\", policy_logits[0][:5][:10])\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# policy_logits = torch.diagonal(policy_logits, offset=0, dim1=0, dim2=1).permute(1,0) # [N, L]\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# print(\"policy_logits\", policy_logits[0][:100])\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# policy_logits = policy_logits * self.logit_scale.exp()\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mBernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_logits\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [N, L]\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist, value\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/distributions/bernoulli.py:59\u001b[0m, in \u001b[0;36mBernoulli.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/distributions/distribution.py:69\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     67\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m     68\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m---> 69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_gen_model.train()\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        # (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "        unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "        unpaded_token_mask[:, :-pad_length] = 1\n",
    "        gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "        # print(gen_tokens[0])\n",
    "        # print(gen_attention_mask[0])\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "        context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "        loss_dict = mask_gen_model.train_one_batch(model, gen_tokens, gen_attention_mask, context_mask, response_mask, optimizer,\n",
    "                                                   num_steps=3, mini_batch_size=16, ppo_epochs=2)\n",
    "\n",
    "        # # Get the last hidden state for the prompt + response sequence\n",
    "        # with torch.no_grad():\n",
    "        #     # full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        #     # last_hidden_state = full_outputs.hidden_states[-1]\n",
    "        #     # last_hidden_state = last_hidden_state.float()\n",
    "        #     embedded = model.get_input_embeddings()(gen_tokens)\n",
    "        #     # last_hidden_state = model.get_encoder()(embedded, attention_mask=gen_attention_mask)[0]\n",
    "        #     last_hidden_state = embedded\n",
    "        #     last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        # mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, \n",
    "        #                                                                    num_samples=1)\n",
    "        # loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\n",
    "        log = f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss']:.4f}, \" \\\n",
    "               f\"Actor Loss = {loss_dict['actor_loss']:.4f}, \" \\\n",
    "               f\"Critic Loss = {loss_dict['critic_loss']:.4f}, \" \\\n",
    "               f\"Entropy = {loss_dict['entropy']:.4f}, \" \\\n",
    "               f\"Returns = {loss_dict['returns']:.4f}, \" \\\n",
    "               f\"Value = {loss_dict['value']:.4f}, \" \\\n",
    "                f\"mask_loss = {loss_dict['mask_loss']:.4f}\" \\\n",
    "                f\"std_loss = {loss_dict['std_loss']:.4f}\" \\\n",
    "            #    f\"Cont_loss = {loss_dict['contrast_loss']:.4f}, \"  \\\n",
    "               \n",
    "        pbar.set_description(log)\n",
    "        # logging.debug(log)\n",
    "    \n",
    "\n",
    "\n",
    "        # # the mask_prob after the updates\n",
    "        # with torch.no_grad():\n",
    "        #     mask_logits_after = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        #     mask_gen_outputs_after = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_after, response_mask, \n",
    "        #                                                                     num_samples=5)\n",
    "        #     loss_after, reward_loss_after, mask_loss_after, mask_mean_after, mean_reward_after = mask_gen_outputs_after['loss'], mask_gen_outputs_after['reward_loss'], mask_gen_outputs_after['mask_loss'], mask_gen_outputs_after['mask_mean'], mask_gen_outputs_after['mean_reward']\n",
    "        #     mask_prob_after = (torch.sigmoid(mask_logits_after) * context_mask).clone().detach()\n",
    "        #     mean_reward_after = mean_reward_after.clone().detach()\n",
    "\n",
    "        # # load the parameters before the updates\n",
    "        # mask_gen_model.load_state_dict(params_before)\n",
    "        # mask_logits_before = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs_before = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_before, response_mask, \n",
    "        #                                                                    num_samples=5)\n",
    "        # loss_before, reward_loss_before, mask_loss_before, mask_mean_before, mean_reward_before = mask_gen_outputs_before['loss'], mask_gen_outputs_before['reward_loss'], mask_gen_outputs_before['mask_loss'], mask_gen_outputs_before['mask_mean'], mask_gen_outputs_before['mean_reward']\n",
    "        # mask_prob_before = (torch.sigmoid(mask_logits_before) * context_mask)\n",
    "\n",
    "        # # calculate the ratio of the mask probabilities before and after the updates\n",
    "        # ratio = mask_prob_after / (mask_prob_before + 1e-6)\n",
    "\n",
    "        # # 定义PPO的损失函数，假设clip_param是你定义的剪切参数\n",
    "        # clip_param = 0.2\n",
    "        # advantage = (mean_reward_after - mean_reward_before).unsqueeze(-1)  # 计算优势函数（advantage），这是根据任务定义的\n",
    "        # surr1 = ratio * advantage\n",
    "        # surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage\n",
    "        # ppo_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # # 更新模型参数\n",
    "        # optimizer.zero_grad()\n",
    "        # ppo_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        if idx % 1 == 0:\n",
    "            print()\n",
    "        if idx % 10 == 0 and idx != 0:\n",
    "            torch.save(mask_gen_model.state_dict(), f'saved_model/mask_gen_model_lora_{epoch}_{idx}.pth') \n",
    "        #     print()\n",
    "        #     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# mask_gen_model.load_state_dict(torch.load('saved_model/mask_gen_model_lora_0_470.pth',map_location=device))\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "# texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I did not like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "# texts = \"I really love this film. The acting was great, and the story was amazing. I would recommend this movie to everyone.\"\n",
    "# # texts = \"I don't like this movie. The acting was terrible, and the story was boring. I would not recommend this movie to anyone.\"\n",
    "# messages_lambda = lambda texts: [\n",
    "#             {\"role\": \"system\", \"content\": \"Answer the question based on the context.\"},\n",
    "#             # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "#             {\"role\": \"user\", \"content\": texts},\n",
    "#         ]\n",
    "# messages = messages_lambda(texts)\n",
    "# messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "\n",
    "# # test_text = [{\"text\": texts, \"label\": None}]\n",
    "# test_text = [{\"sentence\": texts, \"label\": None}]\n",
    "# test_inputs = collate_fn(test_text).to(device)\n",
    "\n",
    "test_inputs = next(iter(test_dataloader)).to(device)\n",
    "# test_inputs = next(iter(train_dataloader)).to(device)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][idx])\n",
    "\n",
    "# generate the answer for the test inputs\n",
    "gen_outputs = model.generate(\n",
    "            input_ids=test_inputs['input_ids'],\n",
    "            attention_mask=test_inputs['attention_mask'],\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "input_ids = test_inputs['input_ids']\n",
    "attention_mask = test_inputs['attention_mask']\n",
    "gen_tokens = gen_outputs.sequences\n",
    "pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "# get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "context_mask = F.pad(test_inputs['context_mask'], (0, pad_length), mode='constant', value=0)\n",
    "# (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "unpaded_token_mask[:, :-pad_length] = 1\n",
    "gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "\n",
    "response_mask = gen_attention_mask.clone()\n",
    "response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "# context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # prompt_outputs = model(input_ids=test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "#     prompt_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "#     last_hidden_state = prompt_outputs.hidden_states[-1].float()\n",
    "#     mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    state = gen_tokens, gen_attention_mask, context_mask, response_mask\n",
    "    dist, value = mask_gen_model.get_dist_critic(model, state)\n",
    "\n",
    "mask_logits = dist.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgb(255, 255, 255); color: black;\">system</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">are</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">chatbot</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">for</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">sentiment</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">analysis</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">can</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">help</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">users</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">with</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">their</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">via</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">concise</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">responses</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">POSITIVE</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">,</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">or</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">NEGATIVE</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><|start_header_id|>user</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(69, 255, 69); color: black;\">If</span> <span style=\"background-color: rgb(75, 255, 75); color: black;\">only</span> <span style=\"background-color: rgb(108, 255, 108); color: black;\">to</span> <span style=\"background-color: rgb(63, 255, 63); color: black;\">avoid</span> <span style=\"background-color: rgb(56, 255, 56); color: black;\">making</span> <span style=\"background-color: rgb(36, 255, 36); color: black;\">this</span> <span style=\"background-color: rgb(111, 255, 111); color: black;\">type</span> <span style=\"background-color: rgb(38, 255, 38); color: black;\">of</span> <span style=\"background-color: rgb(67, 255, 67); color: black;\">film</span> <span style=\"background-color: rgb(67, 255, 67); color: black;\">in</span> <span style=\"background-color: rgb(59, 255, 59); color: black;\">the</span> <span style=\"background-color: rgb(78, 255, 78); color: black;\">future</span> <span style=\"background-color: rgb(99, 255, 99); color: black;\">.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">This</span> <span style=\"background-color: rgb(79, 255, 79); color: black;\">film</span> <span style=\"background-color: rgb(37, 255, 37); color: black;\">is</span> <span style=\"background-color: rgb(42, 255, 42); color: black;\">interesting</span> <span style=\"background-color: rgb(16, 255, 16); color: black;\">as</span> <span style=\"background-color: rgb(80, 255, 80); color: black;\">an</span> <span style=\"background-color: rgb(72, 255, 72); color: black;\">experiment</span> <span style=\"background-color: rgb(48, 255, 48); color: black;\">but</span> <span style=\"background-color: rgb(71, 255, 71); color: black;\">tells</span> <span style=\"background-color: rgb(70, 255, 70); color: black;\">no</span> <span style=\"background-color: rgb(98, 255, 98); color: black;\">cogent</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">story.<br</span> <span style=\"background-color: rgb(200, 255, 200); color: black;\">/><br</span> <span style=\"background-color: rgb(86, 255, 86); color: black;\">/>One</span> <span style=\"background-color: rgb(142, 255, 142); color: black;\">might</span> <span style=\"background-color: rgb(48, 255, 48); color: black;\">feel</span> <span style=\"background-color: rgb(139, 255, 139); color: black;\">virtuous</span> <span style=\"background-color: rgb(95, 255, 95); color: black;\">for</span> <span style=\"background-color: rgb(122, 255, 122); color: black;\">sitting</span> <span style=\"background-color: rgb(64, 255, 64); color: black;\">thru</span> <span style=\"background-color: rgb(87, 255, 87); color: black;\">it</span> <span style=\"background-color: rgb(57, 255, 57); color: black;\">because</span> <span style=\"background-color: rgb(100, 255, 100); color: black;\">it</span> <span style=\"background-color: rgb(110, 255, 110); color: black;\">touches</span> <span style=\"background-color: rgb(37, 255, 37); color: black;\">on</span> <span style=\"background-color: rgb(109, 255, 109); color: black;\">so</span> <span style=\"background-color: rgb(47, 255, 47); color: black;\">many</span> <span style=\"background-color: rgb(102, 255, 102); color: black;\">IMPORTANT</span> <span style=\"background-color: rgb(81, 255, 81); color: black;\">issues</span> <span style=\"background-color: rgb(93, 255, 93); color: black;\">but</span> <span style=\"background-color: rgb(115, 255, 115); color: black;\">it</span> <span style=\"background-color: rgb(124, 255, 124); color: black;\">does</span> <span style=\"background-color: rgb(141, 255, 141); color: black;\">so</span> <span style=\"background-color: rgb(125, 255, 125); color: black;\">without</span> <span style=\"background-color: rgb(121, 255, 121); color: black;\">any</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">discernable</span> <span style=\"background-color: rgb(91, 255, 91); color: black;\">motive</span> <span style=\"background-color: rgb(108, 255, 108); color: black;\">.</span> <span style=\"background-color: rgb(5, 255, 5); color: black;\">The</span> <span style=\"background-color: rgb(147, 255, 147); color: black;\">viewer</span> <span style=\"background-color: rgb(161, 255, 161); color: black;\">comes</span> <span style=\"background-color: rgb(134, 255, 134); color: black;\">away</span> <span style=\"background-color: rgb(111, 255, 111); color: black;\">with</span> <span style=\"background-color: rgb(121, 255, 121); color: black;\">no</span> <span style=\"background-color: rgb(118, 255, 118); color: black;\">new</span> <span style=\"background-color: rgb(91, 255, 91); color: black;\">perspectives</span> <span style=\"background-color: rgb(99, 255, 99); color: black;\">(unless</span> <span style=\"background-color: rgb(137, 255, 137); color: black;\">one</span> <span style=\"background-color: rgb(159, 255, 159); color: black;\">comes</span> <span style=\"background-color: rgb(123, 255, 123); color: black;\">up</span> <span style=\"background-color: rgb(33, 255, 33); color: black;\">with</span> <span style=\"background-color: rgb(132, 255, 132); color: black;\">one</span> <span style=\"background-color: rgb(136, 255, 136); color: black;\">while</span> <span style=\"background-color: rgb(106, 255, 106); color: black;\">one's</span> <span style=\"background-color: rgb(150, 255, 150); color: black;\">mind</span> <span style=\"background-color: rgb(156, 255, 156); color: black;\">wanders</span> <span style=\"background-color: rgb(127, 255, 127); color: black;\">,</span> <span style=\"background-color: rgb(81, 255, 81); color: black;\">as</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">it</span> <span style=\"background-color: rgb(175, 255, 175); color: black;\">will</span> <span style=\"background-color: rgb(170, 255, 170); color: black;\">invariably</span> <span style=\"background-color: rgb(127, 255, 127); color: black;\">do</span> <span style=\"background-color: rgb(81, 255, 81); color: black;\">during</span> <span style=\"background-color: rgb(56, 255, 56); color: black;\">this</span> <span style=\"background-color: rgb(79, 255, 79); color: black;\">pointless</span> <span style=\"background-color: rgb(97, 255, 97); color: black;\">film).</span> <span style=\"background-color: rgb(210, 255, 210); color: black;\"><br</span> <span style=\"background-color: rgb(194, 255, 194); color: black;\">/><br</span> <span style=\"background-color: rgb(101, 255, 101); color: black;\">/>One</span> <span style=\"background-color: rgb(153, 255, 153); color: black;\">might</span> <span style=\"background-color: rgb(159, 255, 159); color: black;\">better</span> <span style=\"background-color: rgb(83, 255, 83); color: black;\">spend</span> <span style=\"background-color: rgb(97, 255, 97); color: black;\">one's</span> <span style=\"background-color: rgb(130, 255, 130); color: black;\">time</span> <span style=\"background-color: rgb(163, 255, 163); color: black;\">staring</span> <span style=\"background-color: rgb(120, 255, 120); color: black;\">out</span> <span style=\"background-color: rgb(144, 255, 144); color: black;\">a</span> <span style=\"background-color: rgb(120, 255, 120); color: black;\">window</span> <span style=\"background-color: rgb(143, 255, 143); color: black;\">at</span> <span style=\"background-color: rgb(149, 255, 149); color: black;\">a</span> <span style=\"background-color: rgb(139, 255, 139); color: black;\">tree</span> <span style=\"background-color: rgb(196, 255, 196); color: black;\">growing.<br</span> <span style=\"background-color: rgb(209, 255, 209); color: black;\">/><br</span> <span style=\"background-color: rgb(110, 255, 110); color: black;\">/></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><|start_header_id|>assistant</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">NEGATIVE</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, 4)\n",
    "test_ids = gen_tokens[idx]\n",
    "test_mask = gen_attention_mask[idx]\n",
    "test_mask_prob = torch.sigmoid(mask_logits[idx])\n",
    "# inverse TODO\n",
    "# test_mask_prob = 1 - test_mask_prob\n",
    "test_context_mask = context_mask[idx]\n",
    "\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_ids)\n",
    "scores = test_mask_prob * test_context_mask\n",
    "def normalize_except_zeros(array):\n",
    "    # Create a mask to identify non-zero elements\n",
    "    mask = (array > 0)\n",
    "    \n",
    "    # Extract non-zero elements\n",
    "    non_zero_elements = array[mask]\n",
    "    \n",
    "    # Normalize non-zero elements\n",
    "    min_val = np.min(non_zero_elements)\n",
    "    max_val = np.max(non_zero_elements)\n",
    "\n",
    "    normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create a copy of the original array to preserve zero values\n",
    "    normalized_array = np.copy(array)\n",
    "    \n",
    "    # Assign normalized values back to the corresponding positions\n",
    "    normalized_array[mask] = normalized_non_zero_elements\n",
    "    \n",
    "    return normalized_array\n",
    "scores = normalize_except_zeros(scores.detach().cpu().numpy())\n",
    "\n",
    "# remove special tokens\n",
    "filtered_token_scores = [(token, score) for token, score in zip(test_tokens, scores) if token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# combine subwords\n",
    "merged_tokens_scores = []\n",
    "current_token = \"\"\n",
    "current_score = 0\n",
    "count = 0\n",
    "\n",
    "for token, score in filtered_token_scores:\n",
    "    if token.startswith(\"Ġ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "            # merged_tokens_scores.append((\" \", 0))  # 添加空格\n",
    "        current_token = token[1:] # remove the speical character\n",
    "        current_score = score\n",
    "        count = 1\n",
    "    elif token.endswith(\"Ċ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "        merged_tokens_scores.append((\"<br><br>\", 0))  # 添加换行符\n",
    "        current_token = \"\"\n",
    "        current_score = 0\n",
    "        count = 0\n",
    "    elif token.startswith(\"<\") and token.endswith(\">\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "            current_token = token\n",
    "            current_score = 0\n",
    "            count = 1\n",
    "    elif token in (',', '.', ':', '\"', \"'\", '?', '!', '-', ';', '(', ')', '[', ']', '{', '}', '<', '>', '/'):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "        current_token = token\n",
    "        current_score = score\n",
    "        count = 1\n",
    "    else:\n",
    "        current_token += token\n",
    "        current_score += score\n",
    "        count += 1\n",
    "    # print(token)\n",
    "\n",
    "if current_token:\n",
    "    merged_tokens_scores.append((current_token, current_score / count))\n",
    "\n",
    "\n",
    "# 根据分数高亮文本（示例中使用HTML标签）\n",
    "highlighted_text = \"\"\n",
    "for token, score in merged_tokens_scores:\n",
    "    # 动态设置背景颜色：score为0时为白色，score为1时为绿色\n",
    "    red = int((1 - score) * 255)\n",
    "    green = 255\n",
    "    blue = int((1 - score) * 255)\n",
    "    color = f'rgb({red}, {green}, {blue})'\n",
    "    highlighted_text += f'<span style=\"background-color: {color}; color: black;\">{token}</span> '\n",
    "\n",
    "# 打印高亮后的文本\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(highlighted_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4551, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_mask_prob * test_context_mask).sum(-1) / test_context_mask.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.4980, 0.4982, 0.4980, 0.4967, 0.4984, 0.4987, 0.4994,\n",
       "        0.4966, 0.4994, 0.4983, 0.4983, 0.4986, 0.4979, 0.4971, 0.5008, 0.4978,\n",
       "        0.4994, 0.4992, 0.5002, 0.4978, 0.4981, 0.4990, 0.4981, 0.4982, 0.4953,\n",
       "        0.4989, 0.4991, 0.4949, 0.4924, 0.4941, 0.4924, 0.4971, 0.4980, 0.4955,\n",
       "        0.4990, 0.4942, 0.4970, 0.4972, 0.4962, 0.4984, 0.4975, 0.4986, 0.4970,\n",
       "        0.4967, 0.4994, 0.4967, 0.4990, 0.4969, 0.4977, 0.4973, 0.4965, 0.4961,\n",
       "        0.4955, 0.4961, 0.4962, 0.4945, 0.4963, 0.4974, 0.4967, 0.5006, 0.4952,\n",
       "        0.4947, 0.4957, 0.4966, 0.4962, 0.4963, 0.4974, 0.4950, 0.4991, 0.4956,\n",
       "        0.4948, 0.4962, 0.4995, 0.4958, 0.4957, 0.4962, 0.4974, 0.4951, 0.4941,\n",
       "        0.4957, 0.4960, 0.4977, 0.4954, 0.4942, 0.4944, 0.4960, 0.4978, 0.4987,\n",
       "        0.4978, 0.4965, 0.4978, 0.4945, 0.4912, 0.4948, 0.4921, 0.4960, 0.4980,\n",
       "        0.4950, 0.4948, 0.4977, 0.4958, 0.4985, 0.4959, 0.4947, 0.4963, 0.4954,\n",
       "        0.4963, 0.4954, 0.4952, 0.4956, 0.4943, 0.4946, 0.4913, 0.4939, 0.4920,\n",
       "        0.4966, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask_prob * test_context_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "top_k_indices = np.argpartition(scores, -k)[-k:]\n",
    "top_k_values = scores[top_k_indices]\n",
    "print(top_k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('<br><br>', 0),\n",
       " ('You', 0.0),\n",
       " ('are', 0.0),\n",
       " ('a', 0.0),\n",
       " ('chatbot', 0.0),\n",
       " ('for', 0.0),\n",
       " ('answering', 0.0),\n",
       " ('questions', 0.0),\n",
       " ('.', 0.0),\n",
       " ('You', 0.0),\n",
       " ('can', 0.0),\n",
       " ('help', 0.0),\n",
       " ('users', 0.0),\n",
       " ('with', 0.0),\n",
       " ('their', 0.0),\n",
       " ('questions', 0.0),\n",
       " ('via', 0.0),\n",
       " ('concise', 0.0),\n",
       " ('responses', 0.0),\n",
       " ('.', 0.0),\n",
       " ('<|start_header_id|>user', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('<br><br>', 0),\n",
       " ('Question', 0.0),\n",
       " (':', 0.0),\n",
       " ('What', 0.0),\n",
       " ('is', 0.0),\n",
       " ('the', 0.0),\n",
       " ('Grotto', 0.0),\n",
       " ('at', 0.0),\n",
       " ('Notre', 0.0),\n",
       " ('Dame', 0.0),\n",
       " ('<br><br>', 0),\n",
       " ('Context', 0.8096184730529785),\n",
       " (':', 0.0),\n",
       " ('Architecturally', 0.8208637833595276),\n",
       " (',', 0.0),\n",
       " ('the', 0.31576451659202576),\n",
       " ('school', 0.776223361492157),\n",
       " ('has', 0.7286538481712341),\n",
       " ('a', 0.6366741061210632),\n",
       " ('Catholic', 0.5874115824699402),\n",
       " ('character', 0.9577630758285522),\n",
       " ('.', 0.0),\n",
       " ('Atop', 0.7504719495773315),\n",
       " ('the', 0.6626720428466797),\n",
       " ('Main', 0.734990656375885),\n",
       " (\"Building's\", 0.8112824559211731),\n",
       " ('gold', 0.48467954993247986),\n",
       " ('dome', 0.8122594356536865),\n",
       " ('is', 0.8033886551856995),\n",
       " ('a', 0.766498863697052),\n",
       " ('golden', 0.40439099073410034),\n",
       " ('statue', 0.9040688276290894),\n",
       " ('of', 0.6650682687759399),\n",
       " ('the', 0.6204079389572144),\n",
       " ('Virgin', 0.827494204044342),\n",
       " ('Mary', 0.9407193064689636),\n",
       " ('.', 0.0),\n",
       " ('Immediately', 0.8272622227668762),\n",
       " ('in', 0.8201749920845032),\n",
       " ('front', 0.8495352864265442),\n",
       " ('of', 0.8028140664100647),\n",
       " ('the', 0.731611967086792),\n",
       " ('Main', 0.7171692848205566),\n",
       " ('Building', 0.8530226945877075),\n",
       " ('and', 0.8262000679969788),\n",
       " ('facing', 0.4628849923610687),\n",
       " ('it', 0.782819390296936),\n",
       " (',', 0.0),\n",
       " ('is', 0.648202121257782),\n",
       " ('a', 0.7353948354721069),\n",
       " ('copper', 0.2720014154911041),\n",
       " ('statue', 0.795612633228302),\n",
       " ('of', 0.646257221698761),\n",
       " ('Christ', 0.815536618232727),\n",
       " ('with', 0.7038322687149048),\n",
       " ('arms', 0.7104554772377014),\n",
       " ('upraised', 0.7783668041229248),\n",
       " ('with', 0.7674903273582458),\n",
       " ('the', 0.6764821410179138),\n",
       " ('legend', 0.7157989740371704),\n",
       " ('\"Venite', 0.40165428320566815),\n",
       " ('Ad', 0.3667326867580414),\n",
       " ('Me', 0.9135758280754089),\n",
       " ('Omnes\".', 0.7748186588287354),\n",
       " ('Next', 0.7966005206108093),\n",
       " ('to', 0.7571422457695007),\n",
       " ('the', 0.6763516068458557),\n",
       " ('Main', 0.7154926657676697),\n",
       " ('Building', 0.8926114439964294),\n",
       " ('is', 0.7005822658538818),\n",
       " ('the', 0.7418494820594788),\n",
       " ('Basilica', 0.5231616497039795),\n",
       " ('of', 0.5437082648277283),\n",
       " ('the', 0.42031094431877136),\n",
       " ('Sacred', 0.7532814741134644),\n",
       " ('Heart', 0.9424141049385071),\n",
       " ('.', 0.0),\n",
       " ('Immediately', 0.8137239813804626),\n",
       " ('behind', 0.8342697024345398),\n",
       " ('the', 0.7185794711112976),\n",
       " ('basilica', 0.6875823736190796),\n",
       " ('is', 0.6961885690689087),\n",
       " ('the', 0.6898209452629089),\n",
       " ('Grotto', 0.7527201970418295),\n",
       " (',', 0.0),\n",
       " ('a', 0.6438319683074951),\n",
       " ('Marian', 0.6572904586791992),\n",
       " ('place', 0.8966969847679138),\n",
       " ('of', 0.6643106341362),\n",
       " ('prayer', 0.9257654547691345),\n",
       " ('and', 0.7432306408882141),\n",
       " ('reflection', 0.9892259836196899),\n",
       " ('.', 0.0),\n",
       " ('It', 0.9702427387237549),\n",
       " ('is', 0.9238350987434387),\n",
       " ('a', 0.7777513861656189),\n",
       " ('replica', 0.9852364659309387),\n",
       " ('of', 0.6425668001174927),\n",
       " ('the', 0.4883881211280823),\n",
       " ('grotto', 0.5806584358215332),\n",
       " ('at', 0.5494269728660583),\n",
       " ('Lourdes', 0.43491435050964355),\n",
       " (',', 0.0),\n",
       " ('France', 0.9393435716629028),\n",
       " ('where', 0.6636925339698792),\n",
       " ('the', 0.5315259099006653),\n",
       " ('Virgin', 0.8062670230865479),\n",
       " ('Mary', 0.8177117109298706),\n",
       " ('reputedly', 0.8159507513046265),\n",
       " ('appeared', 0.7476388812065125),\n",
       " ('to', 0.6793550848960876),\n",
       " ('Saint', 0.5598584413528442),\n",
       " ('Bernadette', 0.8013772964477539),\n",
       " ('Soubirous', 0.721934974193573),\n",
       " ('in', 0.8766824007034302),\n",
       " ('1858', 0.7111188570658366),\n",
       " ('.', 0.0),\n",
       " ('At', 0.6767231822013855),\n",
       " ('the', 0.6622623801231384),\n",
       " ('end', 0.9193253517150879),\n",
       " ('of', 0.8148242235183716),\n",
       " ('the', 0.7153512835502625),\n",
       " ('main', 0.7296199202537537),\n",
       " ('drive', 0.7661490440368652),\n",
       " ('(and', 0.41652488708496094),\n",
       " ('in', 0.7297921180725098),\n",
       " ('a', 0.725412905216217),\n",
       " ('direct', 0.7233719825744629),\n",
       " ('line', 0.6324996948242188),\n",
       " ('that', 0.7880215048789978),\n",
       " ('connects', 0.6456572413444519),\n",
       " ('through', 0.6712274551391602),\n",
       " ('3', 0.5849246978759766),\n",
       " ('statues', 0.5666012763977051),\n",
       " ('and', 0.784496009349823),\n",
       " ('the', 0.6135762929916382),\n",
       " ('Gold', 0.4635375440120697),\n",
       " ('Dome),', 0.7154427766799927),\n",
       " ('is', 0.6319015622138977),\n",
       " ('a', 0.6775225400924683),\n",
       " ('simple', 0.7047150135040283),\n",
       " (',', 0.0),\n",
       " ('modern', 0.7362485527992249),\n",
       " ('stone', 0.6558929681777954),\n",
       " ('statue', 0.8297418355941772),\n",
       " ('of', 0.7329968214035034),\n",
       " ('Mary', 0.8112589120864868),\n",
       " ('.', 0.0),\n",
       " ('<|start_header_id|>assistant', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('<br><br>', 0),\n",
       " ('The', 0.0),\n",
       " ('Grotto', 0.0),\n",
       " ('at', 0.0),\n",
       " ('Notre', 0.0),\n",
       " ('Dame', 0.0),\n",
       " ('is', 0.0),\n",
       " ('a', 0.0),\n",
       " ('Marian', 0.0),\n",
       " ('place', 0.0),\n",
       " ('of', 0.0),\n",
       " ('prayer', 0.0),\n",
       " ('and', 0.0),\n",
       " ('reflection', 0.0),\n",
       " (',', 0.0),\n",
       " ('located', 0.0),\n",
       " ('immediately', 0.0),\n",
       " ('behind', 0.0),\n",
       " ('the', 0.0),\n",
       " ('Basilica', 0.0),\n",
       " ('of', 0.0),\n",
       " ('the', 0.0),\n",
       " ('Sacred', 0.0),\n",
       " ('Heart', 0.0),\n",
       " ('.', 0.0),\n",
       " ('It', 0.0),\n",
       " ('is', 0.0),\n",
       " ('a', 0.0),\n",
       " ('replica', 0.0),\n",
       " ('of', 0.0),\n",
       " ('the', 0.0),\n",
       " ('grotto', 0.0),\n",
       " ('at', 0.0),\n",
       " ('Lourdes', 0.0),\n",
       " (',', 0.0),\n",
       " ('France', 0.0),\n",
       " (',', 0.0),\n",
       " ('where', 0.0),\n",
       " ('the', 0.0),\n",
       " ('Virgin', 0.0),\n",
       " ('Mary', 0.0),\n",
       " ('is', 0.0),\n",
       " ('said', 0.0),\n",
       " ('to', 0.0),\n",
       " ('have', 0.0),\n",
       " ('appeared', 0.0),\n",
       " ('to', 0.0),\n",
       " ('Saint', 0.0),\n",
       " ('Bernadette', 0.0),\n",
       " ('Soubirous', 0.0),\n",
       " ('in', 0.0),\n",
       " ('1858', 0.0),\n",
       " ('.', 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|start_header_id|>', 0.0),\n",
       " ('system', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('ĊĊ', 0.0),\n",
       " ('You', 0.0),\n",
       " ('Ġare', 0.0),\n",
       " ('Ġa', 0.0),\n",
       " ('Ġchat', 0.0),\n",
       " ('bot', 0.0),\n",
       " ('Ġfor', 0.0),\n",
       " ('Ġanswering', 0.0),\n",
       " ('Ġquestions', 0.0),\n",
       " ('.', 0.0),\n",
       " ('ĠYou', 0.0),\n",
       " ('Ġcan', 0.0),\n",
       " ('Ġhelp', 0.0),\n",
       " ('Ġusers', 0.0),\n",
       " ('Ġwith', 0.0),\n",
       " ('Ġtheir', 0.0),\n",
       " ('Ġquestions', 0.0),\n",
       " ('Ġvia', 0.0),\n",
       " ('Ġconcise', 0.0),\n",
       " ('Ġresponses', 0.0),\n",
       " ('.', 0.0),\n",
       " ('<|start_header_id|>', 0.0),\n",
       " ('user', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('ĊĊ', 0.0),\n",
       " ('Question', 0.0),\n",
       " (':', 0.0),\n",
       " ('ĠWhat', 0.0),\n",
       " ('Ġis', 0.0),\n",
       " ('Ġthe', 0.0),\n",
       " ('ĠG', 0.0),\n",
       " ('rot', 0.0),\n",
       " ('to', 0.0),\n",
       " ('Ġat', 0.0),\n",
       " ('ĠNotre', 0.0),\n",
       " ('ĠDame', 0.0),\n",
       " ('?Ċ', 0.0),\n",
       " ('Context', 0.8096185),\n",
       " (':', 0.8191038),\n",
       " ('ĠArchitect', 0.768306),\n",
       " ('urally', 0.87342155),\n",
       " (',', 0.7944344),\n",
       " ('Ġthe', 0.31576452),\n",
       " ('Ġschool', 0.77622336),\n",
       " ('Ġhas', 0.72865385),\n",
       " ('Ġa', 0.6366741),\n",
       " ('ĠCatholic', 0.5874116),\n",
       " ('Ġcharacter', 0.9577631),\n",
       " ('.', 0.82176286),\n",
       " ('ĠAt', 0.685507),\n",
       " ('op', 0.8154369),\n",
       " ('Ġthe', 0.66267204),\n",
       " ('ĠMain', 0.73499066),\n",
       " ('ĠBuilding', 0.936293),\n",
       " (\"'s\", 0.6862719),\n",
       " ('Ġgold', 0.48467955),\n",
       " ('Ġdome', 0.81225944),\n",
       " ('Ġis', 0.80338866),\n",
       " ('Ġa', 0.76649886),\n",
       " ('Ġgolden', 0.404391),\n",
       " ('Ġstatue', 0.9040688),\n",
       " ('Ġof', 0.66506827),\n",
       " ('Ġthe', 0.62040794),\n",
       " ('ĠVirgin', 0.8274942),\n",
       " ('ĠMary', 0.9407193),\n",
       " ('.', 0.77326703),\n",
       " ('ĠImmediately', 0.8272622),\n",
       " ('Ġin', 0.820175),\n",
       " ('Ġfront', 0.8495353),\n",
       " ('Ġof', 0.80281407),\n",
       " ('Ġthe', 0.73161197),\n",
       " ('ĠMain', 0.7171693),\n",
       " ('ĠBuilding', 0.8530227),\n",
       " ('Ġand', 0.82620007),\n",
       " ('Ġfacing', 0.462885),\n",
       " ('Ġit', 0.7828194),\n",
       " (',', 0.8238509),\n",
       " ('Ġis', 0.6482021),\n",
       " ('Ġa', 0.73539484),\n",
       " ('Ġcopper', 0.27200142),\n",
       " ('Ġstatue', 0.79561263),\n",
       " ('Ġof', 0.6462572),\n",
       " ('ĠChrist', 0.8155366),\n",
       " ('Ġwith', 0.70383227),\n",
       " ('Ġarms', 0.7104555),\n",
       " ('Ġup', 0.71329033),\n",
       " ('raised', 0.8434432),\n",
       " ('Ġwith', 0.7674903),\n",
       " ('Ġthe', 0.67648214),\n",
       " ('Ġlegend', 0.715799),\n",
       " ('Ġ\"', 0.0),\n",
       " ('Ven', 0.55799145),\n",
       " ('ite', 0.6469714),\n",
       " ('ĠAd', 0.3667327),\n",
       " ('ĠMe', 0.9135758),\n",
       " ('ĠOm', 0.794855),\n",
       " ('nes', 0.72477853),\n",
       " ('\".', 0.8048224),\n",
       " ('ĠNext', 0.7966005),\n",
       " ('Ġto', 0.75714225),\n",
       " ('Ġthe', 0.6763516),\n",
       " ('ĠMain', 0.71549267),\n",
       " ('ĠBuilding', 0.89261144),\n",
       " ('Ġis', 0.70058227),\n",
       " ('Ġthe', 0.7418495),\n",
       " ('ĠBasil', 0.3443182),\n",
       " ('ica', 0.70200515),\n",
       " ('Ġof', 0.54370826),\n",
       " ('Ġthe', 0.42031094),\n",
       " ('ĠSacred', 0.7532815),\n",
       " ('ĠHeart', 0.9424141),\n",
       " ('.', 0.77332866),\n",
       " ('ĠImmediately', 0.813724),\n",
       " ('Ġbehind', 0.8342697),\n",
       " ('Ġthe', 0.7185795),\n",
       " ('Ġbasil', 0.5686839),\n",
       " ('ica', 0.8064809),\n",
       " ('Ġis', 0.69618857),\n",
       " ('Ġthe', 0.68982095),\n",
       " ('ĠG', 0.72199255),\n",
       " ('rot', 0.6362554),\n",
       " ('to', 0.89991254),\n",
       " (',', 0.86717176),\n",
       " ('Ġa', 0.64383197),\n",
       " ('ĠMarian', 0.65729046),\n",
       " ('Ġplace', 0.896697),\n",
       " ('Ġof', 0.66431063),\n",
       " ('Ġprayer', 0.92576545),\n",
       " ('Ġand', 0.74323064),\n",
       " ('Ġreflection', 0.989226),\n",
       " ('.', 0.8336788),\n",
       " ('ĠIt', 0.97024274),\n",
       " ('Ġis', 0.9238351),\n",
       " ('Ġa', 0.7777514),\n",
       " ('Ġreplica', 0.98523647),\n",
       " ('Ġof', 0.6425668),\n",
       " ('Ġthe', 0.48838812),\n",
       " ('Ġg', 0.53955203),\n",
       " ('rot', 0.54832673),\n",
       " ('to', 0.65409666),\n",
       " ('Ġat', 0.549427),\n",
       " ('ĠL', 0.47655734),\n",
       " ('our', 0.07295393),\n",
       " ('des', 0.7552318),\n",
       " (',', 0.88339263),\n",
       " ('ĠFrance', 0.9393436),\n",
       " ('Ġwhere', 0.66369253),\n",
       " ('Ġthe', 0.5315259),\n",
       " ('ĠVirgin', 0.806267),\n",
       " ('ĠMary', 0.8177117),\n",
       " ('Ġreputed', 0.9135704),\n",
       " ('ly', 0.71833116),\n",
       " ('Ġappeared', 0.7476389),\n",
       " ('Ġto', 0.6793551),\n",
       " ('ĠSaint', 0.55985844),\n",
       " ('ĠBern', 0.75985754),\n",
       " ('ad', 0.8073709),\n",
       " ('ette', 0.8369034),\n",
       " ('ĠS', 0.6964496),\n",
       " ('oub', 0.63372135),\n",
       " ('ir', 0.65513164),\n",
       " ('ous', 0.90243745),\n",
       " ('Ġin', 0.8766824),\n",
       " ('Ġ', 0.54803854),\n",
       " ('185', 0.585318),\n",
       " ('8', 1.0),\n",
       " ('.', 0.8817069),\n",
       " ('ĠAt', 0.6767232),\n",
       " ('Ġthe', 0.6622624),\n",
       " ('Ġend', 0.91932535),\n",
       " ('Ġof', 0.8148242),\n",
       " ('Ġthe', 0.7153513),\n",
       " ('Ġmain', 0.7296199),\n",
       " ('Ġdrive', 0.76614904),\n",
       " ('Ġ(', 0.11101736),\n",
       " ('and', 0.7220324),\n",
       " ('Ġin', 0.7297921),\n",
       " ('Ġa', 0.7254129),\n",
       " ('Ġdirect', 0.723372),\n",
       " ('Ġline', 0.6324997),\n",
       " ('Ġthat', 0.7880215),\n",
       " ('Ġconnects', 0.64565724),\n",
       " ('Ġthrough', 0.67122746),\n",
       " ('Ġ', 0.6394981),\n",
       " ('3', 0.53035134),\n",
       " ('Ġstatues', 0.5666013),\n",
       " ('Ġand', 0.784496),\n",
       " ('Ġthe', 0.6135763),\n",
       " ('ĠGold', 0.46353754),\n",
       " ('ĠDome', 0.5117288),\n",
       " ('),', 0.9191568),\n",
       " ('Ġis', 0.63190156),\n",
       " ('Ġa', 0.67752254),\n",
       " ('Ġsimple', 0.704715),\n",
       " (',', 0.81526834),\n",
       " ('Ġmodern', 0.73624855),\n",
       " ('Ġstone', 0.65589297),\n",
       " ('Ġstatue', 0.82974184),\n",
       " ('Ġof', 0.7329968),\n",
       " ('ĠMary', 0.8112589),\n",
       " ('.', 0.8751),\n",
       " ('<|start_header_id|>', 0.0),\n",
       " ('assistant', 0.0),\n",
       " ('<|end_header_id|>', 0.0),\n",
       " ('ĊĊ', 0.0),\n",
       " ('The', 0.0),\n",
       " ('ĠG', 0.0),\n",
       " ('rot', 0.0),\n",
       " ('to', 0.0),\n",
       " ('Ġat', 0.0),\n",
       " ('ĠNotre', 0.0),\n",
       " ('ĠDame', 0.0),\n",
       " ('Ġis', 0.0),\n",
       " ('Ġa', 0.0),\n",
       " ('ĠMarian', 0.0),\n",
       " ('Ġplace', 0.0),\n",
       " ('Ġof', 0.0),\n",
       " ('Ġprayer', 0.0),\n",
       " ('Ġand', 0.0),\n",
       " ('Ġreflection', 0.0),\n",
       " (',', 0.0),\n",
       " ('Ġlocated', 0.0),\n",
       " ('Ġimmediately', 0.0),\n",
       " ('Ġbehind', 0.0),\n",
       " ('Ġthe', 0.0),\n",
       " ('ĠBasil', 0.0),\n",
       " ('ica', 0.0),\n",
       " ('Ġof', 0.0),\n",
       " ('Ġthe', 0.0),\n",
       " ('ĠSacred', 0.0),\n",
       " ('ĠHeart', 0.0),\n",
       " ('.', 0.0),\n",
       " ('ĠIt', 0.0),\n",
       " ('Ġis', 0.0),\n",
       " ('Ġa', 0.0),\n",
       " ('Ġreplica', 0.0),\n",
       " ('Ġof', 0.0),\n",
       " ('Ġthe', 0.0),\n",
       " ('Ġg', 0.0),\n",
       " ('rot', 0.0),\n",
       " ('to', 0.0),\n",
       " ('Ġat', 0.0),\n",
       " ('ĠL', 0.0),\n",
       " ('our', 0.0),\n",
       " ('des', 0.0),\n",
       " (',', 0.0),\n",
       " ('ĠFrance', 0.0),\n",
       " (',', 0.0),\n",
       " ('Ġwhere', 0.0),\n",
       " ('Ġthe', 0.0),\n",
       " ('ĠVirgin', 0.0),\n",
       " ('ĠMary', 0.0),\n",
       " ('Ġis', 0.0),\n",
       " ('Ġsaid', 0.0),\n",
       " ('Ġto', 0.0),\n",
       " ('Ġhave', 0.0),\n",
       " ('Ġappeared', 0.0),\n",
       " ('Ġto', 0.0),\n",
       " ('ĠSaint', 0.0),\n",
       " ('ĠBern', 0.0),\n",
       " ('ad', 0.0),\n",
       " ('ette', 0.0),\n",
       " ('ĠS', 0.0),\n",
       " ('oub', 0.0),\n",
       " ('ir', 0.0),\n",
       " ('ous', 0.0),\n",
       " ('Ġin', 0.0),\n",
       " ('Ġ', 0.0),\n",
       " ('185', 0.0),\n",
       " ('8', 0.0),\n",
       " ('.', 0.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_token_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
