{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='log/app.log',            # Specify the log file name\n",
    "    level=logging.DEBUG,           # Set the log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Set the log format\n",
    ")\n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/d/dpan/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from transformers import LlamaTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # device_map=device,\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.helper import LlmExpHelper\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "ds = load_dataset(\"rajpurkar/squad\")\n",
    "train_ds = ds['train']\n",
    "test_ds = ds['validation']\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stanfordnlp/sst2\")\n",
    "# train_ds = ds['train']\n",
    "llm_exp_helper = LlmExpHelper(tokenizer, 'squad')\n",
    "collate_fn = llm_exp_helper.get_collate_fun()\n",
    "\n",
    "# Define batch size here!\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next(iter(train_dataloader))\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in mask_gen_model.named_parameters():\n",
    "#     print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmexp.squad_model_lora import MaskGeneratingModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "mask_gen_model = MaskGeneratingModel(hidden_size=4096, mlp_hidden_dim=4096, mlp_bottleneck_dim=768, mlp_num_blocks=5)\n",
    "mask_gen_model.to(device)\n",
    "\n",
    "target_modules = []\n",
    "num_layers = 6  # BERT-base 有 12 层\n",
    "for i in range(num_layers):\n",
    "    target_modules.extend([\n",
    "        f\"explain_map.layer.{i}.attention.self.query\",\n",
    "        f\"explain_map.layer.{i}.attention.self.key\",\n",
    "        f\"explain_map.layer.{i}.attention.self.value\",\n",
    "        f\"explain_map.layer.{i}.attention.output.dense\",\n",
    "        f\"explain_map.layer.{i}.intermediate.dense\",\n",
    "        f\"explain_map.layer.{i}.output.dense\"\n",
    "    ])\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # 低秩矩阵的秩\n",
    "    lora_alpha=32,  # LoRA 的缩放因子\n",
    "    target_modules= target_modules,  # 目标模块\n",
    "    lora_dropout=0.1  # Dropout 概率\n",
    ")\n",
    "mask_gen_model = get_peft_model(mask_gen_model, lora_config)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5475 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Epoch 1, Step 1: Loss = 0.2222, Reward Loss = 0.2222, Mean Reward = 0.4186,advantage = 0.2186, Mask_loss = 0.0816 mask_mean = 0.4006:   0%|          | 1/5475 [00:03<5:55:26,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 11: Loss = 0.2542, Reward Loss = 0.2542, Mean Reward = 0.4989,advantage = 0.2989, Mask_loss = 0.1627 mask_mean = 0.5110:   0%|          | 11/5475 [00:41<5:31:32,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 21: Loss = 0.2855, Reward Loss = 0.2855, Mean Reward = 0.7857,advantage = 0.5857, Mask_loss = 0.4769 mask_mean = 0.7666:   0%|          | 21/5475 [01:17<5:02:57,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 31: Loss = 0.2497, Reward Loss = 0.2497, Mean Reward = 0.6538,advantage = 0.4538, Mask_loss = 0.3672 mask_mean = 0.7682:   1%|          | 31/5475 [01:54<5:50:41,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 41: Loss = 0.3030, Reward Loss = 0.3030, Mean Reward = 0.8009,advantage = 0.6009, Mask_loss = 0.4781 mask_mean = 0.7540:   1%|          | 41/5475 [02:40<6:49:24,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 51: Loss = 0.1965, Reward Loss = 0.1965, Mean Reward = 0.7954,advantage = 0.5954, Mask_loss = 0.5454 mask_mean = 0.8733:   1%|          | 51/5475 [03:22<5:38:37,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 61: Loss = 0.1330, Reward Loss = 0.1330, Mean Reward = 0.7148,advantage = 0.5148, Mask_loss = 0.4782 mask_mean = 0.9205:   1%|          | 61/5475 [04:02<5:02:47,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 71: Loss = 0.1086, Reward Loss = 0.1086, Mean Reward = 0.6651,advantage = 0.4651, Mask_loss = 0.4372 mask_mean = 0.9381:   1%|▏         | 71/5475 [04:38<5:01:55,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 81: Loss = 0.0923, Reward Loss = 0.0923, Mean Reward = 0.9195,advantage = 0.7195, Mask_loss = 0.6973 mask_mean = 0.9611:   1%|▏         | 81/5475 [05:18<6:05:50,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 91: Loss = 0.2027, Reward Loss = 0.2027, Mean Reward = 0.6752,advantage = 0.4752, Mask_loss = 0.3917 mask_mean = 0.8193:   2%|▏         | 91/5475 [05:56<4:58:33,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 101: Loss = 0.2621, Reward Loss = 0.2621, Mean Reward = 0.8450,advantage = 0.6450, Mask_loss = 0.5384 mask_mean = 0.8194:   2%|▏         | 100/5475 [06:37<6:27:38,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 101: Loss = 0.2621, Reward Loss = 0.2621, Mean Reward = 0.8450,advantage = 0.6450, Mask_loss = 0.5384 mask_mean = 0.8194:   2%|▏         | 101/5475 [06:38<7:03:43,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 111: Loss = 0.2585, Reward Loss = 0.2585, Mean Reward = 0.9728,advantage = 0.7728, Mask_loss = 0.6636 mask_mean = 0.8589:   2%|▏         | 111/5475 [07:19<5:30:19,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 121: Loss = 0.1474, Reward Loss = 0.1474, Mean Reward = 0.6939,advantage = 0.4939, Mask_loss = 0.4487 mask_mean = 0.8989:   2%|▏         | 121/5475 [07:55<5:24:23,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 131: Loss = 0.2213, Reward Loss = 0.2213, Mean Reward = 0.9666,advantage = 0.7666, Mask_loss = 0.6759 mask_mean = 0.8792:   2%|▏         | 131/5475 [08:35<5:58:21,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 141: Loss = 0.1671, Reward Loss = 0.1671, Mean Reward = 0.9696,advantage = 0.7696, Mask_loss = 0.7102 mask_mean = 0.9087:   3%|▎         | 141/5475 [09:15<6:11:35,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 151: Loss = 0.1998, Reward Loss = 0.1998, Mean Reward = 0.9256,advantage = 0.7256, Mask_loss = 0.6452 mask_mean = 0.8715:   3%|▎         | 151/5475 [09:50<5:48:58,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 161: Loss = 0.1586, Reward Loss = 0.1586, Mean Reward = 0.8768,advantage = 0.6768, Mask_loss = 0.6222 mask_mean = 0.9137:   3%|▎         | 161/5475 [10:33<5:45:55,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 171: Loss = 0.1777, Reward Loss = 0.1777, Mean Reward = 0.9971,advantage = 0.7971, Mask_loss = 0.7432 mask_mean = 0.9307:   3%|▎         | 171/5475 [11:13<6:49:09,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 181: Loss = 0.1006, Reward Loss = 0.1006, Mean Reward = 0.6332,advantage = 0.4332, Mask_loss = 0.4103 mask_mean = 0.9469:   3%|▎         | 181/5475 [11:54<6:08:19,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 191: Loss = 0.2437, Reward Loss = 0.2437, Mean Reward = 0.9993,advantage = 0.7993, Mask_loss = 0.7182 mask_mean = 0.8945:   3%|▎         | 191/5475 [12:38<6:34:17,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 201: Loss = 0.2452, Reward Loss = 0.2452, Mean Reward = 0.9499,advantage = 0.7499, Mask_loss = 0.6691 mask_mean = 0.8900:   4%|▎         | 200/5475 [13:20<6:24:15,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 201: Loss = 0.2452, Reward Loss = 0.2452, Mean Reward = 0.9499,advantage = 0.7499, Mask_loss = 0.6691 mask_mean = 0.8900:   4%|▎         | 201/5475 [13:21<7:17:40,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 211: Loss = 0.1484, Reward Loss = 0.1484, Mean Reward = 0.9934,advantage = 0.7934, Mask_loss = 0.7465 mask_mean = 0.9430:   4%|▍         | 211/5475 [14:03<6:43:17,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 221: Loss = 0.1224, Reward Loss = 0.1224, Mean Reward = 1.0030,advantage = 0.8030, Mask_loss = 0.7768 mask_mean = 0.9668:   4%|▍         | 221/5475 [14:44<6:20:56,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 231: Loss = 0.1111, Reward Loss = 0.1111, Mean Reward = 0.9820,advantage = 0.7820, Mask_loss = 0.7557 mask_mean = 0.9664:   4%|▍         | 231/5475 [15:23<5:36:32,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 234: Loss = 0.0982, Reward Loss = 0.0982, Mean Reward = 1.0322,advantage = 0.8322, Mask_loss = 0.8042 mask_mean = 0.9664:   4%|▍         | 234/5475 [15:39<5:50:35,  4.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m context_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# get generated texts\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m gen_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m gen_outputs\u001b[38;5;241m.\u001b[39msequences\n\u001b[1;32m     22\u001b[0m pad_length \u001b[38;5;241m=\u001b[39m gen_tokens\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:750\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    753\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mask_gen_model.train()\n",
    "for epoch in range(1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        # (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "        unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "        unpaded_token_mask[:, :-pad_length] = 1\n",
    "        gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "        # print(gen_tokens[0])\n",
    "        # print(gen_attention_mask[0])\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-pad_length] = 0 # TODO: 有问题. 有问题吗？\n",
    "\n",
    "        context_mask = F.pad(context_mask, (0, pad_length), mode='constant', value=0)\n",
    "\n",
    "        # Get the last hidden state for the prompt + response sequence\n",
    "        with torch.no_grad():\n",
    "            full_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden_state = full_outputs.hidden_states[-1]\n",
    "            last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        mask_gen_outputs = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits, response_mask, \n",
    "                                                                           num_samples=1)\n",
    "        loss, reward_loss, mask_loss, mask_mean, mean_reward = mask_gen_outputs['loss'], mask_gen_outputs['reward_loss'], mask_gen_outputs['mask_loss'], mask_gen_outputs['mask_mean'], mask_gen_outputs['mean_reward']\n",
    "        log = (f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss.item():.4f}, \" \n",
    "                             f\"Reward Loss = {reward_loss.item():.4f}, \"\n",
    "                             f\"Mean Reward = {mean_reward.mean().item():.4f},\"\n",
    "                             f\"advantage = {mask_gen_outputs['advantage'].item():.4f}, \"\n",
    "                             f\"Mask_loss = {mask_loss.item():.4f} \"\n",
    "                             f\"mask_mean = {mask_mean.item():.4f}\"\n",
    "\n",
    "        )\n",
    "        pbar.set_description(log)\n",
    "        logging.debug(log)\n",
    "    \n",
    "        # the parameters before updating\n",
    "        params_before = mask_gen_model.state_dict()\n",
    "\n",
    "        # Train the model (inner loop)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # the mask_prob after the updates\n",
    "        # with torch.no_grad():\n",
    "        #     mask_logits_after = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        #     mask_gen_outputs_after = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_after, response_mask, \n",
    "        #                                                                     num_samples=5)\n",
    "        #     loss_after, reward_loss_after, mask_loss_after, mask_mean_after, mean_reward_after = mask_gen_outputs_after['loss'], mask_gen_outputs_after['reward_loss'], mask_gen_outputs_after['mask_loss'], mask_gen_outputs_after['mask_mean'], mask_gen_outputs_after['mean_reward']\n",
    "        #     mask_prob_after = (torch.sigmoid(mask_logits_after) * context_mask).clone().detach()\n",
    "        #     mean_reward_after = mean_reward_after.clone().detach()\n",
    "\n",
    "        # # load the parameters before the updates\n",
    "        # mask_gen_model.load_state_dict(params_before)\n",
    "        # mask_logits_before = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        # mask_gen_outputs_before = mask_gen_model.loss_func(model, gen_tokens, gen_attention_mask, context_mask, mask_logits_before, response_mask, \n",
    "        #                                                                    num_samples=5)\n",
    "        # loss_before, reward_loss_before, mask_loss_before, mask_mean_before, mean_reward_before = mask_gen_outputs_before['loss'], mask_gen_outputs_before['reward_loss'], mask_gen_outputs_before['mask_loss'], mask_gen_outputs_before['mask_mean'], mask_gen_outputs_before['mean_reward']\n",
    "        # mask_prob_before = (torch.sigmoid(mask_logits_before) * context_mask)\n",
    "\n",
    "        # # calculate the ratio of the mask probabilities before and after the updates\n",
    "        # ratio = mask_prob_after / (mask_prob_before + 1e-6)\n",
    "\n",
    "        # # 定义PPO的损失函数，假设clip_param是你定义的剪切参数\n",
    "        # clip_param = 0.2\n",
    "        # advantage = (mean_reward_after - mean_reward_before).unsqueeze(-1)  # 计算优势函数（advantage），这是根据任务定义的\n",
    "        # surr1 = ratio * advantage\n",
    "        # surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantage\n",
    "        # ppo_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # # 更新模型参数\n",
    "        # optimizer.zero_grad()\n",
    "        # ppo_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print()\n",
    "        if idx % 100 == 0 and idx != 0:\n",
    "            torch.save(mask_gen_model.state_dict(), f'saved_model/mask_gen_model_lora_{epoch}_{idx}.pth') \n",
    "            print()\n",
    "            # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# mask_gen_model.load_state_dict(torch.load('saved_model/mask_gen_model_0_1600.pth',map_location=device))\n",
    "\n",
    "mask_gen_model.eval()\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(gen_tokens[idx])\n",
    "# texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I did not like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "# texts = \"I really love this film. The acting was great, and the story was amazing. I would recommend this movie to everyone.\"\n",
    "# # texts = \"I don't like this movie. The acting was terrible, and the story was boring. I would not recommend this movie to anyone.\"\n",
    "# messages_lambda = lambda texts: [\n",
    "#             {\"role\": \"system\", \"content\": \"Answer the question based on the context.\"},\n",
    "#             # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "#             {\"role\": \"user\", \"content\": texts},\n",
    "#         ]\n",
    "# messages = messages_lambda(texts)\n",
    "# messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "\n",
    "# # test_text = [{\"text\": texts, \"label\": None}]\n",
    "# test_text = [{\"sentence\": texts, \"label\": None}]\n",
    "# test_inputs = collate_fn(test_text).to(device)\n",
    "\n",
    "test_inputs = next(iter(test_dataloader)).to(device)\n",
    "# test_inputs = next(iter(train_dataloader)).to(device)\n",
    "\n",
    "# tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][idx])\n",
    "\n",
    "# generate the answer for the test inputs\n",
    "gen_outputs = model.generate(\n",
    "            input_ids=test_inputs['input_ids'],\n",
    "            attention_mask=test_inputs['attention_mask'],\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "input_ids = test_inputs['input_ids']\n",
    "attention_mask = test_inputs['attention_mask']\n",
    "gen_tokens = gen_outputs.sequences\n",
    "pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "# get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "context_mask = F.pad(test_inputs['context_mask'], (0, pad_length), mode='constant', value=0)\n",
    "# (gen_tokens != pad_token_id).long() is the tokens mask, 1 for real tokens and 0 for padding tokens\n",
    "unpaded_token_mask = (gen_tokens != pad_token_id).long()\n",
    "unpaded_token_mask[:, :-pad_length] = 1\n",
    "gen_attention_mask = gen_attention_mask * unpaded_token_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    # prompt_outputs = model(input_ids=test_inputs['input_ids'], attention_mask=test_inputs['attention_mask'], output_hidden_states=True, return_dict=True)\n",
    "    prompt_outputs = model(input_ids=gen_tokens, attention_mask=gen_attention_mask, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "    last_hidden_state = prompt_outputs.hidden_states[-1].float()\n",
    "    mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: rgb(255, 255, 255); color: black;\"><|start_header_id|>system<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">are</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">a</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">chatbot</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">for</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">answering</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions.</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">You</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">can</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">help</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">users</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">with</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">their</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">questions</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">via</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">concise</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">responses.<|start_header_id|>user<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Question:</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">What</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">in</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">front</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Notre</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Dame</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Main</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Building</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Context:</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">Architecturally,</span> <span style=\"background-color: rgb(19, 255, 19); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">school</span> <span style=\"background-color: rgb(6, 255, 6); color: black;\">has</span> <span style=\"background-color: rgb(5, 255, 5); color: black;\">a</span> <span style=\"background-color: rgb(4, 255, 4); color: black;\">Catholic</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">character.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Atop</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Main</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">Building's</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">gold</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">dome</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">a</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">golden</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">statue</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">of</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Virgin</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Mary.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Immediately</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">in</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">front</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">of</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">the</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">Main</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Building</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">and</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">facing</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">it,</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">a</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">copper</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">statue</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">of</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Christ</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">with</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">arms</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">upraised</span> <span style=\"background-color: rgb(9, 255, 9); color: black;\">with</span> <span style=\"background-color: rgb(9, 255, 9); color: black;\">the</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">legend</span> <span style=\"background-color: rgb(16, 255, 16); color: black;\">\"Venite</span> <span style=\"background-color: rgb(11, 255, 11); color: black;\">Ad</span> <span style=\"background-color: rgb(17, 255, 17); color: black;\">Me</span> <span style=\"background-color: rgb(8, 255, 8); color: black;\">Omnes\".</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Next</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">to</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">the</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">Main</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Building</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">the</span> <span style=\"background-color: rgb(4, 255, 4); color: black;\">Basilica</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">of</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Sacred</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Heart.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Immediately</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">behind</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">the</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">basilica</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">the</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">Grotto,</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">a</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">Marian</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">place</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">of</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">prayer</span> <span style=\"background-color: rgb(8, 255, 8); color: black;\">and</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">reflection.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">It</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">a</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">replica</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">of</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">grotto</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">at</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Lourdes,</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">France</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">where</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Virgin</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Mary</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">reputedly</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">appeared</span> <span style=\"background-color: rgb(5, 255, 5); color: black;\">to</span> <span style=\"background-color: rgb(3, 255, 3); color: black;\">Saint</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">Bernadette</span> <span style=\"background-color: rgb(8, 255, 8); color: black;\">Soubirous</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">in</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">1858.</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">At</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">end</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">of</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">main</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">drive</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">(and</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">in</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">a</span> <span style=\"background-color: rgb(2, 255, 2); color: black;\">direct</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">line</span> <span style=\"background-color: rgb(4, 255, 4); color: black;\">that</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">connects</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">through</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">3</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">statues</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">and</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">the</span> <span style=\"background-color: rgb(5, 255, 5); color: black;\">Gold</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">Dome),</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">is</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">a</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">simple,</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">modern</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">stone</span> <span style=\"background-color: rgb(1, 255, 1); color: black;\">statue</span> <span style=\"background-color: rgb(0, 255, 0); color: black;\">of</span> <span style=\"background-color: rgb(153, 255, 153); color: black;\">Mary.<|start_header_id|>assistant<|end_header_id|></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\"><br><br></span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">According</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">to</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">context,</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">copper</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">statue</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Christ</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">with</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">legend</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">\"Venite</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Ad</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Me</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Omnes\"</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">is</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">immediately</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">in</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">front</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">of</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">the</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Notre</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Dame</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Main</span> <span style=\"background-color: rgb(255, 255, 255); color: black;\">Building.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0, 4)\n",
    "test_ids = gen_tokens[idx]\n",
    "test_mask = gen_attention_mask[idx]\n",
    "test_mask_prob = torch.sigmoid(mask_logits[idx])\n",
    "# inverse TODO\n",
    "# test_mask_prob = 1 - test_mask_prob\n",
    "test_context_mask = context_mask[idx]\n",
    "\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(test_ids)\n",
    "scores = test_mask_prob * test_context_mask\n",
    "def normalize_except_zeros(array):\n",
    "    # Create a mask to identify non-zero elements\n",
    "    mask = array != 0\n",
    "    \n",
    "    # Extract non-zero elements\n",
    "    non_zero_elements = array[mask]\n",
    "    \n",
    "    # Normalize non-zero elements\n",
    "    min_val = np.min(non_zero_elements)\n",
    "    max_val = np.max(non_zero_elements)\n",
    "    normalized_non_zero_elements = (non_zero_elements - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create a copy of the original array to preserve zero values\n",
    "    normalized_array = np.copy(array)\n",
    "    \n",
    "    # Assign normalized values back to the corresponding positions\n",
    "    normalized_array[mask] = normalized_non_zero_elements\n",
    "    \n",
    "    return normalized_array\n",
    "# scores = normalize_except_zeros(scores.detach().cpu().numpy())\n",
    "\n",
    "# remove special tokens\n",
    "filtered_token_scores = [(token, score) for token, score in zip(test_tokens, scores) if token not in tokenizer.all_special_tokens]\n",
    "\n",
    "# combine subwords\n",
    "merged_tokens_scores = []\n",
    "current_token = \"\"\n",
    "current_score = 0\n",
    "count = 0\n",
    "\n",
    "for token, score in filtered_token_scores:\n",
    "    if token.startswith(\"Ġ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "            # merged_tokens_scores.append((\" \", 0))  # 添加空格\n",
    "        current_token = token[1:] # remove the speical character\n",
    "        current_score = score\n",
    "        count = 1\n",
    "    elif token.endswith(\"Ċ\"):\n",
    "        if current_token:\n",
    "            merged_tokens_scores.append((current_token, current_score / count))\n",
    "        merged_tokens_scores.append((\"<br><br>\", 0))  # 添加换行符\n",
    "        current_token = \"\"\n",
    "        current_score = 0\n",
    "        count = 0\n",
    "    else:\n",
    "        current_token += token\n",
    "        current_score += score\n",
    "        count += 1\n",
    "\n",
    "if current_token:\n",
    "    merged_tokens_scores.append((current_token, current_score / count))\n",
    "\n",
    "\n",
    "# 根据分数高亮文本（示例中使用HTML标签）\n",
    "highlighted_text = \"\"\n",
    "for token, score in merged_tokens_scores:\n",
    "    # 动态设置背景颜色：score为0时为白色，score为1时为绿色\n",
    "    red = int((1 - score) * 255)\n",
    "    green = 255\n",
    "    blue = int((1 - score) * 255)\n",
    "    color = f'rgb({red}, {green}, {blue})'\n",
    "    highlighted_text += f'<span style=\"background-color: {color}; color: black;\">{token}</span> '\n",
    "\n",
    "# 打印高亮后的文本\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(highlighted_text.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|start_header_id|>system<|end_header_id|>', tensor(0., device='cuda:1')),\n",
       " ('<br><br>', 0),\n",
       " ('You', tensor(0., device='cuda:1')),\n",
       " ('are', tensor(0., device='cuda:1')),\n",
       " ('a', tensor(0., device='cuda:1')),\n",
       " ('chatbot', tensor(0., device='cuda:1')),\n",
       " ('for', tensor(0., device='cuda:1')),\n",
       " ('answering', tensor(0., device='cuda:1')),\n",
       " ('questions.', tensor(0., device='cuda:1')),\n",
       " ('You', tensor(0., device='cuda:1')),\n",
       " ('can', tensor(0., device='cuda:1')),\n",
       " ('help', tensor(0., device='cuda:1')),\n",
       " ('users', tensor(0., device='cuda:1')),\n",
       " ('with', tensor(0., device='cuda:1')),\n",
       " ('their', tensor(0., device='cuda:1')),\n",
       " ('questions', tensor(0., device='cuda:1')),\n",
       " ('via', tensor(0., device='cuda:1')),\n",
       " ('concise', tensor(0., device='cuda:1')),\n",
       " ('responses.<|start_header_id|>user<|end_header_id|>',\n",
       "  tensor(0., device='cuda:1')),\n",
       " ('<br><br>', 0),\n",
       " ('Question:', tensor(0.7202, device='cuda:1')),\n",
       " ('What', tensor(0.8791, device='cuda:1')),\n",
       " ('is', tensor(0.2633, device='cuda:1')),\n",
       " ('in', tensor(0.2088, device='cuda:1')),\n",
       " ('front', tensor(0.5273, device='cuda:1')),\n",
       " ('of', tensor(0.5978, device='cuda:1')),\n",
       " ('the', tensor(0.6007, device='cuda:1')),\n",
       " ('Notre', tensor(0.3078, device='cuda:1')),\n",
       " ('Dame', tensor(0.7168, device='cuda:1')),\n",
       " ('Main', tensor(0.6631, device='cuda:1')),\n",
       " ('Building', tensor(0.9654, device='cuda:1')),\n",
       " ('<br><br>', 0),\n",
       " ('Context:', tensor(0.4635, device='cuda:1')),\n",
       " ('Architecturally,', tensor(0.5434, device='cuda:1')),\n",
       " ('the', tensor(0.1228, device='cuda:1')),\n",
       " ('school', tensor(0.6450, device='cuda:1')),\n",
       " ('has', tensor(0.7905, device='cuda:1')),\n",
       " ('a', tensor(0.6916, device='cuda:1')),\n",
       " ('Catholic', tensor(0.5796, device='cuda:1')),\n",
       " ('character.', tensor(0.7968, device='cuda:1')),\n",
       " ('Atop', tensor(0.8836, device='cuda:1')),\n",
       " ('the', tensor(0.2282, device='cuda:1')),\n",
       " ('Main', tensor(0.6679, device='cuda:1')),\n",
       " (\"Building's\", tensor(0.5156, device='cuda:1')),\n",
       " ('gold', tensor(0.1174, device='cuda:1')),\n",
       " ('dome', tensor(0.9640, device='cuda:1')),\n",
       " ('is', tensor(0.3918, device='cuda:1')),\n",
       " ('a', tensor(0.1890, device='cuda:1')),\n",
       " ('golden', tensor(0.1478, device='cuda:1')),\n",
       " ('statue', tensor(0.9710, device='cuda:1')),\n",
       " ('of', tensor(0.8457, device='cuda:1')),\n",
       " ('the', tensor(0.6111, device='cuda:1')),\n",
       " ('Virgin', tensor(0.7331, device='cuda:1')),\n",
       " ('Mary.', tensor(0.9376, device='cuda:1')),\n",
       " ('Immediately', tensor(0.9952, device='cuda:1')),\n",
       " ('in', tensor(0.9530, device='cuda:1')),\n",
       " ('front', tensor(0.9745, device='cuda:1')),\n",
       " ('of', tensor(0.8712, device='cuda:1')),\n",
       " ('the', tensor(0.8065, device='cuda:1')),\n",
       " ('Main', tensor(0.7482, device='cuda:1')),\n",
       " ('Building', tensor(0.9850, device='cuda:1')),\n",
       " ('and', tensor(0.9824, device='cuda:1')),\n",
       " ('facing', tensor(0.9047, device='cuda:1')),\n",
       " ('it,', tensor(0.9588, device='cuda:1')),\n",
       " ('is', tensor(0.9088, device='cuda:1')),\n",
       " ('a', tensor(0.5586, device='cuda:1')),\n",
       " ('copper', tensor(0.1637, device='cuda:1')),\n",
       " ('statue', tensor(0.9722, device='cuda:1')),\n",
       " ('of', tensor(0.5613, device='cuda:1')),\n",
       " ('Christ', tensor(0.9905, device='cuda:1')),\n",
       " ('with', tensor(0.4444, device='cuda:1')),\n",
       " ('arms', tensor(0.5998, device='cuda:1')),\n",
       " ('upraised', tensor(0.9579, device='cuda:1')),\n",
       " ('with', tensor(0.6696, device='cuda:1')),\n",
       " ('the', tensor(0.5214, device='cuda:1')),\n",
       " ('legend', tensor(0.9929, device='cuda:1')),\n",
       " ('\"Venite', tensor(0.7485, device='cuda:1')),\n",
       " ('Ad', tensor(0.9349, device='cuda:1')),\n",
       " ('Me', tensor(0.8834, device='cuda:1')),\n",
       " ('Omnes\".', tensor(0.9227, device='cuda:1')),\n",
       " ('Next', tensor(0.8923, device='cuda:1')),\n",
       " ('to', tensor(0.9241, device='cuda:1')),\n",
       " ('the', tensor(0.7473, device='cuda:1')),\n",
       " ('Main', tensor(0.8781, device='cuda:1')),\n",
       " ('Building', tensor(0.9919, device='cuda:1')),\n",
       " ('is', tensor(0.8601, device='cuda:1')),\n",
       " ('the', tensor(0.2279, device='cuda:1')),\n",
       " ('Basilica', tensor(0.9211, device='cuda:1')),\n",
       " ('of', tensor(0.7035, device='cuda:1')),\n",
       " ('the', tensor(0.8999, device='cuda:1')),\n",
       " ('Sacred', tensor(0.9058, device='cuda:1')),\n",
       " ('Heart.', tensor(0.9925, device='cuda:1')),\n",
       " ('Immediately', tensor(0.9970, device='cuda:1')),\n",
       " ('behind', tensor(0.9612, device='cuda:1')),\n",
       " ('the', tensor(0.9616, device='cuda:1')),\n",
       " ('basilica', tensor(0.7100, device='cuda:1')),\n",
       " ('is', tensor(0.8746, device='cuda:1')),\n",
       " ('the', tensor(0.3504, device='cuda:1')),\n",
       " ('Grotto,', tensor(0.9314, device='cuda:1')),\n",
       " ('a', tensor(0.9680, device='cuda:1')),\n",
       " ('Marian', tensor(0.9662, device='cuda:1')),\n",
       " ('place', tensor(0.9941, device='cuda:1')),\n",
       " ('of', tensor(0.8735, device='cuda:1')),\n",
       " ('prayer', tensor(0.9962, device='cuda:1')),\n",
       " ('and', tensor(0.9374, device='cuda:1')),\n",
       " ('reflection.', tensor(0.9893, device='cuda:1')),\n",
       " ('It', tensor(0.9724, device='cuda:1')),\n",
       " ('is', tensor(0.9822, device='cuda:1')),\n",
       " ('a', tensor(0.9466, device='cuda:1')),\n",
       " ('replica', tensor(0.9420, device='cuda:1')),\n",
       " ('of', tensor(0.6442, device='cuda:1')),\n",
       " ('the', tensor(0.6639, device='cuda:1')),\n",
       " ('grotto', tensor(0.8211, device='cuda:1')),\n",
       " ('at', tensor(0.7835, device='cuda:1')),\n",
       " ('Lourdes,', tensor(0.8785, device='cuda:1')),\n",
       " ('France', tensor(0.9942, device='cuda:1')),\n",
       " ('where', tensor(0.8816, device='cuda:1')),\n",
       " ('the', tensor(0.7580, device='cuda:1')),\n",
       " ('Virgin', tensor(0.9070, device='cuda:1')),\n",
       " ('Mary', tensor(0.9845, device='cuda:1')),\n",
       " ('reputedly', tensor(0.9555, device='cuda:1')),\n",
       " ('appeared', tensor(0.8482, device='cuda:1')),\n",
       " ('to', tensor(0.7093, device='cuda:1')),\n",
       " ('Saint', tensor(0.5136, device='cuda:1')),\n",
       " ('Bernadette', tensor(0.9824, device='cuda:1')),\n",
       " ('Soubirous', tensor(0.7560, device='cuda:1')),\n",
       " ('in', tensor(0.1941, device='cuda:1')),\n",
       " ('1858.', tensor(0.8059, device='cuda:1')),\n",
       " ('At', tensor(0.9907, device='cuda:1')),\n",
       " ('the', tensor(0.6834, device='cuda:1')),\n",
       " ('end', tensor(0.9912, device='cuda:1')),\n",
       " ('of', tensor(0.9530, device='cuda:1')),\n",
       " ('the', tensor(0.9823, device='cuda:1')),\n",
       " ('main', tensor(0.9857, device='cuda:1')),\n",
       " ('drive', tensor(0.9996, device='cuda:1')),\n",
       " ('(and', tensor(0.9867, device='cuda:1')),\n",
       " ('in', tensor(0.6803, device='cuda:1')),\n",
       " ('a', tensor(0.9077, device='cuda:1')),\n",
       " ('direct', tensor(0.5428, device='cuda:1')),\n",
       " ('line', tensor(0.9760, device='cuda:1')),\n",
       " ('that', tensor(0.7639, device='cuda:1')),\n",
       " ('connects', tensor(0.9222, device='cuda:1')),\n",
       " ('through', tensor(0.9465, device='cuda:1')),\n",
       " ('3', tensor(0.9847, device='cuda:1')),\n",
       " ('statues', tensor(0.9654, device='cuda:1')),\n",
       " ('and', tensor(0.9909, device='cuda:1')),\n",
       " ('the', tensor(0.8592, device='cuda:1')),\n",
       " ('Gold', tensor(0.2124, device='cuda:1')),\n",
       " ('Dome),', tensor(0.9936, device='cuda:1')),\n",
       " ('is', tensor(0.8719, device='cuda:1')),\n",
       " ('a', tensor(0.6079, device='cuda:1')),\n",
       " ('simple,', tensor(0.6053, device='cuda:1')),\n",
       " ('modern', tensor(0.8033, device='cuda:1')),\n",
       " ('stone', tensor(0.8874, device='cuda:1')),\n",
       " ('statue', tensor(0.9706, device='cuda:1')),\n",
       " ('of', tensor(0.8189, device='cuda:1')),\n",
       " ('Mary.<|start_header_id|>assistant<|end_header_id|>',\n",
       "  tensor(0.3917, device='cuda:1')),\n",
       " ('<br><br>', 0),\n",
       " ('According', tensor(0., device='cuda:1')),\n",
       " ('to', tensor(0., device='cuda:1')),\n",
       " ('the', tensor(0., device='cuda:1')),\n",
       " ('context,', tensor(0., device='cuda:1')),\n",
       " ('in', tensor(0., device='cuda:1')),\n",
       " ('front', tensor(0., device='cuda:1')),\n",
       " ('of', tensor(0., device='cuda:1')),\n",
       " ('the', tensor(0., device='cuda:1')),\n",
       " ('Notre', tensor(0., device='cuda:1')),\n",
       " ('Dame', tensor(0., device='cuda:1')),\n",
       " ('Main', tensor(0., device='cuda:1')),\n",
       " ('Building', tensor(0., device='cuda:1')),\n",
       " ('is', tensor(0., device='cuda:1')),\n",
       " ('a', tensor(0., device='cuda:1')),\n",
       " ('copper', tensor(0., device='cuda:1')),\n",
       " ('statue', tensor(0., device='cuda:1')),\n",
       " ('of', tensor(0., device='cuda:1')),\n",
       " ('Christ', tensor(0., device='cuda:1')),\n",
       " ('with', tensor(0., device='cuda:1')),\n",
       " ('arms', tensor(0., device='cuda:1')),\n",
       " ('upraised', tensor(0., device='cuda:1')),\n",
       " ('and', tensor(0., device='cuda:1')),\n",
       " ('the', tensor(0., device='cuda:1')),\n",
       " ('legend', tensor(0., device='cuda:1')),\n",
       " ('\"Venite', tensor(0., device='cuda:1')),\n",
       " ('Ad', tensor(0., device='cuda:1')),\n",
       " ('Me', tensor(0., device='cuda:1')),\n",
       " ('Omnes\".', tensor(0., device='cuda:1'))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_tokens_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2052, 0.0336, 0.0175, 0.0591, 0.0214, 0.0514,\n",
       "        0.0453, 0.1790, 0.3275, 0.0885, 0.4151, 0.2449, 0.7812, 0.2025, 0.1918,\n",
       "        0.1083, 0.4665, 0.1082, 0.5029, 0.7354, 0.1680, 0.1983, 0.0222, 0.1115,\n",
       "        0.2571, 0.0972, 0.0252, 0.3280, 0.3500, 0.7719, 0.1022, 0.4868, 0.6470,\n",
       "        0.4235, 0.3930, 0.1858, 0.3415, 0.2573, 0.2125, 0.4739, 0.7803, 0.9506,\n",
       "        0.1826, 0.5753, 0.7123, 0.3407, 0.8478, 0.1610, 0.5789, 0.7452, 0.4866,\n",
       "        0.3256, 0.4797, 0.1182, 0.1025, 0.0717, 0.3000, 0.4712, 0.3427, 0.5447,\n",
       "        0.0234, 0.9369, 0.0532, 0.6967, 0.0184, 0.0097, 0.3318, 0.9282, 0.5267,\n",
       "        0.8113, 0.4950, 0.2794, 0.3564, 0.3344, 0.3029, 0.6510, 0.1335, 0.3388,\n",
       "        0.2056, 0.0442, 0.2567, 0.8161, 0.3769, 0.0220, 0.0137, 0.0120, 0.0532,\n",
       "        0.0799, 0.0381, 0.0894, 0.0241, 0.2642, 0.1161, 0.1233, 0.6457, 0.5571,\n",
       "        0.0116, 0.0060, 0.2440, 0.0268, 0.0458, 0.0012, 0.0099, 0.0091, 0.2315,\n",
       "        0.1482, 0.1388, 0.0862, 0.0404, 0.0597, 0.3075, 0.3525, 0.2076, 0.6788,\n",
       "        0.5865, 0.2197, 0.0196, 0.2495, 0.0269, 0.0028, 0.0252, 0.0193, 0.0529,\n",
       "        0.3263, 0.2007, 0.0757, 0.0671, 0.2068, 0.0018, 0.3846, 0.8290, 0.0470,\n",
       "        0.8867, 0.4626, 0.2914, 0.1257, 0.1422, 0.3466, 0.8361, 0.6511, 0.1813,\n",
       "        0.0261, 0.3762, 0.7019, 0.4439, 0.8600, 0.9006, 0.1634, 0.8193, 0.8355,\n",
       "        0.9367, 0.2113, 0.8023, 0.4347, 0.8024, 0.5939, 0.0793, 0.6613, 0.5426,\n",
       "        0.4776, 0.3045, 0.6982, 0.1078, 0.6654, 0.8120, 0.2537, 0.0051, 0.0687,\n",
       "        0.0279, 0.0381, 0.1286, 0.0195, 0.0208, 0.0551, 0.0503, 0.0352, 0.0276,\n",
       "        0.0229, 0.9374, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask_prob * test_context_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.63934755, 0.11451219, 0.1225187 , 0.43457144, 0.21965683,\n",
       "       0.28944358, 0.08941142, 0.10701324, 0.09935488, 0.05571543,\n",
       "       0.03000107, 0.07527877, 0.15953447, 0.22993316, 0.15300442,\n",
       "       0.3738694 , 0.22266376, 0.29959634, 0.4525226 , 0.11873144,\n",
       "       0.2386478 , 0.20134698, 0.11468548, 0.33462942, 0.12546155,\n",
       "       0.29114434, 0.3496375 , 0.27918556, 0.8707584 , 0.42532292,\n",
       "       0.18798777, 0.88152486, 0.7778979 , 0.38002315, 0.0177369 ,\n",
       "       0.34096608, 0.19461636, 0.55110186, 0.40781736, 0.6573673 ,\n",
       "       0.56073976, 0.7197896 , 0.5075476 , 0.49354845, 0.57938504,\n",
       "       0.2639287 , 0.53644824, 0.74284345, 0.43071514, 0.17025682,\n",
       "       0.2519394 , 0.24771284, 0.76228124, 0.03937887, 0.31321433,\n",
       "       0.3951155 , 0.12060348, 0.8391926 , 0.19060197, 0.14503507,\n",
       "       0.38200086, 0.09805975, 0.53167886, 0.45860618, 0.34170774,\n",
       "       0.38000423, 0.49717534, 0.12770228, 0.54148155, 0.88480836,\n",
       "       0.5025979 , 0.2180987 , 0.44277272, 0.7061062 , 0.6001298 ,\n",
       "       0.4190885 , 0.15418284, 0.01286074, 0.03205981, 0.7166164 ,\n",
       "       0.42272195, 0.1009967 , 0.5880632 , 0.06157847, 0.03148317,\n",
       "       0.01150252, 0.14847231, 0.40222263, 0.45739612, 0.29020867,\n",
       "       0.20442684, 0.01632834, 0.00439288, 0.22261323, 0.10954274,\n",
       "       0.735632  , 0.250885  , 0.23412147, 0.52762663, 0.6082385 ,\n",
       "       0.70563006, 0.18747613, 0.03181224, 0.1954265 , 0.46577016,\n",
       "       0.08638231, 0.587938  , 0.17073946, 0.19899334, 0.44022155,\n",
       "       0.28126696, 0.18912274, 0.606753  , 0.2354937 , 0.30333954,\n",
       "       0.9088377 , 0.760515  , 0.00247158, 0.65670216, 0.37467483,\n",
       "       0.8893074 , 0.49679863, 0.8678529 , 0.64698756, 0.921829  ,\n",
       "       0.32185513, 0.11627685, 0.19978185, 0.5822873 , 0.46432415,\n",
       "       0.27071175, 0.26083463, 0.35565257, 0.11644382, 0.12587062,\n",
       "       0.02788724, 0.04585992, 0.23267932, 0.52556354, 0.12978171,\n",
       "       0.05500807, 0.86675435, 0.59887177, 0.13369308, 0.37950924,\n",
       "       0.836148  , 0.1870308 , 0.6357755 , 0.18462983, 0.05141451,\n",
       "       0.2866516 , 0.3967239 , 0.9536885 , 0.36054942, 0.44063833,\n",
       "       0.2609197 , 0.61237025, 0.15894488, 0.21989977, 0.69814706,\n",
       "       0.37813613, 0.8174311 , 0.8459768 , 0.77438444, 0.1087799 ,\n",
       "       0.03142452, 0.8144604 , 0.42925656, 0.87952363, 0.12044684,\n",
       "       0.32651398, 0.3073709 , 0.15482455, 0.03003251, 0.5409264 ,\n",
       "       0.06059632, 0.23830052, 0.28520328, 0.5085325 , 0.29540935,\n",
       "       0.17819822, 0.05908529, 0.49207908, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1241, 0.2500, 0.7787, 0.0909, 0.1908, 0.1361, 0.3316, 0.5104, 0.2696,\n",
       "        0.1001, 0.5931, 0.0812, 0.4139, 0.3334, 0.4853, 0.2987],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob = torch.sigmoid(mask_logits)\n",
    "(mask_prob * context_mask).sum(-1) / context_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9998e-01, 1.3412e-02, 2.1168e-05, 7.2758e-04, 9.5831e-01, 2.8712e-06,\n",
       "         1.9530e-03, 6.8251e-04, 9.6814e-01, 7.6582e-04, 2.2164e-06, 1.7009e-02,\n",
       "         2.4596e-09, 1.4519e-04, 3.0463e-06, 7.9092e-04, 4.0336e-06, 5.2874e-05,\n",
       "         1.4755e-07, 5.3417e-09, 1.0859e-07, 8.2496e-07, 1.0534e-05, 2.1604e-06,\n",
       "         6.6980e-10, 1.8266e-01, 5.4030e-07, 4.0081e-02, 3.5185e-04, 3.1933e-01,\n",
       "         3.2836e-09, 1.5197e-06, 8.2054e-01, 8.0839e-01, 5.4567e-01, 4.8829e-01,\n",
       "         1.0000e+00, 4.3639e-05, 3.5097e-07, 3.8482e-09, 1.9672e-04, 1.1242e-07,\n",
       "         2.7982e-13, 5.4683e-09, 4.5730e-01, 9.5505e-01, 6.7815e-04, 7.4368e-02,\n",
       "         9.9924e-01, 6.6332e-03, 1.0508e-08, 1.8774e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,   2675,    527,    264,   6369,\n",
       "          6465,    369,  27065,   6492,     13,   1472,    649,   1520,   3932,\n",
       "           449,    872,   4860,   4669,  64694,  14847,    315,  27592,  45450,\n",
       "            11,    477,  85165,  24093,     13, 128009, 128006,    882, 128007,\n",
       "           271,   2028,   5818,    574,    279,   1888,   5818,    358,    617,\n",
       "          3596,   3970,      0,   1063,  16451,   1051,  27873,     11,    719,\n",
       "         15718,    574,   2294,     13, 128009, 128006,  78191, 128007,    271],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs['input_ids'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ĊĊ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(271)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      " 0.     0.     0.     0.     0.     0.     0.     0.6432 0.8566 0.5179\n",
      " 0.2417 0.     0.1211 0.3355 0.618  0.5345 0.1401 0.3401 0.4729 0.3531\n",
      " 0.661  0.7049 0.0297 0.1724 0.9905 1.     0.1606 0.1107 0.2363 0.2891\n",
      " 0.116  0.0777 0.     0.     0.     0.     0.     0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.8527, 0.1770, 0.1719, 0.1660, 0.1613, 0.1549, 0.1581, 0.1622, 0.1666,\n",
       "        0.1635, 0.1568, 0.1589, 0.1618, 0.1634, 0.1668, 0.1647, 0.1546, 0.1576,\n",
       "        0.1788, 0.1779, 0.1592, 0.1581, 0.1607, 0.1619, 0.1586, 0.1583, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sigmoid(mask_logits) * context_mask)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4822, 0.4821, 0.4816,  ..., 0.4920, 0.4874, 0.4901],\n",
       "        [0.4843, 0.4851, 0.4855,  ..., 0.4941, 0.4861, 0.4891],\n",
       "        [0.4785, 0.4805, 0.4805,  ..., 0.4918, 0.4823, 0.4864],\n",
       "        ...,\n",
       "        [0.4753, 0.4758, 0.4761,  ..., 0.4847, 0.4761, 0.4802],\n",
       "        [0.4876, 0.4883, 0.4882,  ..., 0.4887, 0.4880, 0.4943],\n",
       "        [0.4843, 0.4851, 0.4852,  ..., 0.4946, 0.4853, 0.4947]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(mask_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskGeneratingModel(\n",
       "  (explain_map): MLP(\n",
       "    (input_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (attention_layers): ModuleList(\n",
       "      (0-1): 2 x MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tokens[35])\n",
    "print(expl[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = \"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"\n",
    "# texts = \"I really didn't like this movie. Some of the actors were good, but overall the movie was boring.\"\n",
    "# texts = \"I hate that I love you.\"\n",
    "# texts = \"I don't like this movie.\"\n",
    "# texts = \"I really love this film.\"\n",
    "messages_lambda = lambda texts: [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, or NEGATIVE.\"},\n",
    "    # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": texts},\n",
    "]\n",
    "messages = messages_lambda(texts)\n",
    "messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "print(messages_with_template_applied)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
