{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "# Load the environment configuration JSON data\n",
    "json_path = 'env_config.json'\n",
    "with open(json_path, 'r') as file:\n",
    "    env_config = json.load(file)\n",
    "\n",
    "hf_home = env_config['HF_HOME']\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ['HF_HOME'] = hf_home\n",
    "# Set the access token to huggingface hub\n",
    "access_token = env_config['access_token']\n",
    "os.environ['HUGGINGFACE_HUB_TOKEN'] = access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Llama model and tokenizers\n",
    "also define device via accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/crc/c/conda/23.5.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.80s/it]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from transformers import LlamaTokenizerFast\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B\"  # non-instruct version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=access_token,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_id, token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape torch.Size([1, 51])\n",
      "input_ids tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,   6369,\n",
      "           6465,    369,   3288,   3509,   6492,     13,   1472,    649,   1520,\n",
      "           3932,    449,    872,   4860,   4669,  64694,  14847,    315,  27592,\n",
      "          45450,     11,  85165,  24093,   2794,   8014,   1406,  51785,     13,\n",
      "         128009, 128006,    882, 128007,    271,     40,  12491,    420,   5818,\n",
      "             13, 128009, 128006,  78191, 128007,    271]], device='cuda:0')\n",
      "input tokens\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate this movie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text = r\"\"\"I love this movie. The sentiment of this reivew is POSITIVE.\n",
    "# I found this movie interesting. The sentiment of this reivew is POSITIVE.\n",
    "# I found this movie boring. The sentiment of this reivew is NEGATIVE.\n",
    "# I hate this movie. The sentiment of this review is\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I hate this movie.\"},\n",
    "]\n",
    "\n",
    "messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "inputs = tokenizer.encode_plus(\n",
    "            messages_with_template_applied,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "print(\"input_ids.shape\", inputs.input_ids.shape)\n",
    "print(\"input_ids\", inputs.input_ids)\n",
    "print(\"input tokens\\n\", tokenizer.decode(inputs.input_ids[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the generated texts and the logits for each generated token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full texts:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate this movie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "NEGATIVE<|eot_id|>\n",
      "Generated response: NEGATIVE<|eot_id|>\n",
      "Generated token logits\n",
      " (tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# outputs[0]\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "print(\"Full texts:\\n\", generated_text)\n",
    "input_length = len(messages_with_template_applied)\n",
    "print(\"Generated response:\", generated_text[input_length:])\n",
    "print(\"Generated token logits\\n\", outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([outputs.scores[2].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = generated_text[input_length:]\n",
    "# TODO: add masking process\n",
    "masked_inputs_concat_response = messages_with_template_applied + generated_response\n",
    "masked_inputs = tokenizer.encode_plus(\n",
    "                    masked_inputs_concat_response,\n",
    "                    add_special_tokens=False,\n",
    "                    return_tensors=\"pt\",\n",
    "                    ).to(device)\n",
    "\n",
    "# Get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**masked_inputs, output_hidden_states=True, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state.shape torch.Size([1, 54, 4096])\n",
      "logits.shape torch.Size([1, 54, 128256])\n"
     ]
    }
   ],
   "source": [
    "print(\"last_hidden_state.shape\", last_hidden_state.shape)\n",
    "print(\"logits.shape\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from models import MLP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def similarity_measure(logits, labels, attention_mask):\n",
    "    \"\"\" \n",
    "    args:\n",
    "        logis: torch.Tensor, shape (batch_size, seq_len, vocab_size)\n",
    "        labels: torch.Tensor, shape (batch_size, seq_len)\n",
    "        attention_mask: torch.Tensor, shape (batch_size, seq_len) the original input text is masked with 0\n",
    "\n",
    "    return:\n",
    "        mean_log_probs: torch.Tensor, shape (batch_size,)\n",
    "    \"\"\"\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    # correct_log_probs = log_probs[range(log_probs.shape[0]), labels]\n",
    "\n",
    "    # Flatten the tensors to simplify indexing\n",
    "    batch_size, seq_len, vocab_size = logits.size()\n",
    "    labels = labels.view(-1)  # flatten to (batch_size * seq_len)\n",
    "    log_probs_flat = log_probs.view(-1, vocab_size)\n",
    "\n",
    "    # Extract the log probabilities for the correct labels\n",
    "    correct_log_probs_flat = log_probs_flat[range(batch_size * seq_len), labels]\n",
    "\n",
    "    # Reshape to (batch_size, seq_len)\n",
    "    correct_log_probs = correct_log_probs_flat.view(batch_size, seq_len)\n",
    "\n",
    "    # Mask out the original input texts, only keep the generated tokens\n",
    "    masked_log_probs = correct_log_probs * attention_mask\n",
    "\n",
    "    # Calculate the mean log probability for each sequence\n",
    "    mean_log_probs = masked_log_probs.sum(dim=-1) / attention_mask.sum(dim=-1)\n",
    "\n",
    "    return mean_log_probs\n",
    "\n",
    "\n",
    "class MaskGeneratingModel(nn.Module):\n",
    "    def __init__(self, hidden_size=4096, mlp_hidden_dim=1024, mlp_bottleneck_dim=768, mlp_num_blocks=2):\n",
    "        \"\"\" \n",
    "        hidden_size: int\n",
    "            The hidden size of the output of the generative model, 4096 for llama3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.explain_map = MLP(input_dim=hidden_size, \n",
    "                               hidden_dim=mlp_hidden_dim, \n",
    "                               output_dim=1, \n",
    "                               num_blocks=mlp_num_blocks, \n",
    "                               bottleneck_dim=mlp_bottleneck_dim) # takes [N, L, hidden_size] outputs [N, L, 1]\n",
    "    \n",
    "    def forward(self, pred_features):\n",
    "        \"\"\" \n",
    "        pred_features: torch.Tensor of shape [N, L, hidden_size]\n",
    "        \"\"\"\n",
    "        mask_logits = self.explain_map(pred_features) # [N, L, 1]\n",
    "        return mask_logits.squeeze(-1) # [batch_size, seq_len]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_mask(self, mask_logits, context_mask):\n",
    "        \"\"\"\n",
    "        generate mask based on a Bernoulli distribution with the probabilities of the given logits \n",
    "        args:\n",
    "            mask_logits: torch.Tensor, shape (batch_size, seq_len)\n",
    "            context_mask: torch.Tensor, shape (batch_size, seq_len), the context texts \n",
    "                (for example, the chatbot instruction template, not including the user inputs) are masked with 0.\n",
    "                This mask strategy is to ensure we only focus on the real user inputs.\n",
    "        return:\n",
    "            mask: torch.Tensor, shape (batch_size, seq_len), with contexts being all 1s and user inputs being randomly masked.\n",
    "        \"\"\"\n",
    "        mask_probs = torch.sigmoid(mask_logits) # (batch_size, seq_len)\n",
    "        mask = torch.bernoulli(mask_probs) # (batch_size, seq_len)\n",
    "        mask = context_mask * mask + (1. - context_mask) # (batch_size, seq_len)\n",
    "        return mask # this could be used as the attention mask for the generative model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample_one_batch(self, input_ids, attention_mask, mask_logits, context_mask):\n",
    "        \"\"\" \n",
    "        args:\n",
    "            input_ids: torch.Tensor, shape (batch_size, seq_len)\n",
    "            attention_mask: torch.Tensor, shape (batch_size, seq_len), the attention_mask generated automatically by tokenizer, usually all 1s\n",
    "            mask_logits: torch.Tensor, shape (batch_size, prompt_len), the logits for the mask generation\n",
    "            context_mask: torch.Tensor, shape (batch_size, prompt_len), the context texts \n",
    "                (for example, the chatbot instruction template, not including the user inputs) are masked with 0.\n",
    "        \"\"\"\n",
    "        #  get the mask of interest, i.e., for the user inputs, then pad to match the shape of other masks (of the full length)\n",
    "        seq_len = input_ids.size(1)\n",
    "        prompt_len = mask_logits.size(1)\n",
    "        mask = self.generate_mask(mask_logits=mask_logits, context_mask=context_mask) # (batch_size, prompt_len)\n",
    "        pad_length = seq_len - prompt_len\n",
    "        padded_mask = F.pad(mask, (0, pad_length), mode='constant', value=1)\n",
    "\n",
    "        # get the masked attention_mask\n",
    "        masked_attention_mask = attention_mask * padded_mask\n",
    "\n",
    "        return input_ids, masked_attention_mask, mask\n",
    "    \n",
    "    def compute_similarity(self, logits, labels, attention_mask):\n",
    "        \"\"\" \n",
    "        compute the mean log_probs for the generated tokens\n",
    "        \"\"\"\n",
    "        mean_log_probs = similarity_measure(logits, labels, attention_mask)\n",
    "        return mean_log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "# idx = 0\n",
    "# texts = imdb[\"test\"][idx]['text']\n",
    "# # print(texts)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL.\"},\n",
    "#     {\"role\": \"user\", \"content\": texts},\n",
    "# ]\n",
    "\n",
    "# messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True,\n",
    "# )\n",
    "\n",
    "# print(messages_with_template_applied)\n",
    "\n",
    "# inputs = tokenizer.encode_plus(\n",
    "#             messages_with_template_applied,\n",
    "#             add_special_tokens=False,\n",
    "#             return_tensors=\"pt\",\n",
    "#             ).to(device)\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "# ]\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=128,\n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "#     return_dict_in_generate=True,\n",
    "#     output_scores=True,\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)\n",
    "# print(\"Full texts:\\n\", generated_text)\n",
    "# input_length = len(messages_with_template_applied)\n",
    "# print(\"Generated response:\", generated_text[input_length:])\n",
    "# print(\"Generated token logits\\n\", outputs.scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "train_ds = imdb['train']\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    max_len = 512\n",
    "    texts = [example['text'] for example in examples]\n",
    "    texts = [text if len(text) <= max_len else text[:max_len] for text in texts]\n",
    "    labels = [example['label'] for example in examples]\n",
    "    messages_lambda = lambda texts: [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL with a brief explanation.\"},\n",
    "        # {\"role\": \"system\", \"content\": \"You are a chatbot for sentimate analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": texts},\n",
    "    ]\n",
    "    messages = list(map(messages_lambda, texts))\n",
    "\n",
    "    messages_with_template_applied = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    batch = tokenizer(\n",
    "                messages_with_template_applied,\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "                )\n",
    "    \n",
    "    # find the template boundaries\n",
    "    text_lens = [len(tokenizer.encode(text)) for text in texts]\n",
    "    text_lens_tensor = torch.tensor(text_lens, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    def apply_mask(mask_tensor, text_lens_tensor):\n",
    "        batch_size, seq_len = mask_tensor.shape\n",
    "        for i in range(batch_size):\n",
    "            text_len = text_lens_tensor[i].item()\n",
    "            mask_tensor[i, -text_len-5:-5] = 0\n",
    "        return 1- mask_tensor\n",
    "\n",
    "    mask_tensor = apply_mask(torch.ones_like(batch['input_ids']), text_lens_tensor)\n",
    "\n",
    "    batch['context_mask'] = mask_tensor\n",
    "\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swing!!!!!\n",
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL with a brief explanation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swing<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the collate_fn\n",
    "a = next(iter(train_dataloader))\n",
    "print(tokenizer.decode(a.context_mask[1] * a.input_ids[1]))\n",
    "print(tokenizer.decode(a.input_ids[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskGeneratingModel(\n",
       "  (explain_map): MLP(\n",
       "    (input_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReLU()\n",
       "        (6): Linear(in_features=768, out_features=1024, bias=True)\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_gen_model = MaskGeneratingModel(hidden_size=4096, mlp_hidden_dim=1024, mlp_bottleneck_dim=768, mlp_num_blocks=2)\n",
    "mask_gen_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1: Loss = 0.8586, Reward Loss = 0.4148, Mask Loss = 0.4438 :   0%|          | 0/1563 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set pad_token_id if it is not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(mask_gen_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for idx, data in enumerate(pbar):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        context_mask = data['context_mask'].to(device)\n",
    "        # get generated texts\n",
    "        gen_outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        gen_tokens = gen_outputs.sequences\n",
    "        pad_length = gen_tokens.size(1) - input_ids.size(1)\n",
    "        # get the attention mask for the generated tokens, and also mask the padding tokens\n",
    "        gen_attention_mask = F.pad(attention_mask, (0, pad_length), mode='constant', value=1)\n",
    "        gen_attention_mask = gen_attention_mask * (gen_tokens != pad_token_id).long()\n",
    "        # get the response mask, which is the mask for the generated tokens (the user inputs are masked with 0)\n",
    "        response_mask = gen_attention_mask.clone()\n",
    "        response_mask[:, :-gen_tokens.size(1)] = 0 # TODO: 有问题。\n",
    "        # Get the last hidden state for the prompt sequence\n",
    "        with torch.no_grad():\n",
    "            prompt_outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "            last_hidden_state = prompt_outputs.hidden_states[-1]\n",
    "            last_hidden_state = last_hidden_state.float()\n",
    "        \n",
    "        mask_logits = mask_gen_model(last_hidden_state)\n",
    "\n",
    "        \n",
    "        # mask = mask_gen_model.generate_mask(mask_logits=mask_logits, context_mask=context_mask)\n",
    "        perturbed_input_ids, masked_attention_mask, user_input_mask = mask_gen_model.sample_one_batch(input_ids=gen_tokens, \n",
    "                                                                attention_mask=gen_attention_mask, \n",
    "                                                                mask_logits=mask_logits, \n",
    "                                                                context_mask=context_mask)\n",
    "        \n",
    "        # Get the logits of the perturbed input concatenated with the response\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=perturbed_input_ids, attention_mask=masked_attention_mask, return_dict=True)\n",
    "            logits = outputs.logits.float()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_gt = model(input_ids=perturbed_input_ids, attention_mask=masked_attention_mask, return_dict=True)\n",
    "            logits_gt = outputs_gt.logits.float()\n",
    "        \n",
    "        # Compute the similarity between the logits and the response labels\n",
    "        sim = mask_gen_model.compute_similarity(logits, gen_tokens, response_mask)\n",
    "        sim_gt = mask_gen_model.compute_similarity(logits_gt, gen_tokens, response_mask)\n",
    "\n",
    "        # PPO\n",
    "        \n",
    "        reward = torch.exp(sim - sim_gt)\n",
    "        mask_prob = torch.sigmoid(mask_logits)\n",
    "        bce_loss = nn.BCELoss(reduction='none')\n",
    "        reward_loss = reward.unsqueeze(-1) * bce_loss(context_mask * mask_prob, context_mask * user_input_mask)\n",
    "        reward_loss = reward_loss.mean()\n",
    "        mask_loss = (mask_prob * context_mask).sum(dim=-1) / context_mask.sum(dim=-1)\n",
    "        mask_loss = mask_loss.mean()\n",
    "\n",
    "        loss = reward_loss + mask_loss\n",
    "        pbar.set_description(f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss.item():.4f}, \" \n",
    "                             f\"Reward Loss = {reward_loss.item():.4f}, \"\n",
    "                            #  f\"Regret Loss = {loss_dict['regret_loss'].item():.4f}, \"\n",
    "                             f\"Mask Loss = {mask_loss.item():.4f} \"\n",
    "                            #  f\"alt_mask_loss = {loss_dict['alt_mask_loss'].item():.4f} \"\n",
    "                            #  f\"mask_mean = {loss_dict['mask_mean'].item():.4f} \"\n",
    "                            #  f\"prob_mean = {loss_dict['prob_mean'].item():.4f} \"\n",
    "                             )\n",
    "        # Train the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # pbar.set_description(f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss.item():.4f}, \" \n",
    "        #                      f\"Reward Loss = {reward_loss.item():.4f}, \"\n",
    "        #                     #  f\"Regret Loss = {loss_dict['regret_loss'].item():.4f}, \"\n",
    "        #                      f\"Mask Loss = {mask_loss.item():.4f} \"\n",
    "        #                     #  f\"alt_mask_loss = {loss_dict['alt_mask_loss'].item():.4f} \"\n",
    "        #                     #  f\"mask_mean = {loss_dict['mask_mean'].item():.4f} \"\n",
    "        #                     #  f\"prob_mean = {loss_dict['prob_mean'].item():.4f} \"\n",
    "        #                      )\n",
    "        if idx % 10 == 0:\n",
    "            print()\n",
    "\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "        # scheduler.step()\n",
    "        # pbar.set_description(f\"Epoch {epoch+1}, Step {idx+1}: Loss = {loss_dict['loss'].item():.4f}, \" \n",
    "        #                      f\"Reward Loss = {loss_dict['reward_loss'].item():.4f}, \"\n",
    "        #                     #  f\"Regret Loss = {loss_dict['regret_loss'].item():.4f}, \"\n",
    "        #                      f\"Mask Loss = {loss_dict['mask_loss'].item():.4f} \"\n",
    "        #                     #  f\"alt_mask_loss = {loss_dict['alt_mask_loss'].item():.4f} \"\n",
    "        #                      f\"mask_mean = {loss_dict['mask_mean'].item():.4f} \"\n",
    "        #                      f\"prob_mean = {loss_dict['prob_mean'].item():.4f} \"\n",
    "        #                      )\n",
    "        # if idx % 10 == 0:\n",
    "        #     print()\n",
    "        # if (idx) % 10 == 0:\n",
    "            \n",
    "        #     torch.save(mask_gen_model.state_dict(), f'text_mask_gen_model/mask_gen_model_{epoch}_{idx}.pth') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "masked_attention_mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "        128009, 128000, 128006,   9125, 128007,    271,   2675,    527,    264,\n",
       "          6369,   6465,    369,   3288,   3509,   6492,     13,   1472,    649,\n",
       "          1520,   3932,    449,    872,   4860,   4669,  64694,  14847,    315,\n",
       "         27592,  45450,     11,  85165,  24093,   2794,   8014,   1406,  51785,\n",
       "           449,    264,  10015,  16540,     13, 128009, 128006,    882, 128007,\n",
       "           271,     40,  49959,    358,   6912,  19058,  43752,  30237,  35771,\n",
       "           505,    856,   2835,   3637,   1606,    315,    682,    279,  26654,\n",
       "           430,  23712,    433,    994,    433,    574,   1176,   6004,    304,\n",
       "           220,   5162,     22,     13,    358,   1101,   6755,    430,    520,\n",
       "          1176,    433,    574,  31589,    555,    549,    815,     13,  35869,\n",
       "           422,    433,   3596,   6818,    311,   3810,    420,   3224,     11,\n",
       "          9093,   1694,    264,   8571,    315,  12631,   6646,    330,    778,\n",
       "         12848,    532,      1,    358,   2216,   1047,    311,   1518,    420,\n",
       "           369,   7182,  16134,   1347,  24930,   1347,   6338,    791,   7234,\n",
       "           374,  31288,   2212,    264,   3995,  31209,  20156,   5575,   7086,\n",
       "         82162,    889,   6944,    311,   4048,   4395,   1364,    649,    922,\n",
       "          2324,     13,    763,   4040,   1364,   6944,    311,   5357,   1077,\n",
       "         52309,    919,    311,    296,  14966, 128009, 128006,  78191, 128007,\n",
       "           271], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!!!!!!!!!!!!!!!!!!!<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a chatbot for sentimate analysis. You can help users with their questions via concise responses of POSITIVE, NEGATIVE OR NEUTURAL with a brief explanation.!<|start_header_id|>user<|end_header_id|>!!! I AM CURIOUS-YELLOW from!!! because!!! controversy! surrounded it! it was!! in 196!! I! heard!! first! was!!!!! customs! it ever!!!!!! therefore!!! of films! \"cont!!\" I!!! see this!!!br!!!The! is centered! a! Swedish! student named Lena who wants! learn everything she can about life. In!! wants!! her attent!!!aki!<|start_header_id|>assistant<|end_header_id|>\\n\\nPOSITIVE\\n\\nThe tone of your text is positive because you\\'re expressing your interest in a film that was considered controversial when it was first released, and you\\'re eager to learn more about it. Your curiosity and enthusiasm for the film are evident, and you\\'re not expressing any negative opinions or criticisms.!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(perturbed_input_ids[idx] * masked_attention_mask[idx].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128009"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;196mThe\u001b[0m \u001b[48;5;88mquick\u001b[0m \u001b[48;5;16mbrown\u001b[0m \u001b[48;5;28mfox\u001b[0m \u001b[48;5;46mjumps\u001b[0m \u001b[48;5;124mover\u001b[0m \u001b[48;5;52mthe\u001b[0m \u001b[48;5;22mlazy\u001b[0m \u001b[48;5;34mdog\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_importance(importance_tensor):\n",
    "    \"\"\"Normalize the importance tensor to the range [0, 1].\"\"\"\n",
    "    min_val = np.min(importance_tensor)\n",
    "    max_val = np.max(importance_tensor)\n",
    "    normalized = (importance_tensor - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def importance_to_bg_color(importance):\n",
    "    \"\"\"Map normalized importance value to an ANSI background color code using a custom gradient.\"\"\"\n",
    "    if importance < 0.5:\n",
    "        # Interpolate between red (1, 0, 0) and black (0, 0, 0)\n",
    "        color_rgb = np.array([255 * (1 - 2 * importance), 0, 0])\n",
    "    else:\n",
    "        # Interpolate between black (0, 0, 0) and green (0, 1, 0)\n",
    "        color_rgb = np.array([0, 255 * (2 * (importance - 0.5)), 0])\n",
    "    \n",
    "    color_rgb = color_rgb.astype(int)\n",
    "    \n",
    "    # Convert RGB to 256-color mode\n",
    "    color_code = 16 + 36 * (color_rgb[0] // 51) + 6 * (color_rgb[1] // 51) + (color_rgb[2] // 51)\n",
    "    return f'\\033[48;5;{color_code}m'\n",
    "\n",
    "def print_colored_string(token_tensor, importance_tensor):\n",
    "    normalized_importance = normalize_importance(importance_tensor)\n",
    "    colored_string = \"\"\n",
    "    for token, importance in zip(token_tensor, normalized_importance):\n",
    "        bg_color = importance_to_bg_color(importance)\n",
    "        colored_string += f\"{bg_color}{token}\\033[0m \"\n",
    "    print(colored_string)\n",
    "\n",
    "# Example usage\n",
    "token_tensor = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "importance_tensor = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "print_colored_string(token_tensor, importance_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
